<!DOCTYPE html><html lang="ja"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>cvpaper.challenge</title><link rel="stylesheet" href="css/reveal.css"><link rel="stylesheet" href="css/theme/white.css"><link rel="stylesheet" href="css/layout.css"><link rel="stylesheet" href="lib/css/zenburn.css"></head><script>var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-118576057-1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'UA-118576057-1');
</script><body><div class="reveal"><div class="slides"><section id="ID_RayNet_Learning_Volumetric_3D_Reconstruction_With_Ray_Potentials"><div class="paper-abstract"><div class="title">RayNet: Learning Volumetric 3D Reconstruction With Ray Potentials</div><div class="info"><div class="authors">Despoina Paschalidou, Osman Ulusoy, Carolin Schmitt, Luc Van Gool, Andreas Geiger</div><div class="conference">CVPR 2108</div></div><div class="slide_editor">Takahiro Itazuri</div><div class="item1"><div class="text"><h1>概要</h1><p>異なる視点から撮影された映像から、CNNとMRFを用いて物理的制約を考慮可能な密な３次元復元を行った論文。CNNはタスクに対してネットワーク全体をデータから学習可能であるが、物理的制約を考慮することができない。一方でRay-Potentialを用いたMRFはモデルに陽な物理的制約を与えることができる一方で、大きな表面を上手く扱うことができない。本論文ではこの２つの手法の良いところをそれぞれ活かした手法であるRayNetを提案した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/RayNet.png" alt="RayNet"></p></div></div><div class="item3"><div class="text"><h1>手法・新規性</h1><p>構造としては、Multi-View CNNとMarkov Random Fieldから構成されている。Multi-View CNNは入力として複数の画像とそれに対応するカメラの姿勢を受け取り、視点による影響が小さい特徴量を抽出し、Rayごとにデプスの分布を出力する。Morkov Random Fieldは各視点からにおける遮蔽を考慮して、CNNから出力されたデプスの分布のノイズを除去する。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Paschalidou_RayNet_Learning_Volumetric_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#1]</div><div class="timestamp">2018.7.16 18:14:40</div></div></section><section id="ID_Triplet-Center_Loss_for_Multi-View_3D_Object_Retrieval"><div class="paper-abstract"><div class="title">Triplet-Center Loss for Multi-View 3D Object Retrieval</div><div class="info"><div class="authors">Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai and Xiang Bai</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>多視点画像から3次元物体検索手法を提案。クラスの重心に近づくように最適化する<a href="https://ydwen.github.io/papers/WenECCV16.pdf">center loss</a>と、同一クラス同士の距離を小さくし他クラスとの距離を大きくする<a href="https://arxiv.org/abs/1503.03832">triplet loss</a>を組み合わせたcenter-triplet lossを導入した。
triplet-center lossにより、正解クラスの重心との距離を最小化しつつ、他クラスの重心との距離は最大化する。
triplet,centerそれぞれ単独よりtriplet-center＋softmaxが一番いい。
他の手法よりも3d shape、sketchどちらにおいても精度がいい。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Triplet-Center_Loss_for_Multi-View_3D_Object_Retrieval.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>triplet loss、center loss単独で最適化するよりcenter-triplet loss及びsoftmax lossを組み合わせたものがAUC及びmAPが最も良くなることを確認した。従来手法と比べ、generic 3D shape retrieval及びsketch-based 3D shape retrievalの2種類いずれのタスクにおいて、F1、mAP、NDCGの三つの指標が最も良いという結果が得られた。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Future workとして書かれているが、手法自体は他のタスクにも試せそう。3D Object Retrievalに特化して構築された手法でないにも関わらず他のタスクが紹介されていないのは他のタスクがうまくいっていないということだろうか？</p><ul><li><a href="https://arxiv.org/abs/1803.06189">論文URL</a></li></ul></div></div><div class="slide_index">[#2]</div><div class="timestamp">2018.7.15 02:33:13</div></div></section><section id="ID_Thoracic_Disease_Identification_and_Localization_with_Limited_Supervision"><div class="paper-abstract"><div class="title">Thoracic Disease Identification and Localization with Limited Supervision</div><div class="info"><div class="authors">Zhe Li, Chong Wang, Mei Han, Yuan Xue, Wei Wei and Li Fei-Fei</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>医療画像から、病名の特定及び異常箇所の特定を行う手法を提案した。ResNetにより抽出した特徴を、パッチに分割し各パッチが異常箇所であるかを予測する。
予測したパッチ情報を用いて、病名の判定を行う。
学習時には、病名のみラベルがついていて異常箇所のラベルが付いていない場合がある。
そこで、病名のみしか存在しない場合は少なくとも1つのパッチが異常箇所であると仮定して学習を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Thoracic_Disease_Identification_and_Localization_with_Limited_Supervision.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>病名診断については、14の病名のうち12の病名においてベースラインよりも精度が向上した。異常箇所の特定については、従来手法と比べ8つの病名全てにおいて精度が向上している。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Thoracic_Disease_Identification_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#3]</div><div class="timestamp">2018.7.16 00:59:33</div></div></section><section id="ID_Occlusion-Aware_Rolling_Shutter_Rectification_of_3D_Scenes"><div class="paper-abstract"><div class="title">Occlusion-Aware Rolling Shutter Rectification of 3D Scenes</div><div class="info"><div class="authors">Subeesh Vasu, Mahesh Mohan M. R. and A. N. Rajagopalan</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>カメラモーションによって生じるdistortionをなくすための手法を提案。市販のカメラの多くは、撮影時に行ごとに処理を行うためカメラが動いている場合同じ画像であっても各行のカメラの位置は異なるため、distortionが生じてしまう。
そこで画像の各行が異なるカメラ位置として扱い、distortionのない状態への復元を行う。
具体的には、動画の各フレームからdepth mapを推定することで、backgroundの復元を行う。
続いて3次元空間をlayer分けして考え、background以外のlayerに対するマスクを作成することでocclusion領域を埋めていく。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Occlusion-Aware_Rolling_Shutter_Rectification_of_3D_Scenes.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来手法と比べ、ピクセルの推定値を評価するPSNR、カメラモーションの推定値を評価するAPMEどちらも向上した。特にカメラモーションの推定は従来手法と比べて格段に向上している。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Vasu_Occlusion-Aware_Rolling_Shutter_CVPR_2018_paper.pdf">論文URL</a></li></ul></div></div><div class="slide_index">[#4]</div><div class="timestamp">2018.7.15 04:38:09</div></div></section><section id="ID_Joint_Optimization_Framework_for_Learning_with_Noisy_Labels"><div class="paper-abstract"><div class="title">Joint Optimization Framework for Learning with Noisy Labels</div><div class="info"><div class="authors">Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki and Kiyoharu Aizawa</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>学習データのラベルにノイズが含まれている場合の学習方法を提案した。ネットワークのパラメータを求めるのみならず、ラベルそのものも更新していくことでラベルからノイズを取り除くことを可能とする。
ネットワークのパラメータとラベルの一方を固定した更新を繰り返すことにより最適化していく。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Joint_Optimization_Framework_for_Learning_with_Noisy_Labels.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>CIFAR-10 dataset及びClothing1M datasetにより評価を行った。CIFAR-10の結果は、ノイズの割合に関わらず提案手法がベースラインと比べ精度が向上し、ノイズが50%含まれる場合でもTest Accuracy84.7%、Recovery Accuracy88.1%を記録した。
Clothing1M datasetもベースラインよりaccuracyが良く、72.23%を記録した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.11364">論文</a></li></ul></div></div><div class="slide_index">[#5]</div><div class="timestamp">2018.7.16 02:39:24</div></div></section><section id="ID_Geometry-aware_Deep_Network_for_Single-Image_Novel_View_Synthesis"><div class="paper-abstract"><div class="title">Geometry-aware Deep Network for Single-Image Novel View Synthesis</div><div class="info"><div class="authors">Miaomiao Liu, Xuming He and Mathieu Sapzmann</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>1枚画像から視点を変えた画像を生成する方法を提案した。有限の数の平面の存在を仮定し、各平面の組み合わせによって新たな視点の画像を生成する。
入力画像に対してピクセル単位でdepthとnormalを推定し、平面の数と同様のHomography変換を考える。
同時に入力画像からピクセル単位でどの平面を出力画像の生成に用いるか決定することで、出力画像を得る。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Geometry-aware_Deep_Network_for_Single-Image_Novel_View_Synthesis.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来手法が考慮していなかった3次元的な特徴を考慮することで、歪みなどが存在しない画像を出力することに成功した。数値評価においても、ground truthとのL1ノルムがベースラインと比べ小さくなっている。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.06008">論文</a></li></ul></div></div><div class="slide_index">[#6]</div><div class="timestamp">2018.7.14 15:58:18</div></div></section><section id="ID_Compassionately_Conservative_Balanced_Cuts_for_Image_Segmentation"><div class="paper-abstract"><div class="title">Compassionately Conservative Balanced Cuts for Image Segmentation</div><div class="info"><div class="authors">Nathan D. Cahill, Tyler L. Hayes, Renee T. Meinhold and John F. Hamilton</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>グラフカットの問題において、edgeの重みが他よりも小さい場合そのedgeで切断してしまいnodeが１つしかないクラスができてしまう。この問題を解決するために、Compassionately Conservative Balanced (CCB) Cut costsを提案した。
クラス間のnode数のバランスを取るための方法として、Compassionately Conservative Ratio CutやCompassionately Conservative Normalized Cutなどが提案されているが、CCBはこれらを一般化したcostとなる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Compassionately_Conservative_Balanced_Cuts_for_Image_Segmentation.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来手法が考慮していなかった3次元的な特徴を考慮することで、歪みなどが存在しない画像を出力することに成功した。数値評価においても、ground truthとのL1ノルムがベースラインと比べ小さくなっている。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.06008">論文</a></li></ul></div></div><div class="slide_index">[#7]</div><div class="timestamp">2018.7.14 16:44:14</div></div></section><section id="ID_CLEAR_Cumulative_LEARning_for_One-Shot_One-Class_Image_Recognition"><div class="paper-abstract"><div class="title">CLEAR: Cumulative LEARning for One-Shot One-Class Image Recognition</div><div class="info"><div class="authors">Jedrzej Kozerawski and Matthew Turk</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Positiveデータが1枚のみであり、Negativeデータが存在しないOne-Shot One-Class(OSOC)問題を解く方法としてCulmulative LEARning(CLEAR)を提案した。人間が学習する際、同じことを何度も繰り返すこと、似たような技能を既に修得している場合はそうでない場合よりも上達が早いことに着目した。
学習済みの特徴抽出器から得られた画像特徴より、識別の境界を決定するネットワークによって識別器を構築する。
学習の際には、ImageNetから取って来た1枚の画像に対して境界を決定し、その画像が得られた境界によって正しく識別できているかを見ることで学習する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/CLEAR_Cumulative_LEARning_for_One-Shot_One-Class_Image_Recognition.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>5種類のデータセット(Caltecb-256, Oxford Flowers, Caltech-UCSD Bird-200-2011, MIT Indoor scene recognition and SUN attribute database)で実験した。実験の結果、MAP及びF1の指標がランダム出力、One-ClassSVMと比べ精度が良いことを確認した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Kozerawski_CLEAR_Cumulative_LEARning_CVPR_2018_paper.html">論文</a></li></ul></div></div><div class="slide_index">[#8]</div><div class="timestamp">2018.7.15 16:53:02</div></div></section><section id="ID_A_Hierarchical_Generative_Model_for_Eye_Image_Synthesis_and_Eye_Gaze_Estimation"><div class="paper-abstract"><div class="title">A Hierarchical Generative Model for Eye Image Synthesis and Eye Gaze Estimation</div><div class="info"><div class="authors">Kang Wang, Rui Zhao, Qiang Ji</div><div class="conference">CVPR2018</div><div class="paper_id">70</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>与えられた視線方向から視線画像を生成してくれるHierarchical Generative Model(HGM)を提案．HGMは2つのネットワークから構築されており，KnowledgeベースのHierarchical Generative Shape Model(HGSM)とData-drivenなconditional Bidirectional Generative Adversarial Network(c-BiGAN)から構成されている．
ここで，入力する視線方向は，yaw, pitch, rollである．
HGSMは，与えられた視線方向から目の形状のパラメータを推定する．
c-BiGANでは，2種類の入力によりDiscriminatorを学習する．
Generatorが出力したsynthesized imageとHGSMの出力と，real imageとEncoderで出力した目の形状パラメータであり，これらの入力を用いてDiscriminatorを学習する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/70_overview.jpeg" alt="70_overview"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>生成されたCGを用いて学習するアプローチ．SimGANではCGを作った後に学習しているが，この手法では視線方向等のサンプルパラメータのみで学習サンプルの生成&推定が可能である．
この論文では，視線推定だけでなく，表情推定にも応用することができる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1895.pdf">論文リンク</a></li></ul></div></div><div class="slide_index">[#9]</div><div class="timestamp">2018.7.16 01:12:34</div></div></section><section id="ID_HydraNets_Specialized_Dynamic_Architectures_for_Efficient_Inference"><div class="paper-abstract"><div class="title">HydraNets: Specialized Dynamic Architectures for Efficient Inference</div><div class="info"><div class="authors">Ravi Teja Mullapudi, William R. Mark, Noam Shazeer, Kayvon Fatahalian</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Takahiro Itazuri</div><div class="item1"><div class="text"><h1>概要</h1><p>DNNの高い精度を保持したまま計算コストの削減が可能なHydraNetを提案した。HydraNetには推論時に入力に対して良い精度を出すようにネットワークアーキテクチャの部分集合を選択するsoft gating mechanismが組み込まれている。このような動的な構造を持たせることでaccuracy-per-unit-costを向上させた。実験では、画像分類タスクにおいてResNetやDenseNetと同等の精度をより少ない計算コストで出した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/HydraNets.png" alt=""></p></div></div><div class="item3"><div class="text"><h1>手法・新規性</h1><p>HydraNetは複数のbranchで構成され、各branchは特定のsubtask特化するように学習されている。その後、gating mechanismによって動的に適切なbranchを選択し、その選択されたbranchから来る特徴量を統合し、最終的な推論を行う。HydraNetでは、各branchは最後の推論までは行わず、subtaskに対応する特徴量だけを計算するような構造になっていることが計算効率の向上につながっている。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Mullapudi_HydraNets_Specialized_Dynamic_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#10]</div><div class="timestamp">2018.7.15 20:53:13</div></div></section><section id="ID_Dual_Skipping_Networks"><div class="paper-abstract"><div class="title">Dual Skipping Networks</div><div class="info"><div class="authors">Changmao Cheng, Yanwei Fu, Yu-Gang Jiang, Wei Liu, Wenlian Lu, Jianfeng Feng, Xiangyang Xue</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Takahiro Itazuri</div><div class="item1"><div class="text"><h1>概要</h1><p>右脳と左脳で視覚情報を処理している解像度が異なるという人間の脳の仕組みを模倣したネットワークDual Skipping Networksを提案した。このネットワークは２つのサブネットワークで構成されており、それぞれ同様の構造を持つが、左右でスキップ可能な層のパラメータが異なっており、その結果、左右非対称なネットワークがそれぞれglobalな推論とlocalな推論をするようになっている。画像分類の問題において、既存のデータセットに加えて、小さな文字で他の文字を構成するsb-MNISTデータセットで実験を行い、可視化によってそれぞれがglobalな情報とlocalな情報を保持していることを確認し、また非常に良い精度を出した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Dual_Skipping_Networks.png" alt=""></p></div></div><div class="item3"><div class="text"><h1>手法・新規性</h1><p>Dual Skipping Networksのネットワーク構造は、右脳と左脳に対応する２つのサブネットワークとそれらが共有するCNNから構成される。共有されているCNNは脳におけるV1領域に対応しており、２つのサブネットワークはそれぞれ右脳と左脳に対応し、globalな推論とlocalな推論をするようになっている。各サブネットワークはSkip-Dense BlockとTransition Layerを交互に重ねた構造になっており、Skip-Dense Blockにおけるスキップ率の違いが２つのサブネットワークの差になっている。Skip-Dense BlockはDense LayerとGating Networkで構成され、Gating Networkがスキップをするか否かを司っている。またglobalな推論をするネットワークからlocalな推論を行うネットワークへの情報を伝達するGuideにより、coarse-to-fineな推論が可能になった。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cheng_Dual_Skipping_Networks_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#11]</div><div class="timestamp">2018.7.15 20:05:24</div></div></section><section id="ID_Zigzag_Learning_for_Weakly_Supervised_Object_Detection"><div class="paper-abstract"><div class="title">Zigzag Learning for Weakly Supervised Object Detection</div><div class="info"><div class="authors">Xiaopeng Zhang, Jiashi Feng, Hongkai Xiong and Qi Tian</div><div class="conference">CVPR2018</div><div class="paper_id">551</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>物体検出の弱教師あり学習において，overfittingを防ぐためにretrain・relocalizeを繰り返すジグザグ学習を提案．特定の対象物を参照して学習画像の難しさを自動で測定する指標「mean Energy Accumulated Scores（mEAS，下図）」を導入し，これに基づいて検出ネットワークを学習する．また，学習中に特徴マップのマスキングを行い，細部に集中するだけでなく，ランダムにoccludeされたpositive-instanceを導入することでoverfittingを防ぎ，汎化性能を高める．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180714_ZigZag1.jpg" alt="20180714_ZigZag1.jpg"><img src="slides/figs/20180714_ZigZag2.jpg" alt="20180714_ZigZag2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>対象物体がわかりやすいかわかりにくいかの単純な戦略を用いて検出モデルを学習し，信頼性の高いインスタンスを検出することができる．弱教師あり学習の物体検出手法でSOTAを達成．VOCデータセットを用いた評価により，ほとんどの物体が他の手法よりも良い性能を達成し，総合のmAPは3～6%程度向上した．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>背景がmEASにもたらす影響が気になる．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0397.pdf">論文</a></li></ul></div></div><div class="slide_index">[#12]</div><div class="timestamp">2018.7.14 21:10:38</div></div></section><section id="ID_Boundary_Flow_A_Siamese_Network_That_Predicts_Boundary_Motion_Without_Training_on_Motion"><div class="paper-abstract"><div class="title">Boundary Flow: A Siamese Network That Predicts Boundary Motion Without Training on Motion</div><div class="info"><div class="authors">P. Lei et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">物体の境界の推定と隣接フレーム間での境界のモーションの推定を同時に行うBoundary Flow Estimationという問題の提案．
これができるとMid-levelの表現として色々なタスクに利用できたりして嬉しい．
提案手法は，tフレームとt+1フレームの2入力を受け取るSiamese Net型の構造で，Encoder-Decoderにより両フレームのBoundaryを推定する．
Boundaryのマッチングのために，Excitation Attentionのスコアでマッチングスコアが計算される．
Siameseの2つのパスのモデルは同じ構造で重みは共有されているので，Boundaryのアノテーション付いている静止画データがあれば学習可能．
Boundary Detection, Boundary Flow Estimation, Dense Optical Flow Estimationの3タスクで良い性能を達成．
</div></div><div class="item2"><img src="slides/figs/Boundary_Flow_A_Siamese_Network_That_Predicts_Boundary_Motion_Without_Training_on_Motion.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>Boundary Flow Estimationという新しい問題設定を提案</li><li>Fully Convolutional Siamese Networkという構造の提案手法</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Lei_Boundary_Flow_A_CVPR_2018_paper.html">論文</a></li><li>Excitation Attentionがよくわかってないからか，マッチングの話がよくわからず...</li><li>そこがわかってないからか，なぜBoundaryのマッチングの学習データがなくてもうまくいくのかよくわからず...</li></ul></div></div><div class="slide_index">[#13]</div><div class="timestamp">2018.7.11 15:45:23</div></div></section><section id="ID_Graph-Cut_RANSAC"><div class="paper-abstract"><div class="title">Graph-Cut RANSAC</div><div class="info"><div class="authors">D. Barath and J. Matas</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>RANSACのバリアントに，一番よく見えるモデルが見つかった時に局所最適化でリファインするLO-RANSACがあるが，
この局所最適化の部分を2クラス分類の雄であるGraph-Cutに代替した．
従来法における，ただ最小二乗で局所最適化するより局所最適化の評価回数がかなり少なくなる（理論的にはlog(サンプル＋検証の数)）ようになっており，その結果，CPUでミリ秒単位で動く高速性がある．
実際には，空間的コヒーレンスが効いて理論値より評価回数が更に少ない模様．</p><p>ユーザ定義パラメータは少なく，連結とみなす距離r，局所最適化適用しきい値ε_confを決めればよい．これらは学習可能である．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Graph-Cut_RANSAC.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>特徴として，１．インライヤ・アウトライヤが空間的コヒーレント，２．パラメータは直感的かつ学習可能，４．計算効率がよい，３．収束性がよい．</p><p>タイムリミットを置いて比較したとき，ノイジーなデータにおいての正解数が他のLO-RANSAC系手法より優れていることを示した．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>シンプルで強力な手法に感じたので熟読したが，重複表現が多かったり誤植があったりして読解性が低く感じた．900本強あるCVPR論文の中，
時間を浪費するのでポスターといえど論文としてのクオリティは最低限維持してほしいと
完全読破チャレンジャーとしては思う．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0362.pdf">論文</a></li></ul></div></div><div class="slide_index">[#14]</div><div class="timestamp">2018.7.11 09:14:45</div></div></section><section id="ID_Compressed_Video_Action_Recognition"><div class="paper-abstract"><div class="title">Compressed Video Action Recognition</div><div class="info"><div class="authors">Chao-Yuan Wu, Manzil Zaheer, Hexiang Hu, R. Manmatha, Alexander J. Smola, Philipp Krahenbuhl</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Takahiro Itazuri</div><div class="item1"><div class="text"><h1>概要</h1><p>MPEG-4やH.264のようなコーデックによって圧縮された映像を直接入力として行動認識を行う論文。背景として、映像には時間方向の冗長性が多く含まれており、その事実はコーデックによって大幅に圧縮できることが挙げられる。圧縮された状態に含まれるmotion vectorとresidualを直接入力とするネットワークCoViARによって、高速かつ高精度な行動認識に成功した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Compressed_Video_Action_Recognition.png" alt=""></p></div></div><div class="item3"><div class="text"><h1>手法・新規性</h1><p>提案手法の入力として、初期フレームにおいてはRGBの情報を持っており、後続するフレームには初期フレームに対するmotion vectorとresidualを持っている。通常のコーデックでは1つ前のフレームに対するmotion vectorとresidualが格納されているので、初期フレームから注目フレームまで累積することで、初期フレームと累積したmotion vectorとresidualを用いることで現在フレームを復元することできる。実際に推定する際には、初期フレームにおけるRGBから得られた特徴量と、各フレームのmotion vectorとresidualから得られた特徴量を統合して、各フレームの行動認識スコアを出力する。異なる動画間の入力ドメインでの分布を見ると、motion vectorとresidualは領域を共有しており、その結果効率的に学習することができる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Compressed_Video_Action_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#15]</div><div class="timestamp">2018.7.12 17:00:50</div></div></section><section id="ID_Matryoshka_Networks_Predicting_3D_Geometry_via_Nested_Shape_Layers"><div class="paper-abstract"><div class="title">Matryoshka Networks: Predicting 3D Geometry via Nested Shape Layers</div><div class="info"><div class="authors">Stephan R. Richter, Stefan Roth</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Takahiro Itazuri</div><div class="item1"><div class="text"><h1>概要</h1><p>2次元画像から3次元形状を復元する論文。DNNを使って3次元形状を推定する手法は、voxelを直接出力するようになっており、GPUのメモリ容量の制限から高解像度な3次元形状を復元することができなかった。本論文では、メモリ効率を良くするため、特定の方向へ延びるtubeが各ピクセルに対応する二次元表現voxel tubeを出力するshape layerを提案した。またネスト構造を持たせたshape layerを適用することで、自己遮蔽領域への対応したネットワークMatryoshka Networkを提案した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Matryoshka_Networks.png" alt=""></p></div></div><div class="item3"><div class="text"><h1>手法・新規性</h1><p>shape layerは6軸方向から見た深度画像を出力し、各軸に対応する2つの深度画像に挟まれた領域の共有部分を出力する。この場合、すべての軸から見ても遮蔽されている領域を復元することができないため、マトリョーシカのようなネスト構造を持つshape layerを出力するMatryoshka Networkを提案し、このネットワークは集合の差と和集合を交互に繰り返すネスト構造を持つ。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Richter_Matryoshka_Networks_Predicting_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#16]</div><div class="timestamp">2018.7.12 15:12:46</div></div></section><section id="ID_Depth-Based_3D_Hand_Pose_Estimate_From_Current_Achievements_to_Future_Goals"><div class="paper-abstract"><div class="title">Depth-Based 3D Hand Pose Estimate From Current Achievements to Future Goals</div><div class="info"><div class="authors">Shanxin Yuan et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Takahiro Itazuri</div><div class="item1"><div class="text"><h1>概要</h1><p>3D Hand Pose Estimationのサーベイ的論文。主に以下の2つの点に主眼を置いている。</p><ul><li>デプス画像からの3D Hand Pose Estimationの現状を明らかにする</li><li>次に挑戦するべきである課題は何かを明らかにする</li></ul><p>Hands In the Million Challenge (HIM2017)のトップ10の最新手法に関して、3つのタスク（単一画像からの姿勢推定、3次元トラッキング、物体とインタラクション中の姿勢推定）において調査を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Depth-Based_3D_Hand_Pose_Estimation.png" alt=""></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>DNNによる手法が混濁する中で、業界を整理するサーベイ的論文が評価されている（？）。最終的に、3D Hand Pose Estimationの現状において以下の7点の洞察を得た。</p><ul><li>3DCNNを用いた3次元表現は入力のデプス情報の空間的構造を捉えることができ、良い精度を出した。</li><li>検出ベースの手法は回帰ベースの手法より良い精度を出した。しかし、回帰ベースの手法は明示的に空間的制約を加えることで良い精度を出すことができる。</li><li>遮蔽された関節を推定することはほとんどの手法にとってチャレンジングであるが、明示的な構造の制約や関節間の空間的関係性をモデリングすることで、遮蔽なしとありの差を大きく狭めることができる。</li><li>識別的手法はまだ見ぬ手の形に著しく脆弱であり、良い生成能力を持つ機構を組み合わせることで、今後良い方向に進みそう。</li><li>70~120度の見え角では、非常に良い精度を出す一方で、極端な見え角ではエラーが大きくなる。</li><li>トラッキングでは、現在の識別的手法においては検出を姿勢推定の2つサブタスクに分けて問題を解いている。</li><li>単一画像からの姿勢推定は100万程度のデータセット上で学習すると良い精度を出すが、物体とのインタラクションには一般化できていない。今後の方針として、より良いセグメンテーション方法をデザインするか、物体とのインタラクションを含む大規模データセットで学習することが挙げられる。</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0155.pdf">論文</a></li></ul></div></div><div class="slide_index">[#17]</div><div class="timestamp">2018.7.12 08:03:19</div></div></section><section id="ID_Dimensionalitys_Blessing_Clustering_Images_by_Underlying_Distribution"><div class="paper-abstract"><div class="title">Dimensionality's Blessing: Clustering Images by Underlying Distribution</div><div class="info"><div class="authors">Wen-Yan Lin, Siying Liu, Jian-Huang Lai, Yasuyuki Matsushita</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Takahiro Itazuri</div><div class="item1"><div class="text"><h1>概要</h1><p>画像分野では画像理解のために、画像を高次元の特徴ベクトルにして処理を行うことで大きな成功を収めてきた。しかしながら画像のクラスタリングは現在も非常に難しいタスクである。その理由として挙げられることは、クラス内分散がクラス間分散より大きいため、大部分が重複した分布を持っている点である。本論文では、高次元特徴量の場合、ほぼすべてのサンプルがある位置を中心に特定の半径の領域（hyper-shell）に分布することに着目し、新たなクラスタリング手法であるDistribution-Cluteringを提案した。これにより、従来のクラスタリング手法より良いクラスタリングが可能になった。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Dimensionality_Blessing.png" alt="image"></p></div></div><div class="item3"><div class="text"><h1>手法・新規性</h1><p>高次元の球の体積がほとんど外側に集中していることはよく知られている事実であるが、それを掘り起こしてきて、クラスタリングに生かし、CVPRに通すところがすごい。具体的な手法の部分は正直なところ理解できなかったが、数学的にも妥当なクラスタリングができているようである。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Lin_Dimensionalitys_Blessing_Clustering_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#18]</div><div class="timestamp">2018.7.11 14:39:18</div></div></section><section id="ID_CNN_based_Learning_using_Reflection_and_Retinex_Models_for_Intrinsic_Image_Decomposition"><div class="paper-abstract"><div class="title">CNN based Learning using Reflection and Retinex Models for Intrinsic Image Decomposition</div><div class="info"><div class="authors">D. Xu et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">Intrinsic Image Decomposition（画像を反射特性 (Reflectance) や影 (Shading) などの要素に分解）において，
Deepベースの手法はブラックボックス過ぎるので画像生成 (Image Formation) の原理なども考慮するような手法を2つ提案．
1つ目のIntrinsicNetはEncoder-Decoderのモデルで，DecoderはReflectanceとShadingそれぞれを復元．
単なるReflectanceとShadingのLossに加えて，ReflectanceとShadingから復元した画像のLossも利用するのがポイント．
2つ目のRetiNetは従来手法のRetinex（画像の勾配の大きさからReflectanceとShadingに分解）のアイディアをDeep手法に導入．
勾配の分解をEncoder-Decoderでやった後に元の画像と合わせてReflectanceとShadingを推定する．
新しく提供する大規模データセットでモデルを学習して従来よりも高い性能を達成．
</div></div><div class="item2"><img src="slides/figs/CNN_based_Learning_using_Reflection_and_Retinex_Models_for_Intrinsic_Image_Decomposition.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>Intrinsic Image DecompositionのDeepモデル学習のための大規模データセット（2万画像）を提供</li><li>従来の物理特性を考慮した手法とブラックボックスなDeep手法の良いとこ取りを実現</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Baslamisli_CNN_Based_Learning_CVPR_2018_paper.html">論文</a></li><li><a href="https://ivi.fnwi.uva.nl/cv/retinet">Project Page</a></li></ul></div></div><div class="slide_index">[#19]</div><div class="timestamp">2018.7.11 15:45:23</div></div></section><section id="ID_Structured_Attention_Guided_Convolutional_Neural_Fields_for_Monocular_Depth_Estimation"><div class="paper-abstract"><div class="title">Structured Attention Guided Convolutional Neural Fields for Monocular Depth Estimation</div><div class="info"><div class="authors">D. Xu et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">単眼カメラからのデプス推定のための新しい手法を提案．
CNNにCRFを導入して，途中の層から得られるマルチスケール情報の統合を最適化できるようにしたのが提案手法のポイント．
マルチスケールの統合のためにはアテンション機構を導入して，それをうまく実現している．
実行速度も速く精度も高いという結果が出ている．
</div></div><div class="item2"><img src="slides/figs/Structured_Attention_Guided_Convolutional_Neural_Fields_for_Monocular_Depth_Estimation.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>単眼デプス推定で高い性能を達成 (NYU Depth V2でSOTA超え，KITTIでSOTA並み)</li><li>単眼デプス推定 & CRFによるマルチスケール統合 & アテンション の合わせ技</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Structured_Attention_Guided_CVPR_2018_paper.html">論文</a></li><li>うまいこと流行りの要素を混ぜ合わせました，という感じがしてしまった</li></ul></div></div><div class="slide_index">[#20]</div><div class="timestamp">2018.7.11 14:20:43</div></div></section><section id="ID_Single_Image_Reflection_Separation_with_Perceptual_Losses"><div class="paper-abstract"><div class="title">Single Image Reflection Separation with Perceptual Losses</div><div class="info"><div class="authors">Xuaner Zhang, Ren Ng, Qifeng Chen</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Takahiro Itazuri</div><div class="item1"><div class="text"><h1>概要</h1><p>本論文では、DNNで単一画像から反射成分と透過成分を分離するタスクを解いている。入力画像Iを反射成分Rと透過成分Tに分離する問題は本来ill-posedな問題であり、従来は様々な前提知識を利用してこの問題を解いていた。近年ではDNNが利用され始めているが、最新の手法であるCEILNetでは低レベルなセマンティクスのみを考慮しているため、十分な精度が出ていなかった。そこで提案手法は高レベルなセマンティクスを考慮することで非常に高品質な分離が可能となった。DNNを学習するにあたって、データセットを構築し、またSoTAの精度を実現した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Single_Image_Reflection_Separation_with_Perceptual_Losses.png" alt="image"></p></div></div><div class="item3"><div class="text"><h1>手法・新規性</h1><p>提案手法におけるネットワークの損失はFeature Loss、Adversarial Loss、Exclusion Lossの３つからなる。Feature Lossは提案ネットワークによって分離した画像と正解画像を深い部分における特徴量の差であり、Adversarial LossはCGANを適用しておいリアルな分離を実現するように学習し、Exclusion Lossは基本的に透過部と反射部は１つのエッジを共有しないという観察を元に勾配空間で透過部と反射部をよりはっきりと分けるように学習する。これらの損失を組み合わせたEnd-to-Endのネットワークを用いることでSoTAを実現した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Single_Image_Reflection_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#21]</div><div class="timestamp">2018.7.11 13:58:38</div></div></section><section id="ID_Attention-Aware_Compositional_Network_for_Person_Re-identification"><div class="paper-abstract"><div class="title">Attention-Aware Compositional Network for Person Re-identification</div><div class="info"><div class="authors">Jing Xu, Rui Zhao, Feng Zhu, Huaming Wang and Wanli Ouyang</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Person Re-identificationにattentionを利用したAttention-Aware Compositional Network(AACN)を提案した。体の部位のocculusionや背景の影響を軽減するために、体のどの部分に注目すればいいかを考慮することで精度の向上を計る。
AACNは、Attentionを得るPose-guided Part Attention(PPA)と特徴を得るAttention-aware Feature Composition(AFC)の2つにより構築される。
PPAは、入力画像からnon-rigid part(腕など)、rigid part(頭など)、key pointの3つの観点からattentionを推定する。
AFCは、PPAにより得られたattentionを考慮した対象人物の特徴量を抽出する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Attention-Aware_Compositional_Network_for_Person_Re-identification.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来の姿勢情報を用いた手法は注目領域に背景などを含んでしまったのに対し、より詳細なattentionを得ることを可能とした。これにより、従来手法と比べあらゆるPerson Re-identificationのデータセットにおいて精度の向上を確認した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>この論文に限らずattentionを用いる論文をよく見る気がする</p><ul><li><a href="https://arxiv.org/abs/1805.03344">論文</a></li></ul></div></div><div class="slide_index">[#22]</div><div class="timestamp">2018.7.11 00:47:05</div></div></section><section id="ID_Weakly_Supervised_Instance_Segmentation_using_Class_Peak_Response"><div class="paper-abstract"><div class="title">Weakly Supervised Instance Segmentation using Class Peak Response</div><div class="info"><div class="authors">Yanzhao Zhou, Yi Zhu, Qixiang Ye, Qiang Qiu, Jianbin Jiao</div><div class="conference">CVPR2018, arXive:1804.00880</div><div class="paper_id">399</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>弱教師あり学習に畳み込み層のレスポンスを使ってセグメンテーションを行う手法であるPeak Response Map(PRM)を提案．手法としては，Class Response Mapという各クラスの特徴マップ(Class Activation Mappingのクラス数枚の特徴マップと同意？)からピークを算出し，そのピーク周辺の勾配を各特徴マップから抽出する事でPeak Response Mapを求める．
そして，このピーク等を用いる事でセグメンテーションを行う．Pascal VOCとCOCOにおいて高い性能を達成している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/399_oveerview.png" alt="399_oveerview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>特徴マップにおける特定のピークと勾配情報を用いる事で，セグメンテーションを可能にしている．また，弱教師あり学習(セグメンテーションラベルなし)によりセマンティックとインスタンスセグメンテーションをラベルなしに認識できるため，評価が高い．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.00880">論文リンク</a></li><li><a href="http://yzhou.work/PRM/">オフィシャルページ(コード等あり)</a></li></ul></div></div><div class="slide_index">[#23]</div><div class="timestamp">2018.7.10 17:23:41</div></div></section><section id="ID_V2V-PoseNet_Voxel-to-Voxel_Prediction_Network_for_Accurate_3D_Hand_and_Human_Pose_Estimation_from_a_Single_Depth_Map"><div class="paper-abstract"><div class="title">V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map</div><div class="info"><div class="authors">Moon, Gyeongsik, Ju Yong Chang, and Kyoung Mu Lee</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Depthマップから手の３次元key pointを検出する手法を提案した。従来手法はdepthマップを２次元画像として扱っているため、2次元への射影時にdistorionが生じる、２次元から３次元への推定は非線形 mappingであるという問題があった。
そこで３次元のボクセルデータから、各ボクセルが３次元のkey pointである確率を推定するV2V-PoseNetを提案した。
２次元のDepthマップをボクセル化することで、V2V-PoseNetによってkey pointを推定する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/V2V-PoseNet_Voxel-to-Voxel_Prediction_Network_for_Accurate_3D_Hand_and_Human_Pose_Estimation_from_a_Single_Depth_Map.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>直接key pointの座標を求める手法と比べ、ボクセル毎の確立を求めることで精度が向上した。具体的には、正解値との誤差、mAPの2つの尺度において従来手法よりも数値的に向上したことを確認した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://github.com/mks0601/V2V-PoseNet_RELEASE">github</a></li></ul></div></div><div class="slide_index">[#24]</div><div class="timestamp">2018.7.10 14:00:42</div></div></section><section id="ID_Image_Collection_Pop-up_3D_Reconstruction_and_Clustering_of_Rigid_and_Non-Rigid_Categories"><div class="paper-abstract"><div class="title">Image Collection Pop-up: 3D Reconstruction and Clustering of Rigid and Non-Rigid Categories</div><div class="info"><div class="authors">A. Agudo, M. Pijoan, F. Moreno-Noguer</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>部分的に2Dアノテーションされた複数インスタンスの画像データセットにおいて，3D形状，カメラ姿勢，物体，変形のタイプのクラスタリングを同時に行う．
また，不明瞭(indistinctly)に剛体・非剛体カテゴリ分類を行う．
これは，クラスタが事前知識であるような既存手法の拡張となる．</p><p>物体変形のモデリングを行う．小さい領域の動きを，複雑な変形へと橋渡しできるように，
サブスペーススの複数ユニオンに基づく定式化を行う．
このモデルのパラメータは拡張ラグランジュマルチプライヤーで学習する．
完全に教師無しで行え，学習データが不要である．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Image_Collection_Pop-up_3D_Reconstruction_and_Clustering_of_Rigid_and_Non-Rigid_Categories.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>剛体，非剛体カテゴリ，小さい・大きい変形を含む合成データ，実データセットで検証し，3D復元においてSoTA．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>※拡張ラグランジュ関数は条件を満たすと真凸関数．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1203.pdf">論文</a></li></ul></div></div><div class="slide_index">[#25]</div><div class="timestamp">2018.7.10 12:22:13</div></div></section><section id="ID_NeuralNetwork-Viterbi_A_Framework_for_Weakly_Supervised_Video_Learning"><div class="paper-abstract"><div class="title">NeuralNetwork-Viterbi: A Framework for Weakly Supervised Video Learning</div><div class="info"><div class="authors">A. Richard, H. Kuehne, A. Iqbal and J. Gall</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>弱教師付き動画学習に，ビタビ復号を組み込んでみた話．タスクはアクションセグメンテーション．
用意するのは動画とそのアクションラベルだけ．</p><p>動画がネットワークに入力され，その出力された確率分布に対しビタビ復号を実行する．すると，フレームラベルがビタビ復号で生成される．そして，勾配計算時にフレームワイズのクロスエントロピー計算を行い，逆伝播する．</p><p>明示的なコンテキスト・長さのモデリングが，これがビデオセグメンテーション・ラベリングタスクの改善に大きく作用することも示す．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/NeuralNetwork-Viterbi_A_Framework_for_Weakly_Supervised_Video_Learning.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>アクションセグメンテーションでSoTA．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>勉強していないと知らなそうなアルゴリズムの導入．だいぶ込み入った話をしに行かないと論文が通らなくなってきた？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1203.pdf">論文</a></li></ul></div></div><div class="slide_index">[#26]</div><div class="timestamp">2018.7.10 11:10:38</div></div></section><section id="ID_Eliminating_Background-bias_for_Robust_Person_Re-identification"><div class="paper-abstract"><div class="title">Eliminating Background-bias for Robust Person Re-identification</div><div class="info"><div class="authors">M. Tian, S. Yi, H. Li, S. Li, X. Zhang, J. Shi, J. Yan and X. Wang</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>人物再同定の話．人の領域で丁寧にバウンディングボックスを切ったとしても，やはり背景は映り込んでいて，背景バイアスは免れない．
この事実を，以前作成した人領域をピクセルレベルでセグメンテーションして作ったデータセットで検証した．</p><p>そして，背景バイアス問題を解決すべく，3つのパーツに分ける人パージングマップに基づき，人領域をガイドとしたプーリングを行うDNNを構成．</p><p>また，人画像とランダム背景を合成するという，トレーニングデータのオーギュメンテーション手法も提案．背景画像は監視カメラ映像のフレームから100枚選び，対象の人画像と同じ大きさの背景画像をランダムにオンラインでクロップし，人画像とマージ．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Eliminating_Background-bias_for_Robust_Person_Re-identification.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>背景バイアスに関する調査と，それを低減できる人物再同定DNNの提案．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>人領域を自分たちで色塗りしたデータセットを作る力業ができるSensetime x CUHK．</p><p>Action recognition without humanは引用してくれなかった．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1989.pdf">論文</a></li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1036.pdf">同じデータセットによる別の論文</a></li></ul></div></div><div class="slide_index">[#27]</div><div class="timestamp">2018.7.10 10:57:36</div></div></section><section id="ID_Weakly_Supervised_Facial_Action_Unit_Recognition_Through_Adversarial_Training"><div class="paper-abstract"><div class="title">Weakly Supervised Facial Action Unit Recognition Through Adversarial Training</div><div class="info"><div class="authors">Guozhu Peng, Shangfei Wang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>顔表情の基本構成であるアクションユニット（AU; Action Unit）を弱教師により敵対的学習する論文である。最初に擬似ラベルによりAUを推定し、敵対的学習の枠組みにより高精度にAUを認識できるようにしていく。敵対的学習はAUの認識を行うRと、AUラベルかどうかを見分けるDから構成される（つまり認識した擬似ラベルが本物のラベルかどうか見間違うように学習を進めていく）。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180709WeaklySupervisedFacialActionUnit.png" alt="180709WeaklySupervisedFacialActionUnit"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ラベルづけが困難なタスクである顔表情のアクションユニットに対して効果的なアルゴリズムを提案し、弱教師付き学習ができるようにした。GANの枠組みを改良し、擬似ラベルを正解として十分にするよう学習できたことが大きな貢献である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>擬似ラベルでも使用可能なレベルに持っていく学習はSelf-Supervised Learningでも使われているし、最初は粗いラベルでも徐々に意味のある教師になっていく様子が確認できる。アイディアは世界で同時多発的に思いついて実装が行われるので、思いついたらすぐにやらないといけない。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Peng_Weakly_Supervised_Facial_CVPR_2018_paper.html">論文</a></li></ul></div></div><div class="slide_index">[#28]</div><div class="timestamp">2018.7.9 12:40:17</div></div></section><section id="ID_A_Causal_And-Or_Graph_Model_for_Visibility_Fluent_Reasoning_in_Tracking_Interacting_Objects"><div class="paper-abstract"><div class="title">A Causal And-Or Graph Model for Visibility Fluent Reasoning in Tracking Interacting Objects</div><div class="info"><div class="authors">Yuanlu Xu, Lei Qin, Xiaobai Liu, Jianwen Xie, Song-Chun Zhu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>与えられた人物トラッキングやアピアランス情報から人物/物体間のインタラクション認識（ここではVisibility Fluent Reasoningと呼ばれている）を行う。ここで、通常人物や物体のトラッキングは欠損を含むことが多く、途切れ途切れになっている状態からでも認識ができるようにCausal And-Or Graph（C-AOG）を適用して対応関係を学ぶようにする。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180709VisibilityFluentReasoning.png" alt="180709VisibilityFluentReasoning"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>C-AOGを用いて時間軸に伴うイベントの変化を理解することに成功、物体トラッキングと変化の理由づけを同時に行なっている。オクルージョン時の対応（トラッキングが一部できなくなっている）が行われたデータセットも公開し、より複雑かつ情報の欠損を含む環境においてもFluent Reasoningができるようにした。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>「ビジョンの認識精度は完璧ではない」という前提でより上位のタスクを完結するデータは今後さらに重要！査読に対する理解（完璧でないなら減点するといったことをなくす）も広がってほしい。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Xu_A_Causal_And-Or_CVPR_2018_paper.html">論文</a></li></ul></div></div><div class="slide_index">[#29]</div><div class="timestamp">2018.7.9 11:33:11</div></div></section><section id="ID_Facial_Expression_Recognition_by_De-Expression_Residue_Learning"><div class="paper-abstract"><div class="title">Facial Expression Recognition by De-Expression Residue Learning</div><div class="info"><div class="authors">Huiyuan Yang, Umur Ciftci, Lijun Yin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>顔表情認識を行うために、De-expression（Happy=>Neutralのように顔表情を打ち消す）を学習することにより特徴表現能力を向上させる。De-expression Residue Learning（DeRL）とよばれる、生成的/識別的な誤差計算を同時に学習可能な枠組みを提案（右図）。DeRLではまずConditional GANによりある表情の顔を無表情の顔に生成するモデルを構築。従来ではピクセルレベル/特徴レベルの違いを見分けていたが、本論文では生成モデルにおける中間層レベルの違いを見分けることにより高精度な表情認識モデルが出来上がる。このうち、Encoder/Decorderの2,3,4,5層、最終識別結果においても誤差を計算。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180709DeexpressionResidueLearning.png" alt="180709DeexpressionResidueLearning"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>顔表情認識に関して、表情を打ち消すための識別/生成的モデルから誤差を計算するDe-expression Residue Learning（DeRL）により学習を行なった。BU-4DFE/BP4D-spontaneousと2つのデータセットにより事前学習を行い、CK+/Oulu-CASIA/MMI/BU-3DFE/BP4D+にてテストを行なった結果、従来法を超える顔表情認識精度を達成した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>生成的に顔表情を打ち消す（教師なし）学習が有効とは発想勝ちである。また、それでうまくいく実装力も評価できる。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Facial_Expression_Recognition_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#30]</div><div class="timestamp">2018.7.9 09:52:16</div></div></section><section id="ID_3D_Human_Sensing_Action_and_Emotion_Recognition_in_Robot_Assisted_Therapy_of_Children_With_Autism"><div class="paper-abstract"><div class="title">3D Human Sensing, Action and Emotion Recognition in Robot Assisted Therapy of Children With Autism</div><div class="info"><div class="authors">Elisabeta Marinoiu, Mihai Zanfir, Vlad Olaru, Cristian Sminchisescu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>児童心理ケアのシーンにおいて3D次元姿勢推定、行動認識、感情推定を実施した。長期の動画撮影、多様な行動、部分的にしか身体が映っていない、児童の年齢が異なる、などの課題があるが、このような環境にて上記タスクを行なった。詳細行動/感情認識（fine-grained action, emotion recognition）を行うために3,700動画を撮影（各動画は10-15分の長さを保有）、37人の児童から19の頻出行動を分類。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180709PoseActionEmotion.png" alt="180709PoseActionEmotion"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>提案手法は姿勢推定においてKinectとcompetitiveな精度を実現するとともに、行動認識や感情推定では良好な精度を実現、Child-Robot Interactionに関する新しいタスクを定義した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Child-Robot Interaction、新しいタスクである。大人とは異なり、子供の行動/感情は年齢により大きく異なりそうである。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Marinoiu_3D_Human_Sensing_CVPR_2018_paper.html">論文</a></li><li><a href="http://vision.imar.ro/de-enigma/">Project</a></li></ul></div></div><div class="slide_index">[#31]</div><div class="timestamp">2018.7.9 09:07:24</div></div></section><section id="ID_Monocular_3D_Pose_and_Shape_Estimation_of_Multiple_People_in_Natural_Scenes_-_The_Importance_of_Multiple_Scene_Constraints"><div class="paper-abstract"><div class="title">Monocular 3D Pose and Shape Estimation of Multiple People in Natural Scenes - The Importance of Multiple Scene Constraints</div><div class="info"><div class="authors">Andrei Zanfir, Elisabeta Marinoiu, Cristian Sminchisescu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Deep Multi-task Neural Networksにより複数人物の3次元姿勢+形状を推定する。直接的に画像のアピアランスから人物姿勢を推定するのみならず、環境の拘束条件や推定された関節情報からコンセンサスを取るように文脈を把握しながら（2次元や）3次元の姿勢+形状を決定していく。ビデオに拡張することも可能で、さらに自然環境下における高精度な人物姿勢推定も実行した。右図は処理フローを示す。初期段階では単一人物の姿勢推定と推定結果のフィードバックを行い、次に複数人物同時最適化を行い、最終的な複数人物の3次元姿勢とその形状を取得する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180709Mono3DPoseShapeEstimation.png" alt="180709Mono3DPoseShapeEstimation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>高精度に複数人物の3次元姿勢を推定するとともにその形状も復元可能にした点が貢献点である。さらに、モデルにおいても単一人物/複数人物/環境に関する拘束条件など文脈を把握することにより3次元姿勢や形状を推定した点にも新規性が認められた。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>単眼カメラからのモーキャプまでもう少し？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zanfir_Monocular_3D_Pose_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#32]</div><div class="timestamp">2018.7.9 08:46:45</div></div></section><section id="ID_Inferring_Semantic_Layout_for_Hierarchical_Text-to-Image_Synthesis"><div class="paper-abstract"><div class="title">Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis</div><div class="info"><div class="authors">Seunghoon Hong, Dingdong Yang, Jongwook Choi, Honglak Lee</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://aizawan.github.io/">Hiroaki Aizawa</a></div><div class="item1"><div class="text"><h1>概要</h1><p>テキストからの画像生成において、テキストから画像への写像を直接学習するのではなく、layout generatorよりtextから中間表現としてsemantic layoutを生成するステップと、image generatorによりそれを画像へ変換するステップに分解して画像を生成する枠組みを提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Inferring_Semantic_Layout_for_Hierarchical_Text-to-Image_Synthesis_1.PNG" alt="Inferring_Semantic_Layout_for_Hierarchical_Text-to-Image_Synthesis_1.PNG"><img src="slides/figs/Inferring_Semantic_Layout_for_Hierarchical_Text-to-Image_Synthesis_2.PNG" alt="Inferring_Semantic_Layout_for_Hierarchical_Text-to-Image_Synthesis_2.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>意味のある画像をsemantic layoutに基づき生成する点だけでなく、生成画像のアノテーションも自動で行われている点と生成されたsemantic layoutを修正することによるユーザーがコントロールできる生成も可能にしている点が新しく有用である。StackGANのような鳥や花といった特定対象ではなく、より複雑な一般シーンを想定し、Fine-grained semantic layoutが必要であるという問題設定が良い。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>評価の際に、生成された画像のcaptionを生成し、元の文章との類似度を比較しており、納得できる生成モデルの評価をしていた。StackGANでは行われていなかった気がするが、こういった評価は普通？また画像生成等の中間表現としてSemantic layoutを利用する研究が増えてきた。それゆえ物体の形状とインスタンス情報(この研究で言うところのBox generatorとshape generator)をよりスマートに取得または統合できればと感じる。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Hong_Inferring_Semantic_Layout_CVPR_2018_paper.html">paper</a></li></ul></div></div><div class="slide_index">[#33]</div><div class="timestamp">2018.7.9 06:58:04</div></div></section><section id="ID_Referring_Image_Segmentation_via_Recurrent_Refinement_Networks"><div class="paper-abstract"><div class="title">Referring Image Segmentation via Recurrent Refinement Networks</div><div class="info"><div class="authors">Ruiyu Li, Kaican Li, Yi-Chun Kuo, Michelle Shu, Xiaojuan Qi, Xiaoyong Shen, Jiaya Jia</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://aizawan.github.io/">Hiroaki Aizawa</a></div><div class="item1"><div class="text"><h1>概要</h1><p>自然言語に基づいてsegmentationするタスク(referring image segmentation)においてmulti-scaleなsemantic情報を取得するRecurrent Refinement Network(RRN)を提案。これは入力にPyramid特徴からの得られる情報を適応的に組み込み、segmentation maskを洗練する。実験では、ReferIt、UNC、UNC+、G-RefのデータセットでベースラインとSoTAより性能が優れていることを確認。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Referring_Image_Segmentation_via_Recurrent_Refinement_Networks.PNG" alt="Referring_Image_Segmentation_via_Recurrent_Refinement_Networks.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Referring image segementationへmulti-scaleなsemantic情報を含むpyramid特徴を適用し、単純に利用するのではなく、ConvLSTMにより洗練化している点が新しい。そして4つのデータセットでSoTAの性能を達成。包括的な実験により、RRNの有効性を示している。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>ここでもPyramid特徴が利用され、有効性が示されている。Referring image segmentationにおいて、ConvLSTMにおいてtanhを利用すると大幅な精度改善がみられるのが不思議である。個人的な意見として、maskの生成のためのRefinementに再帰構造を利用するのは自然であり、加えて本論文の結果よりLSTMには、multi-scaleの特徴を適応的に追加・削除する機能があり、これがsegmentation maskに良い影響を与えている点がわかる。興味深い。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Li_Referring_Image_Segmentation_CVPR_2018_paper.html">paper</a></li><li><a href="https://github.com/liruiyu/referseg_rrn">GitHub</a></li></ul></div></div><div class="slide_index">[#34]</div><div class="timestamp">2018.7.9 06:00:21</div></div></section><section id="ID_DenseASPP_for_Semantic_Segmentation_in_Street_Scenes"><div class="paper-abstract"><div class="title">DenseASPP for Semantic Segmentation in Street Scenes</div><div class="info"><div class="authors">Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, Kuiyuan Yang</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://aizawan.github.io/">Hiroaki Aizawa</a></div><div class="item1"><div class="text"><h1>概要</h1><p>自動走行のシーンで現れる物体はスケールの変動が大きく、multi-scaleな情報を適切にEncodeする必要がある。multi-scaleなsemantic情報を抽出するために、複数rateのAtrous ConvolutionによるAtrous Spatial Pyramid Pooling(ASPP)が提案されているが、このような自動走行のシーンではまだ十分ではない。そこで、よりスケールの変動に対応するために、Densely connected Atrous Spatial Pyramid Pooling(DenseASPP)を提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DenseASPP_for_Semantic_Segmentation_in_Street_Scenes.PNG" alt="DenseASPP_for_Semantic_Segmentation_in_Street_Scenes.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ASPPのように、Dilation rateを上げると画素のsampling間隔が広がる。これは大きいストライドのconvolutionのようなもので、大きなrateのatrous convolutionは受容野を広げるが、その分情報の欠落が起こる(低密度化)。この問題を解決すべくStackしかつ密な結合をしたDenseASPPにより高密度化し、異なるdilation rateのlayerの多様なアンサンブルを可能とすることで、ASPPよりも多くのスケールを持つ特徴マップを効果的に得ることができる。これが新しい。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Dense結合 + Pyramid特徴の単純な構造に思えるが、ASPPでのAtrous Convolutionの隙間に着目し、これを効果的に高密度化していることがおもしろい。semantic segmentationにおいて大小様々なスケールの変動への対応策は、最終段へ伝播できるパスが存在するかが重要？これって結局multi-scaleなpyramid特徴？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.html">paper</a></li><li><a href="https://github.com/DeepMotionAIResearch/DenseASPP">GitHub</a></li></ul></div></div><div class="slide_index">[#35]</div><div class="timestamp">2018.7.9 05:52:39</div></div></section><section id="ID_On_the_Importance_of_Label_Quality_for_Semantic_Segmentation"><div class="paper-abstract"><div class="title">On the Importance of Label Quality for Semantic Segmentation</div><div class="info"><div class="authors">Aleksandar Zlateski, Ronnachai Jaroensri, Prafull Sharma, Frédo Durand</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://aizawan.github.io/">Hiroaki Aizawa</a></div><div class="item1"><div class="text"><h1>概要</h1><p>この論文ではcityscapeライクなcoarseラベルでの性能を人工データを使って、Semantic Segmentationでのラベルの品質とCNNの性能との関係を調査した研究。これにより、人間の労力を最小化しつつ、coarseラベルを作るべき時間を提案することができる。ラベル品質とあるが、domain adaptation等の手法によるラベル生成の品質検証というわけではなく、人間の労力は前提で、その上でのcoarseラベルの品質と性能を検証している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/On_the_Importance_of_Label_Quality_for_Semantic_Segmentation.PNG" alt="On_the_Importance_of_Label_Quality_for_Semantic_Segmentation.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>結果から、CNNの性能は人間のアノテーションコストに依存することがわかった。これつまり、大きなcoarseアノテーションデータセットは、小さなfineアノテーションデータセットの性能と同等で、coarseラベルでpretrainし、少ないfineアノテーションデータセットでfine-tuneした場合、大きなfineデータセットで学習した性能に匹敵またはそれ以上の性能を得ることができる可能性があることを示している。また様々なネットワーク構造や都市の様々なオブジェクトに対しても有効であることを証明。</p></div></div><div class="item4"><div class="text"><p>ここではcoarseラベルを対象としていたが、ミスラベルの場合は？、汎化との関係は？、stuffクラスは？と異なる対象でさらなる検証がほしいと思わせる研究。これらについて検証した研究がもうすでにあったりする？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Zlateski_On_the_Importance_CVPR_2018_paper.html">paper</a></li></ul></div></div><div class="slide_index">[#36]</div><div class="timestamp">2018.7.9 05:43:49</div></div></section><section id="ID_A_Memory_Network_Approach_for_Story-based_Temporal_Summarization_of_360_Videos"><div class="paper-abstract"><div class="title">A Memory Network Approach for Story-based Temporal Summarization of 360◦ Videos</div><div class="info"><div class="authors">Sangho Lee, Jinyoung Sung, Youngjae Yu, Gunhee Kim</div><div class="conference">CVPR2018, arXive:1805.02838</div><div class="paper_id">170</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>360°カメラの動画を用いたビデオ要約を，Memory NetworkをベースとしたPast-Future Memory Networkにより実現した研究．はじめに，入力の360°の動画から81個の領域(normal field of view)を，RankNetベースの手法を用いて切り出す．
候補領域は，MemoryNetのMemoryへと記憶される．
PFMNでは，これらの候補領域を過去と将来という形でMemoryに記憶しており，時刻tで最もスコアが高い記憶が過去のMemoryに残される．
印象の強い候補領域を残しつつMemoryをアップデートしていくことで，高性能なビデオ要約が可能となる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/170_overview.png" alt="170_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>この手法では，対象を360°カメラの動画としており，広大な情報量から効率的に印象的なシーンをMemory Networkを活用することで，高性能な成果を出している．Memory Networkをこのような問題設定に応用した事例はこの手法が初めてであり，この点が高い新規性となっている．
また，このタスクを評価する指標として，新たなデータセット360◦ video summarization datasetを提案している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1805.02838">論文リンク</a></li><li><a href="http://vision.snu.ac.kr/projects/pfmn/">オフィシャルページ</a></li></ul></div></div><div class="slide_index">[#37]</div><div class="timestamp">2018.7.9 02:14:01</div></div></section><section id="ID_BlockDrop_Dynamic_Inference_Paths_in_Residual_Networks"><div class="paper-abstract"><div class="title">BlockDrop: Dynamic Inference Paths in Residual Networks</div><div class="info"><div class="authors">Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S. Davis, Kristen Grauman, Rogerio Feris</div><div class="conference">CVPR2018, arXive:1711.08393</div><div class="paper_id">1213</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>強化学習を使い，推論時のResNetの不必要な層(ブロック)を取り除いて計算コストを削減するBlockDropを提案．この研究では，ResNetが特定の層を取り除いた際に性能があまり低下しない能力を利用しており，どのブロックを落とせるかをPolicy Networkにより判定させている．
報酬の設計では，画像認識時により少ないブロックで認識が成功できるほど報酬が高くなるように設計されている．
BlockDropにより，ImageNetにおいてtop-1の性能を76%を保ちつつ，平均で20%の高速化(一部では36%高速化)を実現している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1213_overview.png" alt="1213_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>推論時のネットワーク構造を強化学習により最適化させる手法．強化学習によりネットワーク構造を削減する手法はあまり提案されていないため，新規性が高く評価されたと思われる．
また，BlockDropでは速度を改善するだけでなく，場合によっては若干性能を向上させる事が可能である事を示している(CIFAR, ImageNetで検証)．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>強化学習の新しい使い方で非常に面白い手法．今後，改善や応用が期待できそう．</p><ul><li><a href="https://arxiv.org/abs/1711.08393">論文リンク</a></li><li><a href="https://github.com/Tushar-N/blockdrop">コードリンク</a></li></ul></div></div><div class="slide_index">[#38]</div><div class="timestamp">2018.7.9 02:18:31</div></div></section><section id="ID_CondenseNet_An_Efficient_DenseNet_using_Learned_Group_Convolutions"><div class="paper-abstract"><div class="title">CondenseNet: An Efficient DenseNet using Learned Group Convolutions</div><div class="info"><div class="authors">Gao Huang, Shichen Liu, Laurens van der Maaten, Kilian Q. Weinberger</div><div class="conference">CVPR2018, arXive:1711.09224</div><div class="paper_id">350</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>DenseNetをベースにコンパクトなネットワークを構築するCondenseNetを提案．このCondenseNetは，学習中は更新回数が増えるに連れて畳み込む特徴マップを減らしていく．
そして，推論時は疎になった畳み込み層の特徴マップを入れ替え，Group Convolutionする．
これにより，畳み込みに対する処理時間を大幅に削減する事が可能であり，推定時の計算コストを大幅に削減する事ができる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/350_overview.png" alt="350_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>コンパクトなネットワークを構築するために，学習では畳み込みをスパースにする処理を導入し，推論時には特定の特徴マップを畳み込むようにGroup Convolutionを導入している．このような畳み込みの最適化方法は提案されていないため，新規性として高い．
また，DenseNetの構造も改良しており，複数種類のプーリングを使用する等の改良も導入している．
同会議で提案されているShuffleNetよりコンパクトにする事ができる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.09224">論文リンク</a></li><li><a href="https://github.com/ShichenLiu/CondenseNet">コードリンク</a></li></ul></div></div><div class="slide_index">[#39]</div><div class="timestamp">2018.7.9 02:23:06</div></div></section><section id="ID_Cube_Padding_for_Weakly-Supervised_Saliency_Prediction_in_360_Videos"><div class="paper-abstract"><div class="title">Cube Padding for Weakly-Supervised Saliency Prediction in 360◦ Videos</div><div class="info"><div class="authors">Hsien-Tzu Cheng, Chun-Hung Chao, Jin-Dong Dong, Hao-Kai Wen, Tyng-Luh Liu, Min Sun</div><div class="conference">CVPR2018, arXive:1806.01320v1</div><div class="paper_id">171</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>360°カメラの動画から弱教師あり学習でSailency mapを効率的に求める方法を提案．方法として，360°のシーンを6つのパネルに分割し，チャンネル方向に結合する事で，ネットワークに入力する．
ここで，シーンをパネルに分割する際にCube Paddingという方法を提案しており，特定パネルの周囲のパネルの一部を，その特定パネルの両端に結合させる．
これにより，パネル間の関連性をネットワークに学習させる事が可能である．
また，360°シーンのデータセットを新たに提案している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/171_overview.png" alt="171_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>提案しているCube Paddingという広大なシーンに特化した入力方法は，解像度が高い場合においても処理速度の低下を抑制する事が可能である．また，パネルを分割する際にCube Paddingを導入する事で，パネル間の境界に対してロバストにする事ができる．
今回のタスクに対して新しいデータセット”Wide-360° Dataset”を提案している点も，評価が高い．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>新たな問題設定にチャレンジした研究．そして，結果の見せ方が凄く良い．(特にオフィシャルページの360°のYouTubeを使った動画デモ)</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Cheng_Cube_Padding_for_CVPR_2018_paper.html">論文リンク</a></li><li><a href="http://aliensunmin.github.io/project/360saliency/">オフィシャルページ</a></li></ul></div></div><div class="slide_index">[#40]</div><div class="timestamp">2018.7.9 02:08:03</div></div></section><section id="Finding_beans_in_burgers_Deep_semantic-visual_embedding_with_localization"><div class="paper-abstract"><div class="title">Finding beans in burgers: Deep semantic-visual embedding with localization</div><div class="info"><div class="authors">Martin Engilberge, Louis Chevallier, Patrick Pérez, Matthieu Cord</div><div class="conference">CVPR2018, arXive:1804.01720</div><div class="paper_id">522</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>マルチモーダルに任意の領域を高精度にローカライズする研究．この研究では画像 & テキストを対象としており，右図のように入力されたテキストに適合した領域をヒートマップで推定している．
画像特徴とテキスト特徴を同一空間に落とし込んでネットワークを学習する．
そして，認識時にテキストの特徴ベクトルと画像の特徴マップを使ってヒートマップを出力する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/522_overview.png" alt="522_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>方法としては，画像と単語からResNetとRNNを用いて特徴マップ / 特徴ベクトルを抽出し，同一特徴空間にembeddingさせる．学習では，画像とテキストの特徴からTriplet Ranking Lossを用いて学習させる．
ヒートマップは，画像の特徴マップと文章の特徴ベクトルの掛け合わせから求めることができる．
このローカライゼーションは，非常に高い性能を達成している．また，Zero-shot Learningにも応用できる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.01720">論文リンク</a></li></ul></div></div><div class="slide_index">[#41]</div><div class="timestamp">2018.5.20 19:39:22</div></div></section><section id="ID_Learning_Answer_Embeddings_for_Visual_Question_Answering"><div class="paper-abstract"><div class="title">Learning Answer Embeddings for Visual Question Answering</div><div class="info"><div class="authors">Hexiang Hu, Wei-Lun Chao and Fei Sha</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>VQAの質問と画像、答えそれぞれを表現するembeddingを学習する手法を提案。従来のVQAは、任意の文章を答えとして出すものと用意された選択肢の中から選択するものの２種類に分けることができる。
前者は答えが合っているか否かは主観的なものである、後者は選択肢に含まれない答えを出力できない、runningとjoggingのように似ている単語の区別が難しいといった問題がある。
そこで質問と画像のペア、答えそれぞれを表現するベクトルを学習することで答え同士の類似度の定義や未知の答えへの対応を可能にする。
具体的には、それぞれのベクトルを用いた確率モデルを構築し、最尤推定を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_Answer_Embeddings_for_Visual_Question_Answering.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来手法では学習の際に設定した答えのみしか出力できず、異なるデータセットに適用することが不可能であったが、提案手法により異なるデータセットなどデータセットに含まれていない答えにも適用可能となった。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://link.com/link1/">論文</a></li></ul></div></div><div class="slide_index">[#42]</div><div class="timestamp">2018.7.9 00:41:29</div></div></section><section id="ID_Structure_Inference_Net_Object_Detection_Using_Scene-Level_Context_and_Instance-Level_Relationships"><div class="paper-abstract"><div class="title">Structure Inference Net: Object Detection Using Scene-Level Context and Instance-Level Relationships</div><div class="info"><div class="authors">Yong Liu, Ruiping Wang, Shiguang Shan and Xilin Chen</div><div class="conference">CVPR2018</div><div class="paper_id">876</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像のシーンコンテキストと，物体の関係の2種類のコンテキストを用いて物体検出を行うアルゴリズムを提案．物体検出をグラフ構造の推論問題として扱い，物体をノード，物体間の関係をエッジとしてモデル化する．これを実現するために，Faster R-CNNのような物体検出フレームワークに組み込む構造推論ネットワーク（Structure Inference Network；SIN）を設計した．SINは，特徴マップとしてプールされたRoIをノードとしてFC層にマッピングする．同様に画像全体の特徴をシーンとして抽出し，RoIを連結してエッジとする．グラフは反復的に更新され，最終状態は物体クラス予測の精度向上に貢献する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180708_SIN1.jpg" alt="20180708_SIN1.jpg"><img src="slides/figs/20180708_SIN2.jpg" alt="20180708_SIN2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>物体検出の精度向上のためにコンテキスト（周辺環境，物体の位置関係など）の理解が重要となる．コンテキストをグラフ構造で表して推論する斬新な手法である．VOCとCOCOで評価を行い，一部のクラスはFaster R-CNNよりも高性能であり，全体では76.0mAP（VOC07），73.1mAP（VOC12）とFaster R-CNN（73.2，70.4）よりも高性能であることを示した．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>グラフ構造で物体検出を扱うものはいくつかあるが，エッジの情報と周辺環境のコンテキストも考慮したものは新しい．コンテキストを考慮した物体検出は，未知の物体を検出するためにも重要な要素となり得る？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2114.pdf">論文</a></li></ul></div></div><div class="slide_index">[#43]</div><div class="timestamp">2018.7.8 11:15:43</div></div></section><section id="ID_Deep_Ordinal_Regression_Network_for_Monocular_Depth_Estimation"><div class="paper-abstract"><div class="title">Deep Ordinal Regression Network for Monocular Depth Estimation</div><div class="info"><div class="authors">Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich and Dacheng Tao</div><div class="conference">CVPR2018</div><div class="paper_id">231</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>DCNNを用いてdepth画像を推定するとき，depthを離散化して順序回帰問題として解くdeep ordinal regression network（DORN）を提案．Depthの離散化にはspacing-increasing discretization（SID）を導入した．SIDを用いてログスケールで離散化することで，遠い領域のdepth画像を粗く，手前の領域のdepth画像を細かく離散化してロスの減少に貢献する．ネットワークの構成は高解像度な特徴抽出部，マルチスケール特徴学習器（ASPP），フル画像エンコーダおよび順序回帰optimizerからなる．計算コストを削減するために，skip connectionではなくシンプルな構成を採用した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180708_DORN1.jpg" alt="20180708_DORN1.jpg"><img src="slides/figs/20180708_DORN2.jpg" alt="20180708_DORN2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>DCNNを用いた高解像度なdepth画像推定は，通常skip connectionや複数のdeconv層が必要だったが，この問題を解決または低減した．KITTI，Make3D，NYU Depth v2などのベンチマークで他の手法を大きく上回りSOTAを達成した．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>単純なログスケールでの離散化だけでなく，depth値の出現頻度を考慮した離散化を行えばより高精度化できそう．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3454.pdf">論文</a></li></ul></div></div><div class="slide_index">[#44]</div><div class="timestamp">2018.7.8 07:43:29</div></div></section><section id="ID_Translating_and_Segmentating_Multimodal_Medical_Volumes_with_Cycle-_and_Shape-Consistency_Generative_Adversarial_Network"><div class="paper-abstract"><div class="title">Translating and Segmentating Multimodal Medical Volumes with Cycle- and Shape-Consistency Generative Adversarial Network</div><div class="info"><div class="authors">Zizhao Zhang, Lin Yang, Yefeng Zheng</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Takahiro Itazuri</div><div class="item1"><div class="text"><h1>概要</h1><p>医療画像処理ではCTやMRIなどの異なった種類のデータが存在する。医療の現場において、CTとMRIはどちらも必要となる場面がある一方で、どちらか一方しかデータが存在しないことも多々発生している。そこで本論文では、CTとMRIという3D画像データ間のドメイン変換を行うタスクに取り組んだ。またCTとMRIのそれぞれからセグメンテーションを行うネットワークも学習させた。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Traslating_and_Segmentating_Multimodal_Medical_Volumes_with_Cycle_and_Shape_Consistency_Generative_Aversarial_Network.png" alt="img"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>2D画像におけるImage-to-Image Translationに対応する、医療3D画像におけるVolume-to-Volume Translationに対して以下の点に取り組んだ。</p><ul><li>画像ペアがないデータセットにおける学習を行った点</li><li>解剖学的構造に矛盾が出ないようにした点</li><li>合成画像を利用して、セグメンテーションの精度を向上させた点</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Translating_and_Segmenting_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#45]</div><div class="timestamp">2018.7.6 18:22:51</div></div></section><section id="Learning_Compression_Algorithms_for_Neural_Net_Pruning"><div class="paper-abstract"><div class="title">“Learning-Compression” Algorithms for Neural Net Pruning</div><div class="info"><div class="authors">Miguel et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://twitter.com/tomoyukun">Tomoyuki Suzuki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Pruningを最適化問題として定式化し、交互最適化によって解くLC algorithmの提案。定式化としては0をとらないパラメータ数に対して制約を設けて解くConstrain formとそれを罰則項として損失関数に組み込むPenalty formの二つを提案。メジャーなPruning手法であるパラメータのmagnitudeの小さいものをナイーブにzeroingしていくものよりも、良い結果となった。提案する2つのformに関してはConstrain formの方が良かった。</p></div></div><div class="item2"><img src="slides/figs/Learning_Compression_Algorithms_for_Neural_Net_Pruning.png" alt="Learning_Compression_Algorithms_for_Neural_Net_Pruning.png"></div><div class="item3"><div class="text"><h1>詳細</h1><p>補助パラメータのPruningを行うCompression(C) Stepと本パラメータを補助パラメータに近づけつつ本タスク（識別・回帰など）を学習するLearning Stepからなる。C Stepでは（制約 or 罰則項として） Lp正則をかけながら本パラメータとのMSEを最小化するような補助パラメータを探索する。L Stepでは損失関数における補助パラメータとのMSE項の係数を学習の進行に応じて大きくすることで（μ→∞）、最終的な解がスパースなものに近づく。また、Constrain formでは超パラメータ一つでNN全体において最適化できる。手法の新規性・妥当性が大きく評価されたと考えられる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>magnitudeベースのものは「 magnitude が小さいものは推定への寄与率が低い」という仮定のみでPruningしていくが、この手法ではその仮定をベースにしつつ(C step)、本タスクの性能を担保しながらPruningしていく(L step)点で理にかなっているように思え、面白い。計算効率をモチベーションにされることが多いPruning研究だが、枝刈りの割合によってはLasso回帰のように汎化性能が向上するような地点がないかもきになる。</p><ul><li><a href="http://faculty.ucmerced.edu/mcarreira-perpinan/papers/cvpr18.pdf">論文</a></li></ul></div></div><div class="slide_index">[#46]</div></div></section><section id="ID_DeLS-3D_Deep_Localization_and_Segmentation_with_a_3D_Semantic_Map"><div class="paper-abstract"><div class="title">DeLS-3D: Deep Localization and Segmentation with a 3D Semantic Map</div><div class="info"><div class="authors">Peng Wang, Ruigang Yang, Binbin Cao, Wei Xu, Yuanqing Lin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>GPS IMU，RGBビデオカメラ及び3Dセマンティックマップからカメラ姿勢・自己位置推定及びscene parsingを同時に行えるフレームワークの提案．</li><li>提案フレームワークの概要は:①GPS/IMU及びrenderredセマンティックマップから初期なカメラ姿勢を推定する．② renderredセマンティックマップとRGB画像をpose推定ネットワークに入力し，精密なカメラ姿勢を推定する．またRNNにより更に姿勢推定を精密化する．③推定した精密なカメラ姿勢で新たなセマンティックマップをrenderし， renderredマップとRGB画像を更にsegment CNNによりピクセルレベル精度のセマンティックマップを推定する．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DeLS-3D.png" alt="DeLS-3D"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>GPS IMU，RGBカメラ，3Ｄセマンティックマップのマルチセンサーの情報をDNNにより有効的かつロバストでに融合できる．</li><li>カメラ姿勢推定とScene parsingの2つのタスクを同時に行うことにより，各々で行える場合より良い精度を得られることを実験に通して示した．</li><li>gtカメラ姿勢，denseなセマンティックラベル付きのポイントクラウド及びピクセルレベル精度のビデオカメラ画像の室外運転用データセットを提案した(リアルデータ)．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>入力に3Dセマンティックマップがあるので，ある意味ではscene parsingに対して提案手法は入力画像を手掛かりにレンダリングされたセマンティックマップをマイナー修正だけ？</p></li><li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0936.pdf">論文</a></p></li></ul></div></div><div class="slide_index">[#47]</div><div class="timestamp">2018.7.6 16:26:27</div></div></section><section id="ID_Parallel_Attention_A_Unified_Framework_for_Visual_Object_Discovery_through_Dialogs_and_Queries"><div class="paper-abstract"><div class="title">Parallel Attention: A Unified Framework for Visual Object Discovery through Dialogs and Queries</div><div class="info"><div class="authors">Bohan Zhuang, Qi Wu, Chunhua Shen, Ian Reid, Anton Van den Hengel</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>variable lengthな物体の言語descriptions (一つの単語からmulti-round会話まで)から正しく画像中に物体を参照できるネットワークPLANを提案した．</li><li>PLANネットワークは2種類のattentionを用いている:言語descriptionsのパーツと①画像のグローバルコンテンツ②画像の局所的領域ー物体candidatesを関連付けする．</li><li>recurrent attentionを用いて，異なる処理段階でのattentionを変更できる．更に， attentionを可視化することにより，システムが異なる処理段階で正しい物体領域をattentionしているかを確認できる．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/parallel-attention.png" alt="parallel-attention"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>言語入力が異なるRefCOCO,RefCOCO+,GuessWhat?!などのデータセットでSoTAな精度を達成．</li><li>LSTMとattentionを用いているので，referringプロセスをビジュアライズできて，ネットワークの解釈可能性も高い．</li><li>固定長ではなく長さが異なる言語入力(一つの単語からmulti-round会話まで)から正しく視覚attentionを得られる．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>LSTM+attentionもなかなか良さそう</p></li><li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0234.pdf">論文</a></p></li></ul></div></div><div class="slide_index">[#48]</div><div class="timestamp">2018.7.6 16:14:24</div></div></section><section id="ID_DS_Tighter_Lifting-Free_Convex_Relaxations_for_Quadratic_Matching_Problems"><div class="paper-abstract"><div class="title">DS*: Tighter Lifting-Free Convex Relaxations for Quadratic Matching Problems</div><div class="info"><div class="authors">F. Bernard et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">画像間で対応関係を探すなどのマッチング問題の最適化を解くための新たな手法を提案．
二次計画問題として定式化された問題を効率的に解くための凸緩和手法．
提案手法はScalableでデータ数が増えても計算時間があまり増えないことに加えて，
Tightな解を求めることが可能．

</div></div><div class="item2"><img src="slides/figs/DS_Tighter_Lifting-Free_Convex_Relaxations_for_Quadratic_Matching_Problems.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>ScalableかつTightな解を求められるアルゴリズムを提案</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Bernard_DS_Tighter_Lifting-Free_CVPR_2018_paper.pdf">論文</a></li><li>中身全然理解できてないです...</li></ul></div></div><div class="slide_index">[#49]</div><div class="timestamp">2018.7.6 12:29:32</div></div></section><section id="ID_Robust_Video_Content_Alignment_and_Compensation_for_Rain_Removal_in_a_CNN_Framework"><div class="paper-abstract"><div class="title">Robust Video Content Alignment and Compensation for Rain Removal in a CNN Framework</div><div class="info"><div class="authors">J. Chen, C. Tan, J. Hou, L. Chau and H. Li</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>動画像において，土砂降り的なはっきり見えている雨を除去する．高速に動くカメラの動きにも頑健．</p><p>スーパーピクセルセグメンテーションをし，デプスを含むユニットに分解．シーンコンテンツの位置合わせをスーパーピクセルレベルで実行する．
雨の線の場所や遮蔽された背景コンテンツに関する情報を抽出し，
雨除去の中間出力を得る．
さらに，そこで使った情報を更にCNNの入力特徴として使い，
高周波成分の復元に使う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Robust_Video_Content_Alignment_and_Compensation_for_Rain_Removal_in_a_CNN_Framework.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>土砂降り雨を合成した車載カメラ画像データに適用し，PSNRが改善，見た目もよくなった．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>合成データでしか評価してないのが気になる．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3970.pdf">論文</a></li></ul></div></div><div class="slide_index">[#50]</div><div class="timestamp">2018.7.6 12:10:22</div></div></section><section id="ID_Self-calibrating_Polarising_Radiometric_Calibration"><div class="paper-abstract"><div class="title">Self-calibrating Polarising Radiometric Calibration</div><div class="info"><div class="authors">D.T. Guangwei, B. Shi, Y. Zheng, S. Yeung</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>偏光放射特性のセルフキャリブレーション手法の提案．これまでには，カメラ応答だけ求めるものはあったが，
本研究では，未知のカメラ応答及び未知の偏光角を同時に復元する．</p><p>応答が線形とした場合，偏光フィルタを回転すれば偏光強度の変化は正弦波になるはずではる．この事実を使って，統合的に最適化を定式化する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Self-calibrating_Polarising_Radiometric_Calibration.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>カメラ応答を偏光情報を使って，放射特性と偏光特性の両方を統合的に最適化するというやり方で，自己キャリブレーションを実現したものは初．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3446.pdf">論文</a></li></ul></div></div><div class="slide_index">[#51]</div><div class="timestamp">2018.7.6 11:16:24</div></div></section><section id="ID_Mining_on_Manifolds_Metric_Learning_without_Labels"><div class="paper-abstract"><div class="title">Mining on Manifolds: Metric Learning without Labels</div><div class="info"><div class="authors">A. Iscen, G. Tolias, Y. Avrithis and O. Chum</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>教師なしマイニングの話．ハードポジティブ・ハードネガティブが分別しやすいマニフォールドにおける表現方法を考案．
本手法によれば，
正例たちは一つのマニフォールドに距離が離れて置かれ，
負例たちは複数のマニフォールドに距離が近い形で置かれる．
ユークリッド的な近さとマニフォールド的な近さの不一致性によって，両者を分別可能になる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Mining_on_Manifolds_Metric_Learning_without_Labels.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>学習済みネットワークの教師なしファインチューニングや，特定物体検索に適用させてみて，完全・部分教師ありと比較して性能超え．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2779.pdf">論文</a></li></ul></div></div><div class="slide_index">[#52]</div><div class="timestamp">2018.7.6 10:41:24</div></div></section><section id="ID_Polarimetric_Dense_Monocular_SLAM"><div class="paper-abstract"><div class="title">Polarimetric Dense Monocular SLAM</div><div class="info"><div class="authors">L. Yang, F. Tan, A. Li, Z. Cui, Y. Furukawa and P. Tan</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>通常のカメラとは違い，偏光カメラ画像からは，鏡面反射してしまっているようなところでも，物体表面の法線角度が窺い知れたりするので，組み合わせることで良いDense SLAMができるようになると思われる．ところが，偏光情報からの法線角度推定は，特に境界付近でエラーが載りやすい．従来手法では事前にセグメンテーションマスクを生成しており，オフラインアルゴリズムであった．</p><p>本研究では，・方位ベースデプス伝播・2視点デプス一貫性チェック・デプス最適化の
反復処理を完全自動化し，
注意深くGPU実装できるように設計，
SLAMに組み込んだところでリアルタイムに動くようにした．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Polarimetric_Dense_Monocular_SLAM.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>通常カメラ＋偏光カメラでのSLAMは初．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2612.pdf">論文</a></li></ul></div></div><div class="slide_index">[#53]</div><div class="timestamp">2018.7.6 10:12:41</div></div></section><section id="ID_Left-Right_Comparative_Recurrent_Model_for_Stereo_Matching"><div class="paper-abstract"><div class="title">Left-Right Comparative Recurrent Model for Stereo Matching</div><div class="info"><div class="authors">Z. Jie, P. Wang, Y. Ling, B. Zhao, Y. Weio, J. Feng and W. Liu</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>左右一貫性チェックという，ステレオにおける視差情報を改善する手法がある．従来は，左右でのチェックはそれぞれ独立かつHand-Craftedであった．
本稿では，これを結合的に行えるようなリカレントモデルを提案する．</p><p>両眼の視差結果から，オンラインにミスマッチ領域を判別していく．ここで，ソフトアテンション機構を導入する．
学習したエラーマップを使い，次時間の処理において，信用できない領域に選択的に焦点を当てるという方法．
これにより，視差結果を反復的に改善していく．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Left-Right_Comparative_Recurrent_Model_for_Stereo_Matching.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>3つのベンチマークでSoTA性能を達成．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0483.pdf">論文</a></li></ul></div></div><div class="slide_index">[#54]</div><div class="timestamp">2018.7.5 11:32:39</div></div></section><section id="ID_Taskonomy_Disentangling_Task_Transfer_Learning"><div class="paper-abstract"><div class="title">Taskonomy: Disentangling Task Transfer Learning</div><div class="info"><div class="authors">Alexander Sax, William Shen, Amir Zamir, Jitendra Malik, Silvio Savarese, Leonidas J. Guibas</div><div class="conference">CVPR 2018</div><div class="paper_id">452</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>CVPR2018のベストペーパー．視覚タスクの関連性に基づき高効率的に遷移学習を行えるtaskonomyの提案．</li><li>問題設定：CVの究極的目標の1つとしては全世界の視覚問題を解く．現状では，視覚タスクはほぼ各々にネットワーク，データセットを定義，学習を行っていた．しかし，作者達が視覚タスクの間に関連性が高いと指摘し，究極的視覚タスクを解ける目標に向かう一歩としては①視覚タスクの間の関連性を導く．②そういった関連性に基づき目標タスクに対しより遷移学習を行うことで，少ないデータで高効率的に学習をする．</li><li>概要：taskonomyを求めるのは主に2つのパーツから構成される．①タスクの関連性を量化し，その量化の結果はaffinity matrix.②目標タスクに対し，affinity matrixに基づき最適化を行い，効率的に遷移学習を行えるルートdirected graph(このグラフはtaskonomy)を求める.</li><li>①タスク関連性を表すaffinity matrixを求めるプロセス：目標タスクグループをT，0から学習できる開始タスクグループをSと定義．ステップ１：Sタスクに対し，全部0から学習を行う．(全部encoder-decoderの構造)ステップ２：遷移学習を行う．「タスク間1次関連」s∈S, t∈T, tに対しsのencoderを使い，decoderを学習．（全部のs,tペアに対この学習を行う）「タスク間n次関連(n<=5)」1次関連の効果により，tに対し，効果上位n個のsのencoderのrepresentationを同時に用いてdecoderを学習．ステップ３：ステップ2の全部の遷移学習の最終ロスをベースにaffinity matrixを生成する. それぞれのタスクのロスは異なるロス関数を用いているため，線形的に遷移学習の効果を表せないために，Ordinal Normalizationをベースとした手法を用いてロスを関連度に表示した(この方法はAnalytic Hierachy Processを参考した)．最終的に求めたAffinity matrixの(i,j)の意味はあるタスクに対しタスクiから遷移学習の効果がどれくらいの確率でjタスクから遷移学習の効果より良い．</li><li>② affinity matrixを用いた効率的遷移学習：ある目標タスクｔに対し，最適遷移学習ルートを求めるプロセスは「affinity matrixに対し，subgraph selection問題であり，そのsubgraphのスタートはsで，終点は目標タスクｔ」．具体的には条件１あらかじめ定義した開始タスクｓの数を超えない；２タスクｓに対し，遷移学習の回数上限は1回;３遷移学習の開始及び目標タスクはsubgraphに含める．の3つの拘束条件の元Binary Interger Programmingを用いて最適化を行う．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/taskonomy.png" alt="taskonomy"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>視覚タスクは各々でデータセット，方法を構築することから，視覚タスク間の関係を追究し，その関係を利用ことで，資源を有効的に利用できる．</li><li>3000＋ネットワークをトレーニング</li><li>～50,000GPU時間</li><li>120ｋ画像0から学習，16k画像遷移学習</li><li>400万枚画像，それぞれ26種類のタスクのアノテーション付き</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>今のそれぞれのタスクで解決しようとする研究より一歩先に立っている</p></li><li><p>実行力とコストが想像できないくらい</p></li><li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0744.pdf">論文</a></p></li><li><p><a href="http://taskonomy.stanford.edu/">プロジェクト</a></p></li><li><p><a href="https://github.com/StanfordVL/taskonomy/tree/master/taskbank">task bank</a></p></li></ul></div></div><div class="slide_index">[#55]</div><div class="timestamp">2018.7.6 10:37:53</div></div></section><section id="ID_Single-Image_Depth_Estimation_Based_on_Fourier_Domain_Analysis"><div class="paper-abstract"><div class="title">Single-Image Depth Estimation Based on Fourier Domain Analysis</div><div class="info"><div class="authors">Jae-Han Lee, Minhyeok Heo, Kyung-Rae Kim and Chang-Su Kim</div><div class="conference">CVPR2018</div><div class="paper_id">59</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>フーリエ周波数領域解析をベースとしたCNNを用いて，単一のRGB画像から距離画像を推定する手法を提案．CNNはResNet-152ベースで，depthbalanced Euclidean lossと呼ばれる損失関数を設計し，広範囲の距離画像を推定できるように学習する．次に，入力画像を複数のアスペクト比で切り取って複数のデプスマップ候補を生成する．アスペクト比の小さい画像は，局所的に信頼できるデプスマップを生成するが，アスペクト比の大きい画像は，大域的なデプスマップを生成する．これらをお互いに補完するために，デプスマップ候補を周波数領域で結合する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180705_single-image_depth_estimation1.jpg" alt="20180705_single-image_depth_estimation1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>距離画像推定にフーリエ周波数領域解析を使った（作者の知る限りで）初めての論文である．NYUv2 depth datasetの画像280,000枚を学習し，654枚で評価を行った．fully convolutional residual networksを用いた最新の手法と同等またはそれ以上の性能を得ることができた．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>損失関数やフーリエ周波数領域解析がしっかり構築されており説得力のある論文である．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2873.pdf">論文</a></li></ul></div></div><div class="slide_index">[#56]</div><div class="timestamp">2018.7.5 14:32:07</div></div></section><section id="ID_FlipDial_A_Generative_Model_for_Two-Way_Visual_Dialogue"><div class="paper-abstract"><div class="title">FlipDial: A Generative Model for Two-Way Visual Dialogue</div><div class="info"><div class="authors">Daniela Massiceti, Siddharth Narayanaswamy, Puneet Kumar Dokania, Phil Torr</div><div class="conference">CVPR 2018</div><div class="paper_id">740</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>画像及びキャプションからConditional VAEをベースとした視覚会話(継続的な質問・回答を両方とも生成)を生成できるモデルFLIPDIALを提案した．</li><li>従来の継続的な応答するタスクを1VDと継続的に応答及び質問両方行うタスクを2VDと定義し，構造的に変更を加え提案FLIPDIALが1VD，２VD 両方対応できる．</li><li>FLIPDIALの基本的な考えはCNNによりfull 会話をエンコードし，conditional VAEを用いて会話を生成する． 2VDタスクは画像・キャプション・会話履歴からlatent variableを通して答えをfull dialogueをモデリングし，予測したfull dialogueとgt dialogueのlatent空間においての類似性及び画像との関連性を元にロス関数を定義した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/FlipDial.png" alt="FlipDial"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>提案FLIPDIALが1VDタスクにおいてVisDialデータセットに対し従来のVisual Dialog手法より良い精度を達成し，新規な２VDタスクのbaselineを建てて，新たな評価指標なども提案した．</li><li>提案FLIPDIALが一つの質問に対し，多様な答えを生成できる．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>CNNによりfull 会話をエンコードする考えが大胆的</p></li><li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3275.pdf">論文</a></p></li></ul></div></div><div class="slide_index">[#57]</div><div class="timestamp">2018.7.5 11:59:41</div></div></section><section id="ID_VoxelNet_End-to-End_Learning_for_Point_Cloud_Based_3D_Object_Detection"><div class="paper-abstract"><div class="title">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</div><div class="info"><div class="authors">Yin Zhou, Oncel Tuzel</div><div class="conference">CVPR 2018</div><div class="paper_id">575</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>LiDARなどのセンサーにより得られるポイントクラウドから3次元検出を行うend-to-endなネットワーク構造VoxelNetの提案．</li><li>VoxelNetの全体構造はまずスパースなポイントクラウドから4Dテンサーで特徴を表し，それに対して空間コンテキスト情報を集合する層により処理を行った後，RPN構造により3Dバウンディングボクスの予測を行う．</li><li>提案したポイントクラウド情報抽出するの主なプロセスは①ポイントクラウドをスパース3Dボクセルに変換し②VFE(新規提案)層よりボクセル内のポイントワイズな特徴を集合し，更にVFE層をstackすることにより3D形状の複雑な特徴を抽出する</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/VoxelNet.png" alt="VoxelNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>提案手法はスパースなポイントクラウドを直接処理できる．LiDARポイントクラウド処理の手法の中最も良い性能が得られる．</li><li>KITTIデータセットにおいてSoTA.車・人・自転車を高い精度の検出できる．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>LiDARセンサーから有効的かつ高スピードで識別や検出する研究がまたまた研究の余地があると感じている．</p></li><li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3333.pdf">論文</a></p></li></ul></div></div><div class="slide_index">[#58]</div><div class="timestamp">2018.7.5 11:52:56</div></div></section><section id="ID_Efficient_Optimization_for_Rank-based_Loss_Functions"><div class="paper-abstract"><div class="title">Efficient Optimization for Rank-based Loss Functions</div><div class="info"><div class="authors">P. Mohapatra, M. Rolínek, C.V. Jawahar, V. Kolmogorov and M.P. Kumar</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>情報検索システムにおける精度は，平均精度（AP）や正規化減価累積利得（NDCG）のような複雑なランクベースロス関数で測られるが，このような関数の微分不可能性・分解不可能性は単純勾配最適化においては許されない．これの回避方法として，一般的には，構造化ヒンジロス上界の最適化をロス関数にする方法や，直接ロス最小化のような漸近的手法が使われる．
それでも，loss-augmented inferenceの高い計算複雑性は残る．</p><p>本稿では，それを緩和する，新たなクイックソート・フレーバーな分割統治を導入したアルゴリズムを提案する．分解不可能ロス関数に適用可能である．</p><p>我々のアルゴリズムにも適用できるロス関数の特徴づけも提供する．これはAP，NDCGの両方を含む．
更に，我々の手法の計算複雑性の上では，漸近的に比較ベースアルゴリズムでは改善できないことを証明する．</p><p>あらゆるCVのタスクでの学習モデルでのAP，NDCGの構造化ヒンジロス上界の最適化の文脈において，我々の手法の効果をデモンストレーションする．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Efficient_Optimization_for_Rank-based_Loss_Functions.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>クイックソート的にランクを並べ替え・選択して，というのは面白いやり方に感じる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>専門用語がわからないと読み下すのが難しいかもしれない．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3595.pdf">論文</a></li></ul></div></div><div class="slide_index">[#59]</div><div class="timestamp">2018.7.3 11:34:08</div></div></section><section id="ID_Deep_Learning_of_Graph_Matching"><div class="paper-abstract"><div class="title">Deep Learning of Graph Matching</div><div class="info"><div class="authors">A. Zanfir, C. Sminchisescu</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>グラフマッチングをDeepで扱えるようにしたという，大変汎用的な論文．</p><p>グラフマッチングにおける全パラメータのEnd-to-End学習を可能にした．これは深層特徴抽出階層により表現される．</p><p>モデルの異なる行列計算レイヤの定式化が肝である模様．勾配の一貫性ある効率的な伝播を行えるようにする，
マッチング問題を解くにあたっての組み合わせ最適化レイヤと，特徴抽出階層を通じた，
ロス関数からの完全なパイプラインを提案している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Learning_of_Graph_Matching.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>グラフマッチングは，ノードとその間をつなぐエッジで構成されるグラフ（ノードの幾何学的位置は無意味）の等価性を検索するタスクで，コンピュータビジョンや機械学習のあらゆる方面で適用されるものである．これが深層学習で解けるようになれば，それは当然大きな進歩である．
グラフマッチングを扱おうとする人の第一リファレンスになりえる論文と思われる．</p><p>キーポイント検出において試してみたところ，やはりSoTA性能．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1830.pdf">論文</a></li></ul></div></div><div class="slide_index">[#60]</div><div class="timestamp">2018.7.3 10:27:30</div></div></section><section id="ID_CSRNet_Dilated_Convolutional_Neural_Networks_for_Understanding_the_Highly_Congested_Scenes"><div class="paper-abstract"><div class="title">CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes</div><div class="info"><div class="authors">Yuhong Li, Xiaofan Zhang, Deming Chen</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>混雑状況認識やカウンティング、密度推定のためのネットワークCongested Scene Recognition Network (CSRNet)を提案し、データドリブンで学習する。畳み込みによる特徴抽出とDilated Convにより広範領域から特徴を評価する（ここにおいてプーリング層を置き換えると記述され、純粋に畳み込みそうのみで構成されている）。図はDilated ConvとPoolingの有無によるヒートマップの比較。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180704CSRNet.png" alt="180704CSRNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>データセットはShanghaiTech, UCF_CC_50, WorldEXPO'10, UCSDを用いて検証した。特にShanghaiTechデータセットではMean Absolute Error (MAE)が47.3%も下がった。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3727.pdf">論文</a></li><li><a href="https://github.com/leeyeehoo/CSRNet">Project</a></li></ul></div></div><div class="slide_index">[#61]</div><div class="timestamp">2018.7.4 08:54:53</div></div></section><section id="ID_Stacked_Latent_Attention_for_Multimodal_Reasoning"><div class="paper-abstract"><div class="title">Stacked Latent Attention for Multimodal Reasoning</div><div class="info"><div class="authors">Haoqi Fan, Jiatong Zhou</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>アテンションモデルの改善を行い、VQAに適用する。現在のアテンションに関する弱点は（１）中間層では対応関係といった理由づけに関する情報を除去してしまう（２）StackedAttentionでは局所最適解に陥ってしまうことを挙げた。本論文ではこの問題を解決するため、明示的に中間的な理由づけに関する構造を加えたStacked Latent Attention Modelを提案。マルチモーダルのReasoningに有効であることがわかり、VQAにおいても効果的な手法となった。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180704StackedLatentAttentionModel.png" alt="180704StackedLatentAttentionModel"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>構造をスタックしてより良好なアテンションにしていくモデルを構築した。空間的な理由づけ（Reasoning）を潜在的に行うモデルであり、マルチモーダルであるVQAや画像説明文にも効果的である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>アテンションは論文数増加していて、各方面に広がってきた。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_Stacked_Latent_Attention_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://research.fb.com/publications/stacked-latent-attention-for-multimodal-reasoning/">Project</a></li></ul></div></div><div class="slide_index">[#62]</div><div class="timestamp">2018.7.4 08:35:08</div></div></section><section id="ID_CarFusion_Combining_Point_Tracking_and_Part_Detection_for_Dynamic_3D_Reconstruction_of_Vehicles"><div class="paper-abstract"><div class="title">CarFusion: Combining Point Tracking and Part Detection for Dynamic 3D Reconstruction of Vehicles</div><div class="info"><div class="authors">N Dinesh Reddy, Minh Vo and Srinivasa G. Narasimhan</div><div class="conference">CVPR2018</div><div class="paper_id">221</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>カメラキャリブレーションされていない複数の視点から車の3次元データをパーツ単位で再構成しトラッキングも行うパイプライン「CarFusion」を提案．強いオクルージョンがある場合でも移動車両の検出，localize，再構成を行うことができる．構造化された点（検出された車両のパーツ）と構造化されていない特徴点（Harrisのコーナー検出）を融合して車の正確な再構成と検出を行う．複数視点からの車の再構成にはCar centric RANSAC（cRANSAC）を提案している．通常のRANSACと比較して，左右対称を前提として車の形状を考慮したマッチングを行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180703_CarFusion1.jpg" alt="20180703_CarFusion1.jpg"><img src="slides/figs/20180703_CarFusion2.jpg" alt="20180703_CarFusion2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>キャリブレーションされていない非同期のカメラからダイナミックに剛体を再構成するという，3D Vision分野で重要だが困難な研究を行った．In the wildでの高精度な検出としても新規性がある．cRANSACのみ用いた場合とCarFusion全体パイプラインを用いた場合で，トラッキングの誤差を4倍削減することができた．再構成時のキーポイント検出も従来手法より優れている．さらに，車の半分程度が隠れてしまう強いオクルージョンがある場合でも3D構成を検出することができた．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>車に特化した手法だが，いくつかの剛体に対しては似たアルゴリズムを用いることができそう．検証実験も詳細で一見の価値はある．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0890.pdf">論文</a></li></ul></div></div><div class="slide_index">[#63]</div><div class="timestamp">2018.7.3 22:58:23</div></div></section><section id="ID_Human_Semantic_Parsing_for_Person_Re-Identification"><div class="paper-abstract"><div class="title">Human Semantic Parsing for Person Re-Identification</div><div class="info"><div class="authors">Mahdi M. Kalayeh, Emrah Basaran, Muhittin Gökmen, Mustafa E. Kamasak, Mubarak Shah</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>人物に関して、主に姿勢に関するパーツベースのセマンティック情報を導入することにより人物再同定（Person Re-identification）の精度を向上させる。提案のSPReIDはInception-v3やResNet-152をベースアーキテクチャにしていて、各種データセットに対して向上が見られた。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180703PersonSemanticReID.png" alt="180703PersonSemanticReID"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>各種データセットにて次の通り向上した。セマンティック情報を人物再同定に使うのは有効であることが判明した。Market-1501 (参考文献48) by ∼17% in mAP、∼6% in rank-1, CUHK03 (参考文献24) by ∼4% in rank-1、DukeMTMC-reID (参考文献50) by∼24% in mAP ∼10% in rank-1。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>セマンティック情報を使えば想像通りよくなるが、実際にデータセットに対してアノテーションしてCVPRに通す根性がすごい！見習おう。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Kalayeh_Human_Semantic_Parsing_CVPR_2018_paper.html">論文</a></li></ul></div></div><div class="slide_index">[#64]</div><div class="timestamp">2018.7.3 10:29:10</div></div></section><section id="ID_Monocular_Relative_Depth_Perception_With_Web_Stereo_Data_Supervision"><div class="paper-abstract"><div class="title">Monocular Relative Depth Perception With Web Stereo Data Supervision</div><div class="info"><div class="authors">Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao, Ruibo Li, Zhenbo Luo</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Web画像により相対的なステレオ視に関するデータセットを作成した。RankingLossを改善した誤差関数によりデータセット内のステレオ視を学習、ペアリングが困難なものについての対応付けを行なった。作成したデータセットに対してState-of-the-artであるのみならず、他のピクセルベースの密な推定（距離推定、セマンティックセグメンテーション）についても有効性を示した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180703RelativeDepthPerception.png" alt="180703RelativeDepthPerception"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Web画像により密なステレオ視を推定する枠組みを考案、Relative Depth from Web（ReDWeb）の概念を提唱。RankingLossの改善版によりペアリングが困難な対応付についても行った。DIW/NYUDv2データセットにて評価、State-of-the-artな性能を達成した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Web x StereoVisionという掛け合わせがよい。さらに、アノテーションの枠組みも参考になる。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Xian_Monocular_Relative_Depth_CVPR_2018_paper.html">論文</a></li><li><a href="https://blogs.adelaide.edu.au/machine-learning/2018/02/19/monocular-relative-depth-perception-with-web-stereo-data-supervision/">Project</a></li><li><a href="https://sites.google.com/site/kexianshomepage/">著者</a></li></ul></div></div><div class="slide_index">[#65]</div><div class="timestamp">2018.7.3 01:52:59</div></div></section><section id="ID_Depth_and_Transient_Imaging_With_Compressive_SPAD_Array_Cameras"><div class="paper-abstract"><div class="title">Depth and Transient Imaging With Compressive SPAD Array Cameras</div><div class="info"><div class="authors">Qilin Sun, Xiong Dun, Yifan Peng, Wolfgang Heidrich</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>イメージング技術において、Time-of-flight（ToF）やTransient Imagingに関する研究である。これらの技術は研究の関心に反して解像度が上がらず、低コスト化も進んでいない。本論文ではセンサの設計を変更し、Arrays of Single Photon Avalanche Diodes (SPADs)を改善することでこの問題に取り組む。DMDを用い、光学系をカスタマイズすることでSPADの解像度を800x400まで向上。時系列ヒストグラムを調整するモデルでは効果的にノイズ除去できることも示した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180703DepthTransient.png" alt="180703DepthTransient"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>右図は提案のイメージング技術であり、SPADsの高解像度化を実現した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Sun_Depth_and_Transient_CVPR_2018_paper.html">論文</a></li><li><a href="https://vccimaging.org/Publications/Sun2018CompressiveSPADArray/">Project</a></li></ul></div></div><div class="slide_index">[#66]</div><div class="timestamp">2018.7.3 01:19:46</div></div></section><section id="ID_GVCNN_Group-View_Convolutional_Neural_Networks_for_3D_Shape_Recognition"><div class="paper-abstract"><div class="title">GVCNN: Group-View Convolutional Neural Networks for 3D Shape Recognition</div><div class="info"><div class="authors">Yifan Feng, Zizhao Zhang, Xibin Zhao, Rongrong Ji, Yue Gao</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>3次元形状認識のためにGroup-View Convolutional Neural Netowrk (GVCNN)を提案し、形状に関するビュー不変な内的かつ階層的な相関関係を記述する。識別性が高くなるようGroupingModuleによりビューポイントのグルーピングを行い、途中の層でViewPoolingやGroupFusionを行い、3次元形状認識を行う。右図はGVCNNのアーキテクチャである。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180703GVCNN.png" alt="180703GVCNN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ビューポイントに不変な認識を実施可能なEnd-to-Endな学習フレームワークであるGVCNNを提案した。MVCNNとの比較により有効性を示した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>ビューポイントに不変は学習可能。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Feng_GVCNN_Group-View_Convolutional_CVPR_2018_paper.html">論文</a></li></ul></div></div><div class="slide_index">[#67]</div><div class="timestamp">2018.7.3 00:56:48</div></div></section><section id="ID_Deflecting_Adversarial_Attacks_with_Pixel_Deflection"><div class="paper-abstract"><div class="title">Deflecting Adversarial Attacks with Pixel Deflection</div><div class="info"><div class="authors">Aaditya Prakash et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://twitter.com/tomoyukun">Tomoyuki Suzuki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>対象ピクセルを近傍のピクセルと入れ替えるPixel Deflectionを利用した敵対的摂動に対しての防御手法。NNは敵対的摂動ノイズに対しては弱いのに対し、ランダムノイズには強いという経験的な傾向から、敵対的摂動が加わっていないサンプルへの性能をできるだけ保持した状態でノイズを加えるためにPixel Deflection＋ Wavelet Denoisingを行う。既存手法よりも良い防御性能を示した。</p></div></div><div class="item2"><img src="slides/figs/Deflecting_Adversarial_Attacks_with_Pixel_Deflection.png" alt="Deflecting_Adversarial_Attacks_with_Pixel_Deflection.png"></div><div class="item3"><div class="text"><h1>詳細・なぜ通ったか？</h1><p>Pixel Deflectionはある対象ピクセルをその近傍からランダムにサンプルされたピクセル値に置き換える。対象ピクセルを決める際には、正しい識別を行う際に重要となる領域以外からサンプリングする。具体的には敵対的摂動による影響が少ないsaliencyであるRobust CAMを定義し、そのsaliencyが低い領域からサンプル。この背景には敵対的摂動は画像に対して大域的に（物体に関係せず）現れる傾向があるので、できるだけ正しい識別に影響を与えない領域にPixel Deflectionを行いたいという考えがある。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>NNのパラメータに関する変更はせずに入力への変更を行うことで敵対的摂動の種類にかかわらず汎用的に防御できるという点が面白い。手法の裏付けとして敵対的摂動とランダムノイズの識別精度への影響比較も行っており、面白かった。</p><ul><li><a href="https://arxiv.org/abs/1801.08926">論文</a></li></ul></div></div><div class="slide_index">[#68]</div></div></section><section id="ID_OATM_Occlusion_Aware_Template_Matching_by_Consensus_Set_Maximization"><div class="paper-abstract"><div class="title">OATM: Occlusion Aware Template Matching by Consensus Set Maximization</div><div class="info"><div class="authors">S. Korman, M. Milam and S. Soatto</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>本稿の手法により，60%のピクセルがノイズに侵されている（occluded）テンプレートでもマッチングできる．しかも結果の証明が可能．</p><p>N高次元ベクトルの最近傍探索をsqrt(N)次元ベクトルにおける2つのセットの間の最近傍探索
の変換を行う．
これで探索効率が二乗でよくなる．</p><p>また，コンセンサスセット最大化（cf. RANSAC）による，ハッシング手法も提案．これにより，遮蔽を扱うことができる．</p><p>これらのスキームは，高い確率で最適解を得るのに求められるイタレーション数を考慮する，ランダム化仮説＆テストアルゴリズムとみなすことができる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/OATM_Occlusion_Aware_Template_Matching_by_Consensus_Set_Maximization.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>SoTAなロバスト性・高速性・精度を達成．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>やはりエレガントさを求める勢力が台頭してきているように感じる．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3289.pdf">論文</a></li></ul></div></div><div class="slide_index">[#69]</div><div class="timestamp">2018.7.2 18:18:37</div></div></section><section id="ID_Context_Contrasted_Feature_and_Gated_Multi-scale_Aggregation_for_Scene_Segmentation"><div class="paper-abstract"><div class="title">Context Contrasted Feature and Gated Multi-scale Aggregation for Scene Segmentation</div><div class="info"><div class="authors">H. Ding, X. Jiang, B. Shuai, A.Q. Liu, G. Wang</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>セマンティックセグメンテーションにおいて，FCNの中に2つの機構を取り入れた．</p><ul><li>Context Contrasted Local feature<br>コンテキストを見るには広く見るべきだが，ローカルな特徴も実際必要なので，
そういう構造のカーネルを採用したフィルタを定義．</li><li>Gated sum<br>それぞれの場所におけるスケールごとに対応したスケールの特徴を選択的に集計．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Context_Contrasted_Feature_and_Gated_Multi-scale_Aggregation_for_Scene_Segmentation.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>３つのセマンティックセグメンテーションのデータベースでSoTA．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1276.pdf">論文</a></li></ul></div></div><div class="slide_index">[#70]</div><div class="timestamp">2018.7.2 17:44:27</div></div></section><section id="ID_Now_You_Shake_Me_Towards_Automatic_4D_Cinema"><div class="paper-abstract"><div class="title">Now You Shake Me: Towards Automatic 4D Cinema</div><div class="info"><div class="authors">Yuhao Zhou, Makarand Tapaswi, Sanja Fidler</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Kazuho Kito</div><div class="item1"><div class="text"><h1>概要</h1><p>4D映画を自動で作成するための研究。63本の映画に9286のエフェクトのアノテーションをしたデータセットであるMOVIE4dを提案。エフェクトは、揺れ、天候、風、水しぶきなど。また、人の形のみでなく、視聴覚情報をまとめるニューラルネットワークとしてConditional Random Field modelを提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Now_You_Shake_Me_Towards_Automatic_4D_Cinema.PNG" alt="Now_You_Shake_Me_Towards_Automatic_4D_Cinema.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>映画のスレッドだけでなく、クリップ内でのキャラクター間のエフェクトの相関関係を利用。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Now_You_Shake_CVPR_2018_paper.pdf">論文</a></li><li><a href="http://www.cs.toronto.edu/~henryzhou/movie4d/index.html">ProjectPage</a></li></ul></div></div><div class="slide_index">[#71]</div><div class="timestamp">2018.6.23 02:05:00</div></div></section><section id="ID_Objects_as_context_for_detecting_their_semantic_parts"><div class="paper-abstract"><div class="title">Objects as context for detecting their semantic parts</div><div class="info"><div class="authors">Abel Gonzalez-Garcia, Davide Modolo and Vittorio Ferrari</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Kazuho Kito</div><div class="item1"><div class="text"><h1>概要</h1><p>物体の情報に効果的な影響があるセマンティックパーツの検出アプローチを提案。どのパーツを予想するべきかという指標として、オブジェクトの見た目とクラスを用い、その見た目を基に物体の中でのそのパーツに期待される相対的な位置をモデル化。OffsetNetという新しいネットワークモジュールで所定の物体の中の一部の場所を効果的に予測することを達成。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Objects_as_context_for_detecting_their_semantic_parts_1.PNG" alt="Objects_as_context_for_detecting_their_semantic_parts_1.PNG"><img src="slides/figs/Objects_as_context_for_detecting_their_semantic_parts_2.PNG" alt="Objects_as_context_for_detecting_their_semantic_parts_2.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>the PASCAL-Part datasetにおいて+5mAPの改善。PASCAL-PartとCUB200-2011において他のパーツ検出手法より優れた成果を達成。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0952.pdf">論文</a></li></ul></div></div><div class="slide_index">[#72]</div><div class="timestamp">2018.6.23 01:42:55</div></div></section><section id="ID_Solving_the_Perspective-2-Point_Problem_for_Flying-Camera_Photo_Composition"><div class="paper-abstract"><div class="title">Solving the Perspective-2-Point Problem for Flying-Camera Photo Composition</div><div class="info"><div class="authors">Ziquan Lan, David Hsu and Gim Hee Lee</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ドローンのような飛行体にユーザーが指定した２つの被写体を含んだ画像を撮影させる手法の提案。ユーザーは希望の２つの被写体を指定し、それぞれどのように配置したいかを指定する。
ここでは、n=2の場合のPnP問題を考えることでドローンの撮影位置を決定する。
カメラの姿勢を求める６自由度の問題として考えるが、P2P問題は解が一意に定まらないので移動距離が最小となる撮影位置を解とする。
ワールド座標系とカメラ座標系間の直接の変換を考えるのではなく、２つの被写体がx軸上に配置される座標系を考えることで、計算を簡略化する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Solving_the_Perspective-2-Point_Problem_for_Flying-Camera_Photo_Composition.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>仮想環境によって実験を実施し、被写体の位置情報にノイズが含まれている場合でも頑健なことを確認した。実環境における実験は、SLAMにより得られた自己位置を使用して行ったが、推定誤差があるような場合においても高い精度で撮影位置を求めることに成功した。
撮影位置の最適化は、１つの物体を先に最適化した後にもう一方の物体の位置を調整するという実験結果が得られた。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>幾何学的な計算が中心である論文であり、数少ない機械学習が全く登場しない論文である。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Lan_Solving_the_Perspective-2-Point_CVPR_2018_paper.html">論文</a></li></ul></div></div><div class="slide_index">[#73]</div><div class="timestamp">2018.7.2 01:32:33</div></div></section><section id="ID_3D_Pose_Estimation_and_3D_Model_Retrieval_for_Objects_in_the_Wild"><div class="paper-abstract"><div class="title">3D Pose Estimation and 3D Model Retrieval for Objects in the Wild</div><div class="info"><div class="authors">Alexander Grabner et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Pavel A. Savkin</div><div class="item1"><div class="text"><h1>概要</h1><p>RGB画像から６DOF姿勢推定＋３Dモデル検索を同時に行えるようにする手法。厳密な中身は画像から６DOF姿勢するパートと、その姿勢とRGB画像情報から最適な３Dモデルを検索して見つけてくるパートに分けられる。三次元姿勢推定については既存手法からInspireされ、認識された物体を内包するProjected 3D Bounding Box(16 Parameters)及び3D Scale(3 Parameters)をResNetやVGGをベースとしたCNNで推定し、PnP問題を解いた。これによりモデル既知でないにもかかわらず、Pascal３D＋データセットでState of the artな６DOF姿勢推定精度を実現。３Dモデル検索パートでは、RGB特徴量とDepthImage特徴量の取得を異なるのCNNで定義し、RGB特徴量、対応するDepth特徴量、間違ったDepth特徴量をそれぞれAnchor, Positive, Negativeと扱いTripletLossを計算することで学習。これによりRGB画像とDepth画像という全く異なるドメイン間での特徴量マッチングを実現し、テクスチャレスな３DモデルであったりRGB画像の照明環境不明であっても最適な３Dモデルの検索を行えるようになった。同カテゴリでは似たような形状のモデルが多数存在するにもかかわらず、画像に対する人間のAnnotationに対して約50％の精度での検索結果を実現した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-3D_Pose_Estimation_and_3D_Model_Retrieval_for_Objects_in_the_Wild.png" alt="fukuhara-3D_Pose_Estimation_and_3D_Model_Retrieval_for_Objects_in_the_Wild.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Projected 3D Bounding Box を用いた６DOF 姿勢推定ではモデル既知でしか解けなかったところをモデル既知でState of the art、モデルなしでもCompatibleな結果を出した点。検索パートではハイコストな３D畳み込みや既知DepthImageを要することなくRGBとDepthImage間の共通記述特徴量の学習・その有効性を示した点。結果については姿勢推定においてはState of the art、検索においては人間のAnnotationに対して50%の精度を実現。６DOF姿勢の高精度推定と、RGB・Depth間の共通記述子を学習することにより画像から３Dモデル検索までを行うシステムを実現したことが通った理由と思われる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://arxiv.org/abs/1803.11493 " target="blank">[論文] 3D Pose Estimation and 3D Model Retrieval for Objects in the Wild</a></li><li>共通記述子にはもう少し議論がほしかった印象。TripletLossを使うというアイデアはすごく良かった。３D Bounding Boxという考え方自体も美しい。</li></ul></div></div><div class="slide_index">[#74]</div><div class="timestamp">2018.6.30 23:18:55</div></div></section><section id="ID_Neural_Sign_Language_Translation"><div class="paper-abstract"><div class="title">Neural Sign Language Translation</div><div class="info"><div class="authors">Necati Cihan Camgoz, Simon Hadfield, Oscar Koller, Hermann Ney and Richard Bowden</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>手話動画を言語に翻訳する手法を提案。手話の各フレーム及び文章中の各単語を表現する特徴ベクトルを取得し、RNNによりそれぞれのsequenceを取得する。
手話動画から得られるsequenceを文章のsequenceに変換することで翻訳を実現する。
その際、手話動画のフレーム数は文章中の単語数と比べて圧倒的に多いため対応付けが難しい。
そこで、Attentionを導入することで手話動画中の重要なフレームに対して重み付けを行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Neural_Sign_Language_Translation.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来のデータセットは機械学習に用いるには数が少ないため、手話動画、手話の単語、対応するドイツ語の文章を含んだRWTH-PHOENIX=Weather 2014Tというデータセットを提案した。従来の手話に関する研究は、Recognitionの問題として考えていたのに対して、Sequence間の変換と考えることにより文章を出力することを可能とした。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Camgoz_Neural_Sign_Language_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#75]</div><div class="timestamp">2018.6.30 21:07:35</div></div></section><section id="ID_4DFAB_A_Large_Scale_4D_Facial_Expression_Database_for_Biometric_Applications"><div class="paper-abstract"><div class="title">4DFAB: A Large Scale 4D Facial Expression Database for Biometric Applications</div><div class="info"><div class="authors">Shiyang Cheng, Irene Kotsia, Maja Pantic and Stefanos Zafeiriou1</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Kazuho Kito</div><div class="item1"><div class="text"><h1>概要</h1><p>180万枚以上の3Dのメッシュを含んだダイナミックで高解像度な3Dの顔のデータベースである4DFABを提案。このデータベースには、5年以上かけて異なる4つの期間で撮られた180のサブジェクトの記録を含んでいる。サブジェクトには、自然な表情とそうでない表情の両方の4Dビデオが含まれており、行動に関するバイオミメティクスだけでなく、顔と表情の認識に使うことができる。また、表情をパラメータ化させるためのパワフルなblendshapeを学習することに使うこともできる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/4DFAB.PNG" alt="4DFAB.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>自然な表情と笑顔，泣き笑い，混乱している表情などの自然でない表情が含まれている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.01443.pdf">論文</a></li></ul></div></div><div class="slide_index">[#76]</div><div class="timestamp">2018.6.20 19:36:05</div></div></section><section id="ID_Transparency_by_Design_Closing_the_Gap_BetweenPerformance_and_Interpretability_in_Visual_Reasoning"><div class="paper-abstract"><div class="title">Transparency by Design: Closing the Gap Between　Performance and Interpretability in Visual Reasoning</div><div class="info"><div class="authors">D. Mascharka et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">モデルの挙動を解釈しやすい，かつ高精度なVQAモデルの提案．
Neural Module Networkは結果の解釈がしやすいが，複雑なタスクだと精度が落ちる．
それを改善した手法はあるが，解釈性を犠牲にしている．
ということで，性能と解釈性のGapを埋めよう，というモチベーション．
Moduleに分解するのは従来と同じアイディアなものの，
モジュール設計のデザインを工夫することで改善を図っている．
なお，Transparencyは途中経過を可視化できるという意味で使っている．
</div></div><div class="item2"><img src="slides/figs/Transparency_by_Design_Closing_the_Gap_BetweenPerformance_and_Interpretability_in_Visual_Reasoning.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>VQAのタスクにおいて解釈しやすいモデルでありながらSOTAを達成</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3279.pdf">論文</a></li><li>ざっと読んだだけだとなぜこの論文のモジュール構成が優れているのかわかりにくい</li></ul></div></div><div class="slide_index">[#77]</div><div class="timestamp">2018.6.29 18:30:49</div></div></section><section id="ID_PoTion_Pose_MoTion_Representation_for_Action_Recognition"><div class="paper-abstract"><div class="title">PoTion: Pose MoTion Representation for Action Recognition</div><div class="info"><div class="authors">V. Choutas et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">行動認識のためのPoseベース特徴表現の提案．
まず姿勢推定手法を適用して各Jointのヒートマップを計算．
各時刻のヒートマップを時間情報を色で表現してTemporal Aggregationすることで，
各動画でJointごとに1枚のヒートマップ表現を得る．
これをチャンネルにスタッキングしたのがPoTionという提案特徴表現．
PoTionをCNNに入力して識別するのが提案手法．
従来手法と組み合わせることで相補的に働き精度の向上が可能．
</div></div><div class="item2"><img src="slides/figs/PoTion_Pose_MoTion_Representation_for_Action_Recognition.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>Pose情報を利用した新しい特徴表現を提案</li><li>時間情報を含めて画像1枚に落とせるので入力時間長に依存せずCNNで扱いやすい</li><li>元々のSOTAのI3Dと組み合わせて更に高い精度を達成</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.pdf">論文</a></li><li>姿勢推定がかなり良くできるようになってきた時代の手法という感じ</li><li>色を使って時間情報をAggregationしてるのが面白い</li></ul></div></div><div class="slide_index">[#78]</div><div class="timestamp">2018.6.29 18:10:57</div></div></section><section id="ID_Deep_Learning_under_Privileged_Information_Using_Heteroscedastic_Dropout"><div class="paper-abstract"><div class="title">Deep Learning under Privileged Information Using Heteroscedastic Dropout</div><div class="info"><div class="authors">John Lambert et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://twitter.com/tomoyukun">Tomoyuki Suzuki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>テスト時に入力できる情報に対して、学習時にはより強い情報が使用できる場合にその+αの情報（特権情報）を学習時にうまく活用する研究。テスト時には特権情報が得られないので、特権情報に対して周辺化したものを出力とする方針をとるが、一般にその値を求めるのは難しい。そこで特権情報をGaussian Dropoutの分散の中に埋め込み学習することでテスト時に特別な計算をせずに周辺化することができる。画像認識・機械翻訳で実験し、学習サンプルが少ない状況下で特に効果を発揮する。</p></div></div><div class="item2"><img src="slides/figs/Deep_Learning_under_Privileged_Information_Using_Heteroscedastic_Dropout.png" alt="Deep_Learning_under_Privileged_Information_Using_Heteroscedastic_Dropout.png"></div><div class="item3"><div class="text"><h1>詳細・なぜ通ったか？</h1><p>Gaussian Dropout部分での逆伝搬ではVAEなどで用いられるreparameterization trickを利用している。画像認識においては特権情報として物体のbounding boxを与えている。SGDでのNNの最適化が理想的に完了する条件下でデータ効率が上がるという理論的な保証と、実験結果による精度向上が評価されたと考えられる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>マルチタスクでの学習よりもしっかり良い結果となっていて興味ふかい。理論的保証はあるものの、Gaussian noiseが具体的にどのようなサンプルに対してどのように作用しているのかを確認する実験なども欲しかった。</p><ul><li><a href="https://arxiv.org/abs/1805.11614">論文</a></li></ul></div></div><div class="slide_index">[#79]</div></div></section><section id="ID_Motion-Guided_Cascaded_Refinement_Network_for_Video_Object_Segmentation"><div class="paper-abstract"><div class="title">Motion-Guided Cascaded Refinement Network for Video Object Segmentation</div><div class="info"><div class="authors">Ping Hu, Gang Wang, Xiangfei Kong, Jason Kuen, Yap-Peng Tan</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>通常、物体のモーションは背景（カメラ）モーションとは異なることを事前知識として動画に対する物体セグメンテーションを実行した。提案モデルであるCascaded Refinement Network（CRN）は最初にオプティカルフローにより荒くセグメントしてから高解像なセグメンテーションをCNNにより実施する（ここらへんがMotion-Guidedと呼ばれる理由）。CRN構造に対してSingle-channel Residual Attention Moduleも提案して学習/推論時間を効率化。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180629MotionGuidedCRN.png" alt="180629MotionGuidedCRN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>疎密探索の枠組みを採用しており、まずはオプティカルフローを抽出、Active Contourにより荒くセグメント。次にCRNによりセグメンテーションを実施した。動画に対して84.4%@mIOU, 0.73 sec/frame（semi-supervision）を達成した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Old-fashionな手法を組み合わせて弱教師にする方法を提案。また、DAVISは少量教師や教師なしが当たり前のように出てくる。コンペで教師なしを用いる設定はうまいと思った。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0391.pdf">論文</a></li><li><a href="https://davischallenge.org/">DAVIS dataset</a></li><li><a href="https://github.com/feinanshan/Motion-Guided-CRN">GitHub</a></li><li><a href="https://sites.google.com/view/pinghu/projects/video-object-segmentation">Project</a></li></ul></div></div><div class="slide_index">[#80]</div><div class="timestamp">2018.6.29 07:51:28</div></div></section><section id="ID_Multi_Content_GAN_for_Few_Shot_Font_Style_Transfer"><div class="paper-abstract"><div class="title">Multi-Content GAN for Few-Shot Font Style Transfer</div><div class="info"><div class="authors">Samaneh Azadi et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://twitter.com/tomoyukun">Tomoyuki Suzuki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>26のアルファベットのうちfewな種類しかデータがない状況で、そのフォントで書かれた他種類のアルファベットを生成する研究。アルファベットの形状をグレースケールで生成するGlyph Netとそれらにカラーで装飾を行うOrnamentation Netの二つからなる。単純にpix2pixのようにsingle-shotな構造で生成するよりも形状生成と装飾を多段に行う方がはるかに実際に近いアルファベットが生成できた。</p></div></div><div class="item2"><img src="slides/figs/Multi_Content_GAN_for_Few_Shot_Font_Style_Transfer.png" alt="Multi_Content_GAN_for_Few_Shot_Font_Style_Transfer.png"></div><div class="item3"><div class="text"><h1>詳細・なぜ通ったか？</h1><p>Glyph Netではチャネル方向に配列されたアルファベットを入力する。ないアルファベットは０埋めし、敵対的損失を用いて26×H×Wのグレースケールアルファベットを生成する。 Glyph Netはデータベースのあらゆるフォントサンプルに対して同一のモデルを学習する。 Ornamentation Netは上記のグレースケール画像に対し正解サンプルに近づくよう敵対的損失とMSEによって学習。ここで、正解はfewな種類しかないためそれらにのみ損失を計算。 Ornamentation Netはフォントごとに逐一異なるモデルを学習する。問題設定の面白さ、実際の完成度の高さが評価されたと考えられる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>画像生成において今回の「形状」と「色（装飾）」のように相関が薄いと考えられるものに関しては多段に生成を行った方が良い結果が得られるのだろうと考えられた。</p><ul><li><a href="https://arxiv.org/abs/1712.00516">論文</a></li></ul></div></div><div class="slide_index">[#81]</div></div></section><section id="ID_Self_Supervised_Feature_Learning_by_Learning_to_Spot_Artifacts"><div class="paper-abstract"><div class="title">Self-Supervised Feature Learning by Learning to Spot Artifacts</div><div class="info"><div class="authors">Simon Jenni et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>画像情報の欠損を検出することによる表現獲得手法。encoder-decoder modelの特徴マップ上の領域をランダムに欠損させて、decodeされた画像が欠損されたものがどうかを識別する。</p></div></div><div class="item2"><img src="slides/figs/Self_Supervised_Feature_Learning_by_Learning_to_Spot_Artifacts.png" alt="Self_Supervised_Feature_Learning_by_Learning_to_Spot_Artifacts.png"></div><div class="item3"><div class="text"><h1>詳細・なぜ通ったか？</h1><p>学習はdecoder内の補完レイヤーと識別器間で敵対的に行う。識別器は欠損された部分を示すマスクも出力する。encoder-decoderモデルをreal/fake問わず最初にかませる理由としては、CNNに入力することによるartifactによって識別器が判断しないようにするため、 また高次な特徴マップ上での欠損を行うことで高次な情報が欠損した画像の生成を行うためである。SoTAに近い精度が出ていることが評価されたと考えられる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>生成された画像を見る限り高次な情報が欠損しているかどうかがよく分からない。また、pretrainを終了するタイミングを含め全体的に学習が難しそうであると感じた。</p><ul><li><a href="https://arxiv.org/abs/1806.05024">論文</a></li></ul></div></div><div class="slide_index">[#82]</div></div></section><section id="ID_Weakly-Supervised_Semantic_Segmentation_by_Iteratively_Mining_Common_Object_Features"><div class="paper-abstract"><div class="title">Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features</div><div class="info"><div class="authors">Xiang Wang, Shaodi You, Xi Li, Huimin Ma</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>弱教師付き学習に対してボトムアップ（物体レベルで似ている特徴量をマイニング）とトップダウン（リファインされた領域をセグメンテーションの教師として学習）のアプローチを組み合わせる手法を考案。右図の（１）RegionNetによる出力/リファイン結果とPixelNetによる出力との比較によりセグメンテーションの誤差を比較、（２）PixelNetによ出力とマイニングした物体マスクと（Class Activation Mappingにより領域抽出された）RegionNetの出力を比較して領域に対する識別の誤差を計算する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180623WSSegmentMining.png" alt="180623WSSegmentMining"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>識別ベースによる物体領域抽出とセグメンテーションの誤差を繰り返し最適化することにより弱教師付きセマンティックセグメンテーションを実行する。SuperPixelの導入、類似物体マイニング、領域のリファインなどが徐々にセグメンテーション結果をよくしていく。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>弱教師付き学習はうまくいくときとうまくいかない時がありそう？なんどもやればランダムで良い結果が得られる？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Weakly-Supervised_Semantic_Segmentation_CVPR_2018_paper.pdf">論文</a>￥</li></ul></div></div><div class="slide_index">[#83]</div><div class="timestamp">2018.6.23 22:07:14</div></div></section><section id="ID_MAttNet_Modular_Attention_Network_for_Referring_Expression_Comprehension"><div class="paper-abstract"><div class="title">MAttNet: Modular Attention Network for Referring Expression Comprehension</div><div class="info"><div class="authors">Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, Tamara L. Berg</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>言語の入力から画像中の領域を指定するネットワークModular Attention Network (MAttNet)を提案する。本論文では２種類のアテンション（言語ベースのアテンションと視覚ベースのアテンション）を導入した。言語ベースのアテンションではどこに着目して良いかを学習、視覚ベースのアテンションではサブジェクトとその関係性を記述することができる。それぞれのスコアは統合され、最終的には文章を入力すると対応する領域がbboxの形式で出力される。右図はMAttNetの枠組みを示す。文章の入力から言語ベースのアテンションによりワードが厳選され、画像中から探索される。画像ではSubject-/Location-/Relationship-Moduleが働き、最後は統合して総合的に判断、画像中の物体相互関係を考慮した検出が可能になった。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180623MAttNet.png" alt="180623MAttNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来の枠組みと比較して、提案手法は（bboxレベルでもpixelレベルでも）高い精度を達成。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Language and Visionの一例。最近はやっている。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_MAttNet_Modular_Attention_CVPR_2018_paper.pdf">論文</a></li><li><a href="vision2.cs.unc.edu/refer/comprehension">デモ</a></li><li><a href="https://github.com/lichengunc/MAttNet">コード</a></li></ul></div></div><div class="slide_index">[#84]</div><div class="timestamp">2018.6.23 21:37:17</div></div></section><section id="ID_HashGAN_Deep_Learning_to_Hash_with_Pair_Conditional_Wasserstein_GAN"><div class="paper-abstract"><div class="title">HashGAN: Deep Learning to Hash with Pair Conditional Wasserstein GAN</div><div class="info"><div class="authors">Yue Cao, Bin Liu, Mingsheng Long, Jianmin Wang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Wasserstein GAN (WGAN)の枠組みでハッシング技術を行うHashGANを実装する。主となるアイディアはハッシングのためのデータ拡張を行うためにGANの枠組みを導入。通常は画像生成のみに用いられる仕組み自体を、データバリエーションの拡張のために用いて識別器を強くする。さらに、画像ペアの類似度を計測しながら画像生成を行う枠組みであるPair Conditional WGAN（PC-WGAN）を提案した。図はPC-WGANのアーキテクチャを示し、主に２つの構造から構成される。ひとつは画像生成部Gと識別部Dであり、ランダムノイズuと類似特徴vの連結から画像を生成してリアルな画像を生成。もうひとつはベイジアン学習によりコンパクトなバイナリハッシュを生成するハッシュエンコーダFである。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180623HashGAN.png" alt="180623HashGAN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>GANの枠組みにより高品質なバイナリコードを生成。生成器Gと識別器DのみならずハッシュエンコーダFを同時に学習する枠組みを考案。NUS-WIDE/CIFAR-10/MS-COCOにおいてSoTA。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>戦いの中で強くなるGAN、ですね。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_HashGAN_Deep_Learning_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#85]</div><div class="timestamp">2018.6.23 21:20:15</div></div></section><section id="ID_Clinical_Skin_Lesion_Diagnosis_using_Representations_Inspired_by_Dermatologist_Criteria"><div class="paper-abstract"><div class="title">Clinical Skin Lesion Diagnosis using Representations Inspired by Dermatologist Criteria</div><div class="info"><div class="authors">Jufeng Yang, Xiaoxiao Sun, Jie Liang, Paul L. Rosin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>肌疾患（Sin Disease）の診断を医師が行いながら、同時にデータ/モデルをIterativeに蓄積・構築する枠組みを考案。従来はComputer Aided Diagnosis（CAD）が肌疾患を判断するために役立ってきたが、2次元画像による判断は（ほぼ）行われていなかった。本論文ではデータの蓄積を行うと同時に、医師の判断材料をベースにした表現方法を学習することで、診断するモデルを構築する。診断の特徴としては、テクスチャの分布（複数箇所に渡り対称性が見られる領域が存在するかどうか）や色の表現（ここでは参考文献39,40のColorNameを適用）、形状を用いる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180623ClinicalSkinLesionDiagnosis.png" alt="180623ClinicalSkinLesionDiagnosis"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>医師による診察の目を実装したこと、データを繰り返し実装する枠組みを構築できたことが分野（特に医用画像処理）に貢献した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>この枠組みはうまいと思う。さらにお医者さんと連携してデータ収集/アルゴリズム強化の枠組みが整えば、より病気を判断するための手助けをする技術が発達する。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Clinical_Skin_Lesion_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#86]</div><div class="timestamp">2018.6.23 21:00:26</div></div></section><section id="ID_Deep_Cauchy_Hashing_for_Hamming_Space_Retrieval"><div class="paper-abstract"><div class="title">Deep Cauchy Hashing for Hamming Space Retrieval</div><div class="info"><div class="authors">Yue Cao, Mingsheng Long, Bin Liu, Jianmin Wang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>効率的かつ効果的なDeep Hash ModelであるDeep Cauchy Hashing（DCH）を提案する。主たるアイディアはCauchy分布によるPairwise Cross-Entropy Lossを提案することであり、類似する画像に対してHamming距離により誤差の重み付けを行う。図はDCHの構造を示しており、畳み込みにより表現を学習、全結合を通り抜けFully-Connected Hash Layer（FCH）によりK-bitのハッシュコードを生成、Cauchy Cross-Entropyにより類似度により誤差を計算して誤差を伝播させる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180623DeepCauchyHashing.png" alt="180623DeepCauchyHashing"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>画像検索において３種のデータ（NUS-WIDE/CIFAR-10/MS-COCO）に対してSoTA。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Deep Hashingの研究、データセットをより大きくしてハード面での実装も含めて評価する枠組みが必要？Hashingなので、FCC100Mのように1億枚くらいの画像検索をやってほしい（し、日本でも取り組んでいる人はいる）。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_Deep_Cauchy_Hashing_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#87]</div><div class="timestamp">2018.6.23 17:49:41</div></div></section><section id="ID_Blazingly_Fast_Video_Object_Segmentation_with_Pixel-Wise_Metric_Learning"><div class="paper-abstract"><div class="title">Blazingly Fast Video Object Segmentation with Pixel-Wise Metric Learning</div><div class="info"><div class="authors">Yuhua Chen, Jordi Pont-Tuset, Alberto Montes, Luc Van Gool</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ユーザインタラクティブに動画セマンティックセグメンテーションのための距離学習（Metric Learning）を行い、特徴空間を最適化する。入力画像から任意のモデルに対してセグメンテーションを実施、ユーザが良いと判断したセグメント領域を正解値として特徴空間を設定、一方でテスト（バリデーション?）画像を参照して動画セマンティックセグメンテーションを実行して学習する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180623PixelWiseMetricLearning.png" alt="180623PixelWiseMetricLearning"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ユーザインタラクティブというところが良い。セグメンテーションに対するアノテーションはコストがかかる（かかりすぎる）が、これをコンピュータによる推論と、ユーザのクリックのみにして特徴空間を学習していく方がコストが最小化される。精度も出るのでCVPRにアクセプトされている。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>セマンティックセグメンテーションに対するアノテーションは一枚あたり$10~12であると言われる。アノテーションコストを下げる方向に研究は進んでいて、特に動画セマンティックセグメンテーションは低コスト/弱教師学習/ドメイン変換等により進められると考えられる。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Blazingly_Fast_Video_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#88]</div><div class="timestamp">2018.6.23 17:26:13</div></div></section><section id="ID_Mask-guided_Contrastive_Attention_Model_for_Person_Re-Identification"><div class="paper-abstract"><div class="title">Mask-guided Contrastive Attention Model for Person Re-Identification</div><div class="info"><div class="authors">Chunfeng Song, Yan Huang, Wanli Ouyang, Liang Wang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>人物再同定のための特徴表現学習のためにTriplet学習を行う。オリジナルの全体画像（Anchor Image）、マスクされた人物領域（Positive Image）と背景領域（Negative Image）を用いて学習する。ここで、Triplet学習ではAnchor/Positiveをできる限り近く、Anchor/Negativeをできる限り遠くの特徴空間に置くことでよりよく対象となる物体を見ることができ、良好な特徴量を生成することができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180623MaskguidedContrastiveAttention.png" alt="180623MaskguidedContrastiveAttention"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>前景/背景を別々に学習し、背景ではなくできる限り前景に対してアテンションを置いて識別することで、人物再同定において良好な精度での識別を確認した。前景抽出のマスク画像に関するアノテーション（Mars/Market-1501/CUHK03）も公開することで、人物再同定の分野に貢献する。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>マスクを全部作成した、ということで膨大な労力がかかっている研究。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Song_Mask-Guided_Contrastive_Attention_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#89]</div><div class="timestamp">2018.6.23 17:04:57</div></div></section><section id="ID_Video_Person_Re-identification_with_Competitive_Snippet-similarity_Aggregation_and_Co-attentive_Snippet_Embedding"><div class="paper-abstract"><div class="title">Video Person Re-identification with Competitive Snippet-similarity Aggregation and Co-attentive Snippet Embedding</div><div class="info"><div class="authors">Dapeng Chen, Hongsheng Li, Tong Xiao, Shuai Yi, Xiaogang Wang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>適切な長さの動画分割（Video Snippet; ビデオスニペット）とCo-Attention機構による人物再同定の研究である。動画からの人物再同定では長いフレーム長をそのまま入力するよりもスニペットに分割して、さらには分割動画間のCo-Attentionに着目することで特徴表現を学習する方が認識に有利であることを実証した。スニペット間で類似度が計算され、ランク付が行われる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180623SnippetSimilarityCoAttention.png" alt="180623SnippetSimilarityCoAttention"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>動画スニペットごとに類似度を計算し、それぞれに対してCo-Attentionを求めて特徴量を学習する方法で複数のデータセットにてSoTA。iLIDS-VIDにてTOP1が85.4、TOP5が96.7（上位に正解が含まれているかどうかであり、TOP5は5人中1人が正解であればよい）であり強い手法が構築できた。PRID2011においてもそれぞれ93.0/99.3、Marsにおいても86.3/94.7である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>人物再同定は数年前までTOP5（〜TOP20）が高い精度であれば許される時代だったがTOP5で95+%（驚くべきは99%も出ているデータセットがあるということ）という数値である。中国の事情もあり、その解決のためにSenseTimeがその役を買っているというわけである。今後はさらなるデータ作成と社会実装の推進が進むと思われる。SenseTime/CUHKの連携ラボの枠組みも整った（CUHK-SenseTime Joint Lab.と著者リストにある）ことで、さらに研究が大規模に進められる。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Video_Person_Re-Identification_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#90]</div><div class="timestamp">2018.6.23 16:51:25</div></div></section><section id="ID_Recognizing_Human_Actions_as_the_Evolution_of_Pose_Estimation_Maps"><div class="paper-abstract"><div class="title">Recognizing Human Actions as the Evolution of Pose Estimation Maps</div><div class="info"><div class="authors">Mengyuan Liu, Junsong Yuan</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>動画に対する姿勢+ヒートマップからの行動認識を解く問題である。通常、動画中の姿勢推定は不安定なものであるが、動画内での平均化や連続する姿勢、ヒートマップから補完的に改善して行動を認識する枠組みを提案。ヒートマップのスパース性を考慮、Spatial Rank Poolingを実装してEvolutionImageを作成しヒートマップや姿勢の変動に対応できるようにした。この枠組みはNTU RGBD/UTD-MHAD/PennActionに対して有効であることを示した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180623EvolutionPoseMap.png" alt="180623EvolutionPoseMap"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>不安定な姿勢変動に対応するためにSpatial Rank Poolingを実装した。位置づけ的にはDynamicImage/VideoDarwinがTwo-Stream ConvNetsに対する改善なのに対して本論文は姿勢に対してこれらの枠組みを試行。この枠組みを用いてNTU RGBD/UTD-MHAD/PennActionに対してSoTA。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>直感的に言うと、テスト動画に対する中間特徴（中間値）みたいのを作成して、外れ値を防ぐことで精度向上？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Recognizing_Human_Actions_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#91]</div><div class="timestamp">2018.6.23 16:25:14</div></div></section><section id="ID_Video_Representation_Learning_Using_Discriminative_Pooling"><div class="paper-abstract"><div class="title">Video Representation Learning Using Discriminative Pooling</div><div class="info"><div class="authors">Jue Wang, Anoop Cherian, Fatih Porikli, Stephen Gould</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>行動認識における特徴は独立ではなく、動画を通して共通する部分が多い。これら共通特徴を捉えるためのプーリング（Pooling）手法を確立すると共に特徴表現を学習する。戦略としてはMultiple Instance Learning（MIL）により未知だが識別性に優れた非線形の識別境界（Hyperplane）を求めるようにPooling自体をDNNの中で学習する。右図は従来法のDynamicImages（参考文献2; 図中(iii)）と提案手法であるSVM Pooling（図中(iv)）の比較である。SVM Poolingは動画像全体の動きを捉える特徴量が抽出しやすくなり、精度向上に寄与した。識別決定境界を学習、動画レベルの識別を最適化することから、SVM Poolingと呼ぶ。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180623DiscriminativePooling.png" alt="180623DiscriminativePooling"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>３種類の公開データセット（HMDB51/Charades/NTU-RGBD）にてSoTA。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Pooling/Conv自体のパラメータを固定ではなく、学習可能にしてしまう、というアイディアは多くなってきた。構造自体を学習するNAS（Neural Architecture Search）なんかにも使うことでさらなる精度向上ができないか？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Video_Representation_Learning_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#92]</div><div class="timestamp">2018.6.23 16:05:37</div></div></section><section id="ID_SGPN_Similarity_Group_Proposal_Network_for_3D_Point_Cloud_Instance_Segmentation"><div class="paper-abstract"><div class="title">SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation</div><div class="info"><div class="authors">Weiyue Wang, Ronald Yu, Qiangui Huang, Ulrich Neumann</div><div class="conference">CVPR 2018</div><div class="paper_id">335</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>ポイントクラウドを対象としたインスタンスセグメンテーションネットワークSGPNを提案した．</li><li>SGPNが入力されたポイントクラウドに対してまずpointnet++などを用いて特徴抽出を行い，抽出特徴に対し類似性を評価することによってグルーピングを行う．グルーピングと同時にセマンティックを予測する．グループの結果をインスタンスセグメンテーションに用いる</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SGPN.png" alt="SGPN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>初めてのポイントクラウドに対しインスタンスセグメンテーションを行うネットワークの提案と指摘した．</li><li>SGPNがflexibleに2D CNN特徴を導入でき，これによって更なる良い性能を得られる．</li><li>3次元Shape及び実三次元シーンのセグメンテーション用データセットShapeNetとStanford Indoor Semantic Dataset及びNYUV2においてSoTAなインスタンスセグメンテーション結果を得られた．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>ネットワークの説明が簡潔で，結果も良いのでつかってみたい</p></li><li><p><a href="https://arxiv.org/pdf/1711.08588.pdf">論文</a></p></li></ul></div></div><div class="slide_index">[#93]</div><div class="timestamp">2018.6.26 13:56:20</div></div></section><section id="ID_Recurrent_Slice_Networks_for_3D_Segmentation_of_Point_Clouds"><div class="paper-abstract"><div class="title">Recurrent Slice Networks for 3D Segmentation of Point Clouds</div><div class="info"><div class="authors">Qiangui Huang, Weiyue Wang, Ulrich Neumann</div><div class="conference">CVPR 2018</div><div class="paper_id">341</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>有効的にポイントクラウドの局所的構造をモデリングできるポイントクラウドを直接処理する3D セグメンテーションフレームワークRSNetを提案した．</li><li>RSNetは主に3つの部分から構成され:①slice pooling layerが入力ポイントクラウドをslicesスにグループし， sliceごとにポイントの特徴をaggregateすることによりグローバル特徴を抽出する②RNNsにより特徴を抽出する③slide unpooling layerにより抽出特徴をポイントに戻す．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/RecurrentSliceNetworks.png" alt="RecurrentSliceNetworks"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>S3DIS, ScanNet, ShapeNetの3つのデータセットにおいて最も高いセグメンテーション精度を達成した．</li><li>RSNetは従来の3DCNNと比べ精度が高いほか，時間とメモリー消耗がより少ない．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>RNN構造をポイントクラウドの情報抽出に用いるのが良い精度を達成した原因だと思う</p></li><li><p><a href="https://arxiv.org/pdf/1802.04402.pdf">論文</a></p></li><li><p><a href="https://github.com/qianguih/RSNet">コード</a></p></li></ul></div></div><div class="slide_index">[#94]</div><div class="timestamp">2018.6.26 13:51:24</div></div></section><section id="ID_3D_Object_Detection_with_Latent_Support_Surfaces"><div class="paper-abstract"><div class="title">3D Object Detection with Latent Support Surfaces</div><div class="info"><div class="authors">Zhile Ren, Erik Sudderth</div><div class="conference">CVPR 2018</div><div class="paper_id">121</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>点群密度，3次元法線方向ヒストグラム，COG特徴などの3つの特徴に基づいた3次元検出手法の提案．</li><li>RGB-D画像にoriented cuboidsをアラインして，更にcanonical座標フレームに変換する．ボクセルごとに点群密度特徴，3次元法線方向ヒストグラム及びCOG特徴(Latent Support Surfaces特徴)を抽出し，SVMにより識別及びバウンディングボクスの検出を行う．提案するCOG特徴は5<em>5</em>5ボクセルでボクセルごとに主要法線方向の表示をベースとしたdescriptor．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/3D-detection-latentsupportsurfaces.png" alt="3D-detection-latentsupportsurfaces"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来の3次元検出手法は局所的形状及び表現から物体カテゴリを決定し，異なる視覚スタイル・スケールの物体を検出するロバスト性が低い．提案手法は異なるスケールの物体検出を行える．特に小さい物体の検出が従来より強い．</li><li>SUN RGB-D DatasetにおいてSOTAな精度を達成．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://www.ics.uci.edu/~sudderth/papers/cvpr18support.pdf">論文</a></li></ul></div></div><div class="slide_index">[#95]</div><div class="timestamp">2018.6.26 13:46:17</div></div></section><section id="ID_Learning_3D_Shape_Completion_From_Laser_Scan_Data_With_Weak_Supervision"><div class="paper-abstract"><div class="title">Learning 3D Shape Completion From Laser Scan Data With Weak Supervision</div><div class="info"><div class="authors">David Stutz, Andreas Geiger</div><div class="conference">CVPR 2018</div><div class="paper_id">226</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>弱監督なラーニングベースな3次元形状補完手法を提案した．3次元CGモデルデータにより形状priorを学習し，形状予測学習に対しmaximum likelihoodロスを用いて弱監督学習を行う．</li><li>具体的に，2段階で学習を行う．段階①で三次元CGモデルによりfull監督でリコンストラクションロスを用いてauto-encoder（VAE）をトレーニングし，段階②では欠損した実三次元モデルからencoderを行い，段階①で学習済みのdecoderにより形状補完を行い，復元した形状と入力形状間のmaximum likelihood lossにより学習を行う．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/3DshapeCompletion-laser-weak.png" alt="3DshapeCompletion-laser-weak"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Data-driven型な3次元形状補完手法と比べ，実行時間が短く，full supervised的な手法と比べリアルデータに対し監督信号がなくても行える．</li><li>ShapeNet, ModelNetにおいてData-driven型な手法と同レベルな精度．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>弱監督・無監督がホットスポット．</p></li><li><p><a href="http://www.cvlibs.net/publications/Stutz2018CVPR.pdf">論文</a></p></li></ul></div></div><div class="slide_index">[#96]</div><div class="timestamp">2018.6.26 13:40:50</div></div></section><section id="ID_SurfConv_Bridging_3D_and_2D_Convolution_for_RGBD_Images"><div class="paper-abstract"><div class="title">SurfConv: Bridging 3D and 2D Convolution for RGBD Images</div><div class="info"><div class="authors">Hang Chu, Wei-Chiu Ma, Kaustav Kundu, Raquel Urtasun, Sanja Fidler</div><div class="conference">CVPR 2018</div><div class="paper_id">378</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>3次元サーフェスに沿って，2次元畳み込みフィルタリング処理を行う新たな畳み込み処理方法SurfConvを提案した．</li><li>従来の2次元畳み込み操作は空間スケールの変化に弱い，3次元畳み込み操作はデータのスパース性により効率が良くないなどの問題点から，3次元空間中のサーフェスに沿って畳み処理を行う手法を提案した．提案するdepth-guided畳み込み操作は，デプス値によりreceptive fieldのサイズをコントロールし， receptive fieldごとの幾何情報をHHAにより表示する．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SurfConv.png" alt="SurfConv"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>SurfConvを用いて連続なデプス情報を離散的に取り扱い，一つのreceptive field内でx,yはfull解像度で同時にzの解像度は従来の3次元畳み込みより低いので効率が良い．</li><li>従来の3DCNＮ手法と比べ良い精度を得られるほか，モデルのサイズが小さい．</li><li>KITTI,NYUv2データセットにおいてSOTAな精度を達成した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>考え方が新しい</p></li><li><p>調整する必要があるhyper parameterが多いので，訓練しにくいかもしれない</p></li><li><p><a href="http://chuhang.github.io/files/publications/CVPR_18_1.pdf">論文</a></p></li><li><p><a href="https://github.com/chuhang/SurfConv">コード</a></p></li></ul></div></div><div class="slide_index">[#97]</div><div class="timestamp">2018.6.26 13:34:49</div></div></section><section id="ID_Unsupervised_Learning_of_Monocular_Depth_Estimation_and_Visual_Odometry_with_Deep_Feature_Reconstruction"><div class="paper-abstract"><div class="title">Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction</div><div class="info"><div class="authors">Huangying Zhan, Ravi Garg, Chamara Weerasekera, Kejie Li, Harsh Agarwal, Ian Reid</div><div class="conference">CVPR 2018</div><div class="paper_id">60</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>デプス推定及びビジュアルodometryを同時に行える無監督学習フレームワークの提案．</li><li>デプス推定及びodometry推定の結果をそれぞれ互いにwarpingし比較することをベースとしたimageリコンストラクション及びfeatureリコンストラクションロスを提案し，従来の従来のphotometricベースなロス関数と比べ良い精度を得られた．またデプス推定及びodometry推定をwarping，比較することにより，自己監督を得て，監督データなしで学習を行える</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/unsupervised-depthvo.png" alt="unsupervised-depthvo"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>デプス推定及びvisual odometryを同時に行う方が良い精度を得られると宣言した．</li><li>KITTIデータセットにおいて，デプス推定及びvisual odometryがトップ１の精度を達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>2017年及び2018年のCVPRでの左右視，自己監督などを用いたデプス推定の手法が多い</p></li><li><p><a href="https://arxiv.org/pdf/1803.03893.pdf">論文</a></p></li><li><p><a href="https://github.com/Huangying-Zhan/Depth-VO-Feat">コード</a></p></li></ul></div></div><div class="slide_index">[#98]</div><div class="timestamp">2018.6.26 13:23:06</div></div></section><section id="ID_Divide_and_Grow_Capturing_Huge_Diversity_in_Crowd_Images_with_Incrementally_Growing_CNN"><div class="paper-abstract"><div class="title">Divide and Grow: Capturing Huge Diversity in Crowd Images with Incrementally Growing CNN</div><div class="info"><div class="authors">Deepak Babu Sam, Neeraj N Sajjan, R. Venkatesh Babu, Mukundhan Srinivasan</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Shusuke Shigenaka</div><div class="item1"><div class="text"><h1>概要</h1><p>訓練データの複雑さに基づいてモデル容量を反復的に拡張するIG-CNNの提案。CNNは個人の検出だけでなく群衆の特徴を学び群衆密度マップを生成することができる。
しかし、多くのデータセットは群衆が一様ではないため疎の画像を高密度と予測してしまう。
提案したIG-CNNは、データセット全体で訓練されたベースのCNN密度回帰分析から始まり、
訓練データに応じて階層的なCNNツリーを作成していくことで細かく分類していくことである。
提案手法は群衆データセットで高いカウント精度を達成している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Divide_and_Grow_Capturing_Huge_Diversity_in_Crowd_Images_with_Incrementally_Growing_CNN_1.png" alt="Divide_and_Grow_Capturing_Huge_Diversity_in_Crowd_Images_with_Incrementally_Growing_CNN_1"><img src="slides/figs/Divide_and_Grow_Capturing_Huge_Diversity_in_Crowd_Images_with_Incrementally_Growing_CNN_2.png" alt="Divide_and_Grow_Capturing_Huge_Diversity_in_Crowd_Images_with_Incrementally_Growing_CNN_2"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>CNNを階層的に成長させるモデルであるが、手動で指定された基準なしに作成することができる</li><li>階層を作った後はIG-CNNを廃棄させ、CNNツリーのリーフノードのネットワークだけで選択が可能になる</li><li>UCF-CC_50とWorldExpo'10のデータセットにおいて高い精度を誇る</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>群衆のデータセットは通常高密度のデータばかりに目を向けがちだが、疎なデータに目を向けているのが良いと思った。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2726.pdf">論文</a></li></ul></div></div><div class="slide_index">[#99]</div><div class="timestamp">2018.6.26 11:39:43</div></div></section><section id="ID_Trapping_Light_for_Time_of_Flight"><div class="paper-abstract"><div class="title">Trapping Light for Time of Flight</div><div class="info"><div class="authors">R. Xu, M. Gupta, S.K. Nayar</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>一回のスキャンだけで，かなり複雑な形状の物体を全周囲計測し，復元する3D復元システムの提案．</p><p>Light trapと名付けた，Time-of-Flight(ToF)式3Dスキャナの光を反射しまくる鏡部屋を使うのがキーアイデア．Trapの形状を入射光が複数回trapの中で跳ね返るように選択することで，
対象物体に対し，あらゆる位置・あらゆる方向から複数回数光が注ぐことになる．
ToFセンサはそれぞれの光の移動距離を入手でき，Trapの形状は既知（予め計測しておく）なので，
全ての完全なパスが再現可能である．
そのためのアルゴリズムを提案する．</p><p>通常すごく遮蔽する，球格子をかなり複雑な形状物体の例としたときに，シミュレーションによって99.9%の表面に光を当てられることを示す．
また，ハードウェアプロトタイプを実装し，
様々な物体の大きさ，反射特性の物体に対し試してみた．</p></div></div><div class="item2"><div class="text"><p style="text-align:center"><img src="slides/figs/Trapping_Light_for_Time_of_Flight_Figure1.png" alt="Figure1" style="width:50%"><br><img src="slides/figs/Trapping_Light_for_Time_of_Flight_Figure2.png" alt="Figure2"><br><img src="slides/figs/Trapping_Light_for_Time_of_Flight_Figure3.png" alt="Figure3"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>この手のシステムは反射屈折式(Catadioptric)で通っているようだが，問題となるのは一貫性，ラベリング問題（どの受容光が発射光だったのか）を解決しなければならないという困難さがある．</p><p>このシステムでは，ToF（パスの長さが分かる）を使っているので，ラベリング問題を解く必要がない．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>カオス感（パイ捏ね変換）．カオスは複雑さと単純さを仲立ちできる点が面白いので，問題を簡単にするのに使えるという好例の一つに感じる．DNNも複雑ネットワークという点では同様である．</li><li>物体形状の周期性などの条件がたまたま合ってしまうと，全然見えなくなる可能性はないだろうか．</li><li>システムとして工夫している点が複数あり，制約もあるので，各々論文を確認いただきたい．</li></ul><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3054.pdf">論文</a></li></ul></div></div><div class="slide_index">[#100]</div><div class="timestamp">2018.6.26 11:26:50</div></div></section><section id="ID_Separating_Style_and_Content_for_Generalized_Style_Transfer"><div class="paper-abstract"><div class="title">Separating Style and Content for Generalized Style Transfer</div><div class="info"><div class="authors">Yexun Zhang, Ya Zhang, Wenbin Cai</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>StyleとContent、それぞれを抽出するEncoderにより得られた特徴を結合することによりStyle Transferを実現するEMDモデルを提案。学習の際、Style Encoderの学習にはStyleが一緒だがContentが違う画像を、Content Encoderの学習にはContentが一緒だがStyleが異なる画像のセットを用いて学習する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Separating_Style_and_Content_for_Generalized_Style_Transfer.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Styleとして漢字のフォント、Contentとして漢字の種類を考え検証を行った。Style及びContentのセットは、枚数が多いほど精度がよくなるが増えていくと飽和して変わらなくなる。
ベースラインと比べるときれいな文字が生成されている。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Style Transferの一般化と書いてある割に、漢字という一部の地域でしか用いられていない文字でしか実験がされておらず他の対象に適用可能であるかが不明。（ロスの設計も漢字を前提とした重み付けがされている）そもそも学習画像のセットにStyleとContentが一緒であるという仮定が必要であり、これらが明らかであるという理由で漢字で実験したとあるように、漢字以外でやる場合StyleとContentとは何かを考えなければならない。</p><ul><li><a href="https://arxiv.org/abs/1711.06454">論文</a></li></ul></div></div><div class="slide_index">[#101]</div><div class="timestamp">2018.6.25 14:11:55</div></div></section><section id="ID_Learning_Globally_Optimized_Object_Detector_via_Policy_Gradient"><div class="paper-abstract"><div class="title">Learning Globally Optimized Object Detector via Policy Gradient</div><div class="info"><div class="authors">Yongming Rao et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>強化学習（Policy Gradient）を応用して大域最適化された物体検出器の学習を行う end-to-end なフレームワークの提案. 既存の物体検出器の学習に RoI 間の相互関係が用いられていないことに着目し, 検出された物体の mAP の総和を最大にする様な学習を行うために強化学習を用いている. 提案手法はネットワークの構造には依存しないので既存の多くの手法に適用が可能. 評価実験では, COCO-style mPA で Faster R-CNN を 2.0%, Faster R-CNN with Feature Pyramid Networks を 1.8% 向上させた.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Learning-Globally-Optimized-Object-Detector-via-Policy-Gradient.png" alt="fukuhara-Learning-Globally-Optimized-Object-Detector-via-Policy-Gradient.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>強化学習を応用して大域最適化された物体検出器の学習を行う end-to-end なフレームワークの提案（厳密には強化学習では無い）</li><li>検出された物体の mAP の総和を最大にする様に学習するため, 大域最適化が可能 (既存手法は multi-task loss で個々を独立して学習）</li><li>提案手法はネットワークの構造には依存しないので既存の手法に適用が可能（汎用性）. 計算のオーバーヘッドも無い(高速). 通常の Cross-Entropy Gradient に簡単な修正を加えるだけで適用可能（単純）</li><li>強化学習の reward は mAP の総和を使用, action は Bounding Box の選択</li><li>action が膨大になってしまうのを防ぐため, 物体のカテゴリーは既存の手法で適当に選択されていると仮定（学習済みのモデルに追加で学習）, それでも action が膨大なので, 強化学習の各イテレーションでサンプリングをして行動を決定</li><li>評価実験では, COCO minival set において COCO-style mPA で評価して, Faster R-CNN を 2.0%, Faster R-CNN with Feature Pyramid Networks を 1.8% 向上</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2657.pdf" target="blank">[論文] Learning Globally Optimized Object Detector via Policy Gradient</a></li><li>強化学習の手法をCVのタスクに応用した例. 既存手法に提案手法を上乗せすることで精度を向上させているところが上手い.（強化学習の際の action の数が多くなり過ぎてしまう問題も, 事前学習済みの検出器に追加で学習を行うことで回避している.）</li><li>Policy Gradient の式を上手く Cross-Entropy Loss の特殊な場合となる様に変形することで"単純"で効果的な手法となっている.</li></ul></div></div><div class="slide_index">[#102]</div><div class="timestamp">2018.6.23 20:33:55</div></div></section><section id="ID_Through-Wall_Human_Pose_Estimation_Using_Radio_Signals"><div class="paper-abstract"><div class="title">Through-Wall Human Pose Estimation Using Radio Signals</div><div class="info"><div class="authors">Mingmin Zha,et al</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Masaki Miyamoto</div><div class="item1"><div class="text"><h1>概要</h1><p>この研究では，壁や閉塞空間を通した正確な人間の姿勢推定を説明している．これはWiFiの電波が，壁を通り抜け人体に反射する現象を利用している．このとき，人間は無線信号に対してアノテーションを行うことができないため，最先端のビジョンモデルを用いる．具体的には，訓練中に同期された無線信号と視覚情報を用いてビジュアルストリームから姿勢情報を抽出し、それを使用して訓練プロセスを誘導する．いったん訓練されると，このシステムは姿勢推定のために無線信号のみを使用する．人が視認できる状態でテストすると、信号ベースのシステムは、それを訓練するために使用された視覚情報ベースのシステムとほぼ同じ精度であることがわかる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Through-Wall_pose.PNG" alt="Through-Wall_pose.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>コンピュータビジョンにおいてはキーポイントから姿勢を推定する際にこれまでのカメラなどのセンサから情報を得るのではなく，高周波信号を用いている．モデリング面においては教師 - 学生ネットワークを用いている．そのため，このネットワークは具体的な信頼できるキーポイントのマップに関するより豊かな知識を伝達する．ワイヤレス面においては，時間の異なる時点で検出された複数の身体部分を費えることによって、壁の後ろの姿勢の不鮮明な説明を作成するRF-Captureと呼ばれるシステムとなっている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul></ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2406.pdf">論文</a></li><li><a href="https://www.youtube.com/watch?v=HgDdaMy8KNE">デモ動画</a></li></div></div><div class="slide_index">[#103]</div><div class="timestamp">2018.6.23 16:41:59</div></div></section><section id="ID_DiverseNet_When_One_Right_Answer_is_not_Enough"><div class="paper-abstract"><div class="title">DiverseNet: When One Right Answer is not Enough</div><div class="info"><div class="authors">Michael Firman et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>教師あり学習において, test 時に同じ入力から異なる結果を出力可能にする Loss と学習方法 (DiverseNet) を提案. 提案手法はあらゆる教師あり学習の手法に対して適用が可能であり, 提案された Loss は GAN などで報告されている mode-collapse を起こしにくい. 複数のタスクに対して評価実験を行い有効性を確認した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-DiverseNet-When-One-Right-Answer-is-not-Enough.png" alt="fukuhara-DiverseNet-When-One-Right-Answer-is-not-Enough.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>学習の画像と一緒に制御変数（整数）を入力する, 制御変数を変更することで test 時に同じ画像から異なる結果を得られる</li><li>複数の正解ラベルについて Loss の和をとると mode-collapse を起こしやすいため, 提案された Loss では各ラベルについてそれぞれ Loss を計算し, 最小の値を取ったものを Loss として使用 　</li><li>提案手法はあらゆる教師あり学習の手法に対して適用が可能. また, 正解ラベルが１つしか無いタスクにおいても, 最もらしい結果を複数生成可能</li><li>評価実験では提案手法を 2D image completion, 3D volume estimation, flow prediction などの複数のタスクにおける手法に適用し, 特に小さなネットワークのモデルに対して良い結果となった</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.05472" target="blank">[論文] Optimizing Video Object Detection via a Scale-Time Lattice</a></li><li><a href="http://mmlab.ie.cuhk.edu.hk/projects/ST-Lattice/" target="blank">[Project page] Optimizing Video Object Detection via a Scale-Time Lattice</a></li></ul></div></div><div class="slide_index">[#104]</div><div class="timestamp">2018.5.17 12:19:55</div></div></section><section id="ID_Attention_Clusters_Purely_Attention_Based_Local_Feature_Integration_for_Video_Classification"><div class="paper-abstract"><div class="title">Attention Clusters: Purely Attention Based Local Feature Integration for Video Classification</div><div class="info"><div class="authors">Kiang Long et al.</div><div class="conference">CVPR2018</div><div class="paper_id">1711.09550</div></div><div class="slide_editor">TakumuIkeya</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>動画のクラス分類タスクにおいて時系列の情報，特に長期間のパターンは必要な情報ではないことを示し，純粋にattentionに基づいた局所特徴の統合フレームワークを提案をした研究である．</li><li>提案したフレームワークを用いて動画分類タスクを実行することで評価した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Attension_Clusters.PNG" alt="Attension_Clusters.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>提案したフレームワークはKineticsデータセットにおいてtop-1で79.4%,top-5で94.0%の精度を達成した．</li><li>提案したフレームワークではシフト操作を伴うMultimodal Attention Clustersを導入することでフレームの類似性が高い動画に対しても良好な結果が得られる</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1711.09550.pdf">論文</a></li></ul></div></div><div class="slide_index">[#105]</div><div class="timestamp">2018.6.22 22:56:48</div></div></section><section id="ID_CVM-Net_Cross-View_Matching_Network_for_Image-Based_Ground-to-Aerial_Geo-Localization"><div class="paper-abstract"><div class="title">CVM-Net: Cross-View Matching Network for Image-Based Ground-to-Aerial Geo-Localization</div><div class="info"><div class="authors">Sixing Hu et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p> Ground-to-Aerial Geolocalization の研究. CNNを用いて局所特徴量を抽出した後, NetVLAD によって局所特徴量から大域特徴量を生成してマッチングを行う. また, 新しい Loss を提案し学習時間を短縮した. CVUSA dataset 等を用いて行った評価実験では既存手法に大差で優位な結果を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-CVM-Net-Cross-View-Matching-Network-for-Image-Based-Ground-to-Aerial-Geo-Localization.png" alt="fukuhara-CVM-Net-Cross-View-Matching-Network-for-Image-Based-Ground-to-Aerial-Geo-Localization.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>地上で撮影された写真から, 衛星写真上のどの位置で撮影されたかを推定する（Ground-to-Aerial Geolocalization）</li><li>両方の写真からCNNを用いて局所特徴量を抽出した後, NetVLAD によって局所特徴量から大域特徴量を生成, 後述の weighted soft margin ranking loss を用いて学習を行う</li><li>新しく提案した weighted soft margin ranking loss は従来の soft-margin triplet loss よりも学習の収束の速度を早めると共に, ネットワークの精度を向上させた</li><li>CVUSA dataset と Vo and Hays dataset を用いて行った評価実験では既存手法に大差で優位な結果を示した（評価基準は上位 1% の recall）. 特にパノラマ写真を入力とした場合は90%以上の精度を達成</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_CVM-Net_Cross-View_Matching_CVPR_2018_paper.pdf" target="blank">[論文] CVM-Net: Cross-View Matching Network for Image-Based Ground-to-Aerial Geo-Localization</a></li><li><a href="https://github.com/david-husx/crossview_localisation" target="blank">[Code] GitHub</a></li></ul></div></div><div class="slide_index">[#106]</div><div class="timestamp">2018.6.22 6:22:55</div></div></section><section id="ID_Cross-Domain_Self-supervised_Multi-task_Feature_Learning_using_Synthetic_Imagery"><div class="paper-abstract"><div class="title">Cross-Domain Self-supervised Multi-task Feature Learning using Synthetic Imagery</div><div class="info"><div class="authors">Zhongzheng Ren and Yong Jae Lee</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>人手によるアノテーションを使用しない本当の意味での自己教師学習を行うために、合成画像の法線マップ、デプス、物体輪郭と実画像とのadversarial trainingを行う手法を提案。実画像に対して汎用的な特徴量が取得できたことを主張している。
輪郭線はキャニーフィルタによるエッジだが、これによって人がつける曖昧なアノテーションを緩和することができる。
デプスを推定することで高次元のセマンティックな情報やオブジェクトの相対的な位置を得ることが可能。
既存研究により法線マップとデプスのそれぞれの推定が良い影響を与えることがわかっているため、法線マップの推定も行う。
GANの学習において、ディスクリミネータの更新は実画像、合成画像に対するGANのロス、ジェネレータの更新は合成画像に対するGANロス、
3つのタスクの推定におけるロスを使用している。ドメインに不変な特徴料を得るために実画像を用いたジェネレータの学習も行ったが、
精度が良くなかった。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Cross-Domain_Self-supervised_Multi-task_Feature_Learning_using_Synthetic_Imagery.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>人手によるアノテーションを使用せずに自己教師学習を行うために合成画像の法線マップ、デプス、オブジェクトの輪郭を推定するネットワークを構築し、さらに実画像に対して汎用的な特徴量を得るために実画像とのadversarial trainingを行う。</li><li>PASCAL VOCを用いた最近傍によるリトリーバルを行った。トレーニングデータにはバスや車などの区別しづらい画像が含まれているにも関わらず、車を入力した際には車のりトリーバルに成功。</li><li>conv1ですでにガボールフィルタのような特徴量を取得できていることを確認。これはImageNetをただ学習させるだけでは得ることができないことを確認している。</li><li>Pascal VOCを用いたクラシフィケーション、ディテクションにおいてSoTAと同等の精度を達成。</li><li>クラシフィケーションとディテクションに対して3つのタスクのうちどれが効果的なのか、どの層の特徴量が効果的なのか、domain adaptaionを行う際にどの層の特徴量が効果的なのかを検証。</li><li>NYUDデータセットを用いた法線推定において、既存の自己教師学習と比べてSoTAを達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0408.pdf">論文</a></li><li><a href="https://jason718.github.io/project/cvpr18/main.html">Project page</a></li></ul></div></div><div class="slide_index">[#107]</div></div></section><section id="ID_Dynamic_Feature_Learning_for_Partial_Face_Recognition"><div class="paper-abstract"><div class="title">Dynamic Feature Learning for Partial Face Recognition</div><div class="info"><div class="authors">Lingxiao He, Haiqing Li, Qi Zhang, Zhenan Sun</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>マスクなどから見えている顔領域のみを検出するPartial face recognition(PFR)をFCNで高速かつ高精度に行う手法を提案。トレーニング時には顔全体と顔が見えているパッチのそれぞれに対してパラメタを共有したFCNをで特徴量マップを適用し、
パッチ領域から得られる特徴量マップと同サイズのマップを顔全体からえられた特徴量マップからスライディングウィンドウによって複数個切り出し、
パッチから得られた特徴量マップとの比較を行う。
この比較のことをDynamic Feature Matching(DFM)と読んでいる。
DFMを行う際の工夫として、パッチから得られた特徴量マップを顔全体から得られた特徴量ウィンドウの線形和で表す際の重み、
パッチから得られた特徴量マップと特に類似している特徴量ウィンドウに対する重みの学習を行っている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Dynamic_Feature_Learning_for_Partial_Face_Recognition.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>PFMを行う際に顔全体から得られた特徴量マップを切り出した複数の特徴量ウィンドウと顔パッチ部分から得られた、特徴量ウィンドウと同サイズの特徴量マップを比較するDFMを行う手法を提案。</li><li>既存手法であるMR-CNNの20倍の速度で実行可能。</li><li>CASIA-WebFace 1万枚を用いて学習。LFWなどのデータセットでテストを行う。face recognition, verificationにおいてSoTA。</li><li>切り取るサイズや、パラメタに対する考察も行っている。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>FCNを用いることで任意のサイズの入力を扱えることに着目したことが根幹となるアイディア。</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/He_Dynamic_Feature_Learning_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#108]</div></div></section><section id="ID_Mean-Variance_Loss_for_Deep_Age_Estimation_from_a_Face"><div class="paper-abstract"><div class="title">Mean-Variance Loss for Deep Age Estimation from a Face</div><div class="info"><div class="authors">Hongyu Pan, Hu Han, Shiguang Shan, Xilin Chen</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔画像から年齢を推定する際に正確に年齢を推定するのではなく、ガウス分布を用いてある程度幅のある推定を行う手法を提案。大きなコントリビューションはロス関数としてガウス分布の平均値と分散に関するロスをとったことであり、
平均値はGTの年齢との差分をとり、分散は分布がよりシャープになるようにロス関数を設計する。
学習の際には上記2つのロス関数の他に1歳刻みの年齢をそれぞれクラスと見立てソフトマックスロスを取る。
分布を学習する既存手法と異なる点は、提案手法ではGTの平均値、分散を使用しない点である。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Mean-Variance_Loss_for_Deep_Age_Estimation_from_a_Face.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>人間の年齢は正確に推定することは難しいが、ある程度の範囲内であれば推定は容易、という観察に基づいてロス関数を設計。</li><li>FG-NET, MORPH Album Ⅱ, CLAP2016, AADBデータセットにおいてMAE、CSを評価指標として使用し多くのテストプロトコルにおいてSoTA。</li><li>照明環境に依存し、顔が赤い光で照らされているなどの特殊な照明環境では推定誤差が大きい。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul> <li>年齢推定だけでなく、同様の性質を持つタスクならば適用可能。</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2139.pdf">論文</a></li></ul></div></div><div class="slide_index">[#109]</div></div></section><section id="ID_Anatomical_Priors_in_Convolutional_Networks_for_Unsupervised_Biomedical_Segmentation"><div class="paper-abstract"><div class="title">Anatomical Priors in Convolutional Networks for Unsupervised Biomedical Segmentation</div><div class="info"><div class="authors">Adrian V. Dalca, John Guttag, Mert R. Sabuncu</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>MRIのスキャンデータに対するセグメンテーションを、MRIのソース画像とセグメント画像のペアを使用せずに行う手法を提案。はじめにセグメント画像のみを用いてVAEを学習。 
次に教師無しでセグメンテーションを行うためにdecoderの重みを固定してソース画像に対するセグメンテーションの推定を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Anatomical_Priors_in_Convolutional_Networks_for_Unsupervised_Biomedical_Segmentation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>医療用画像に対する教師無しのセグメンテーション手法を初めて提案。</li><li>T1w scanデータセットのうち、5000枚のセグメンテーション画像を使用してauto-encoderをプリトレーニング。残りの9000枚のスキャンデータを用いて教師無し学習。</li><li>T1wデータセットよりも解像度が低く、スライス間隔も広いT2-FLAIR scanデータセットでもテストを実行。ただしアノテーションが存在しないのでセグメンテーションの見た目で良し悪しを判断。</li><li>評価尺度はGTとの領域の重なりを評価するDice。Dice、セグメンテーションの結果の見た目として良好な結果が得られていると主張。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>Diceを使って定量的に評価しているため、境界線の引き方などの細かい部分のセグメンテーション結果を詳細に評価していないが、実用上は問題無いのだろうか？</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Dalca_Anatomical_Priors_in_CVPR_2018_paper.pdf">論文</a></li><li> <a href=" http://openaccess.thecvf.com/content_cvpr_2018/Supplemental/4335-supp.pdf">Supplementary material</a></li><li><a href=" https://github.com/adalca/neuron">GitHub</a></li></ul></div></div><div class="slide_index">[#110]</div></div></section><section id="ID_GeoNet_Unsupervised_Learning_of_Dense_Depth_Optical_Flow_and_Camera_Pose"><div class="paper-abstract"><div class="title">GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose</div><div class="info"><div class="authors">Author</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>単視点動画に映っている物体を静的物体と動的物体に分離することで教師なしでデプス、オプティカルフロー、カメラ向きを推定する手法を提案。フレームワークは二段階で構成されており、
まずはじめにデプスとカメラ向きをそれぞれ独立に推定することで道路や街路樹などの静的物体のモーション情報を得る。
続いて静的物体との差分情報を使用することで歩行者などの動的物体のモーション情報を得る。教師無しの推定を行うため、
参照フレームから推定されたモーション情報の逆変換をターゲットフレームに適用し参照フレームを推定することで
consistency lossをとることで精度が向上。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/GeoNet_Unsupervised_Learning_of_Dense_Depth_Optical_Flow_and_Camera_Pose.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>consistency lossによってオクルージョンに対する精度の向上も確認。</li><li>同じネットワークを持つ既存研究に対して、ロス関数の優位性を確認</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://github.com/yzcjtr/GeoNet">GitHub</a></li></ul></div></div><div class="slide_index">[#111]</div></div></section><section id="ID_CSGNet_Neural_Shape_Parser_for_Constructive_Solid_Geometry"><div class="paper-abstract"><div class="title">CSGNet: Neural Shape Parser for Constructive Solid Geometry</div><div class="info"><div class="authors">Gopal Sharma et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Shape Parsing の研究. ２次元画像, ３次元ボクセルから同じ形状を生成するプログラムを推定する. 学習のための2次元や３次元のLogoやCADモデルなどを含む synthetic dataset を作成・公開した. また, 教師データが無い場合でも強化学習を用いた学習が可能.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-CSGNet_Neural_Shape_Parser_for_Constructive_Solid_Geometry.png" alt="fukuhara-CSGNet_Neural_Shape_Parser_for_Constructive_Solid_Geometry.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>入力された形状からCNNで特徴量を抽出し, RNN（GRUs） によって形状を生成する一連のプログラムを生成</li><li>Ground Truth が無い場合は強化学習(Policy Gradient)で学習可能（評価実験では教師ありと強化学習を組み合わせたものが一番高精度）</li><li>2次元や３次元の形状とそれを生成するプログラムのデータセット(2D and 3D synthetic dataset)を作成・公開</li><li>評価実験では, 2次元と３次元のいずれの場合も Nearest Neighbor を用いた手法よりも高精度を達成</li><li>また, Primitive detection のタスクにおいては Faster R-CNN よりも高い Mean Average Precision を達成</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://people.cs.umass.edu/~kalo/papers/CSGNet/CSGNet.pdf" target="blank">[論文] CSGNet: Neural Shape Parser for Constructive Solid Geometry</a></li><li><a href="https://github.com/hippogriff/CSGNet" target="blank">[Code] GitHub</a></li></ul></div></div><div class="slide_index">[#112]</div><div class="timestamp">2018.6.22 19:37:55</div></div></section><section id="ID_Context_Embedding_Networks"><div class="paper-abstract"><div class="title">Context Embedding Networks</div><div class="info"><div class="authors">Kun Ho Kim, Oisin Mac Aodha and Pietro Perona</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ラベル付けする人の評価尺度やcontextを考慮して画像の類似度を求めるContext Embedding Networksを提案した。クラウドワーカーによるアノテーションは、個人独自の評価尺度やコンテキストに影響される。
例えば、人物顔画像をクラスタリングする際にはある人は性別によってクラスタリングするが、別の人は表情によってクラスタリングしてしまうと考えられる。
そこで、workerと見せた画像(context)それぞれから、画像のどのような点に注目するかを表すattributeをAttribute Encoderにより求める。
画像の類似度は、2枚の画像それぞれに対してImage Encoderから得られる画像特徴を、attributeによる重みつきの類似度によって求める。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Context_Embedding_Networks.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>クラウドワーカーに応じた類似度の算出が可能になった。各クラウドワーカーがどのattributeに基づいて画像クラスタリングをしているかを予測することに成功した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>クラウドソーシングによるアノテーションにおいて、クラウドワーカーの個人差は避けては通れないので重要な問題になりそう。</p><ul><li><a href="https://arxiv.org/abs/1710.01691">論文</a></li></ul></div></div><div class="slide_index">[#113]</div><div class="timestamp">2018.6.22 18:15:24</div></div></section><section id="ID_Visual_Feature_Attribution_using_Wasserstein_Gans"><div class="paper-abstract"><div class="title">Visual Feature Attribution using Wasserstein Gans</div><div class="info"><div class="authors">Christian F. Baumgartner, Lisa M. Koch, Kerem Can Tezcan and Jia Xi Ang</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像中のどの箇所がクラス分類に寄与するかを可視化する手法を提案。多くの手法は、クラス分類のタスクを学習することで重要な特徴を調べている。
しかし、識別への寄与が強い特徴が存在する場合ネットワークは強い特徴のみに注目してしまい、他の特徴は無視されてしまう。
医療画像からの病気の診断では、病気のステージを見極める、複数の要因が絡む病気を発見するなど無視されてしまう特徴を探すことは極めて重要である。
本研究では、Wasserstein GANを用いてある病気を発見する上で重要な領域を示したマップMを生成する。
病気のラベルがついた入力画像xに対して、x+Mが病気でないと判定されるMを生成するGeneratorを学習する。
その際、患者の個人性による画像の違いを考慮するためにL1正則化項をロスに加える。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Visual_Feature_Attribution_using_Wasserstein_Gans.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>合成画像と実際の医療画像の2種類により評価した。従来の特徴を可視化する手法は、病気の際に見られる特徴のうち一部しか取れない、エッジなどの高周波情報が取れないという結果に対して、提案手法はこれら2つを改善した。
Normalized Cross Correlation(NCC)による数値評価では、ベースラインと比べ提案手法が最も良い数値を記録した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://github.com/orobix/Visual-Feature-Attribution-Using-Wasserstein-GANs-Pytorch">コード</a></li><li><a href="https://arxiv.org/abs/1711.08998">論文</a></li></ul></div></div><div class="slide_index">[#114]</div><div class="timestamp">2018.6.22 17:18:25</div></div></section><section id="ID_Learning_to_Estimate_3D_Human_Pose_and_Shape_from_a_Single_Color_Image"><div class="paper-abstract"><div class="title">Learning to Estimate 3D Human Pose and Shape from a Single Color Image</div><div class="info"><div class="authors">Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou and Kostas Daniilidis</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>1枚のRGB画像から人間の全身の3次元モデルを推定するEnd-to-Endのネットワークを提案した。DNNを用いた3次元モデルの推定は、膨大なアノテーションが必要となり現実的ではない。
そこで、画像からの2次元特徴の抽出と2次元特徴から3次元モデルの推定の2段階に分けることによりDNNベースの手法を実現する。
始めに、Human2DというRGB画像から2次元の特徴点及び人物のシルエットを推定する。
2次元特徴点及びシルエットから3次元モデルの推定には、<a href="https://dl.acm.org/citation.cfm?id=2818013">SMPL</a>という統計モデルを用いて作成した学習データにより学習を行う。
加えて、得られた三次元モデルから2次元特徴点とシルエットを取得し、画像から得られた情報と一致するかをロスに加える。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Estimate_3D_Human_Pose_and_Shape_from_a_Single_Color_Image.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>推定した3次元モデルの誤差を評価したところ、提案手法が最もground truthに近づいたことを確認した。1枚の画像に対して50msという従来研究と比べ大幅に高速化することができた。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>データ作成の問題をCGを駆使して解決しており、同様のアイデアを活用できないだろうか？</p><ul><li><a href="https://arxiv.org/abs/1805.04092">論文</a></li></ul></div></div><div class="slide_index">[#115]</div><div class="timestamp">2018.6.22 16:05:18</div></div></section><section id="ID_Zero_shot_Kernel_Learning"><div class="paper-abstract"><div class="title">Zero shot Kernel Learning</div><div class="info"><div class="authors">Hongguang Zhang and Piotr Koniusz</div><div class="conference">CVPR2018</div></div><div class="slide_editor">TengaWakamiya</div><div class="item1"><div class="text"><h1>概要</h1><p>ゼロショット学習のオープンな問題に取り組む上で，カーネルを利用したゼロショット学習の手法を提案する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/zeroshot_1.png" alt="zeroshot_1.png"><img src="slides/figs/zeroshot_2.png" alt="zeroshot_2.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>提案する手法は，回転とスケーリングが組み込まれているため，制約のないモデルでは，より自由度が高いために過学習を防止することができる．1枚目の画像はゼロショットカーネルの配置．
2枚目の画像は一般化ゼロショット学習プロトコルと新たに提案されたデータ集合についての評価．
（tr）はtrain + testクラス，（ts）はテストクラスの平均トップ1精度，(H)はハーモナイズされたスコア，(Better than SOA）は提案手法が他の最先端の方法（表の上部）よりも優れているデータセットの数を示す．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.01279">link1</a></li></ul></div></div><div class="slide_index">[#116]</div><div class="timestamp">2018.6.20 19:56:59</div></div></section><section id="ID_VITAL"><div class="paper-abstract"><div class="title">VITAL: VIsual Tracking via Adversarial Learning</div><div class="info"><div class="authors">Yibing Song, Chao Ma, Xiaohe Wu, Lijun Gong, Linchao Bao, Wangmeng Zuo, Chunhua Shen, Rynson Lau, Ming-Hsuan Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="" target="blank">Takahiro Itazuri</a></div><div class="item1"><div class="text"><h1>概要</h1><p>tracking-by-detectionベースの手法は、(1)各フレームにおけるpositive sampleが空間的に重なった領域を取りやすいため、十分な見た目のばらつきを学習できない点と(2)positive sampleとnegative sampleの不均等さ（class imbalance）が顕著に出てしまうという点が問題である。本論文では、positive sampleのデータ拡張を行うため、GANを用いて長い時間のスパンで頑健な特徴を学習可能なVITALアルゴリズムを提案した。またclass imbalanceを解決するため、識別が容易なnegative sampleを取り除くためのhigh-order cost sensitive lossを提案した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/VITAL.png" alt="VITAL"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>提案手法はCNNで抽出した特徴量に適用するマスクを複数（論文では9個）用意し、マスクを通じて重み付けられた特徴量に対して識別器Dが対象物体か背景かの二値分類を行う。学習時には識別器Dに最も悪い識別性能を出させたマスクを学習させる。テスト時には生成器Gは取り除いておく。また識別が簡単すぎる大量のnegative sampleのロスが合計されて大きくなってしまう現象であるclass imbalanceを、あまり学習に寄与しないようにする。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1804.04273.pdf">論文</a></li><li><a href="https://youtu.be/uGMoOom6_90">デモ動画</a></li><li><a href="https://ybsong00.github.io/cvpr18_tracking/index">プロジェクト</a></li><li><a href="https://github.com/ybsong00/Vital_release">GitHub</a></li></ul></div></div><div class="slide_index">[#117]</div></div></section><section id="ID_SINT"><div class="paper-abstract"><div class="title">SINT++: Robust Visual Tracking via Adversarial Positive Instance Generation</div><div class="info"><div class="authors">Xiao Wang, Chenglong Li, Bin Luo, Jin Tang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="" target="blank">Takahiro Itazuri</a></div><div class="item1"><div class="text"><h1>概要</h1><p>物体追跡タスクでは追跡対象の画像を１フレーム目においてのみ与えられるため、トレーニングデータの多様性が不足していることがDNNを適用する際の障壁となっている。そこで変形や遮蔽といった困難な環境下における正解サンプルを生成する手法（SINT++）を提案した。提案手法は他の物体追跡手法に取り入れることが可能である点も非常に重要である。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SINT++.png" alt="SINT++"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>VAEを用いて追跡対象の多様体を生成し、その多様体局面上を移動させることで正解サンプルを増やすネットワーク（PSGN）と識別器の認識性能にクリティカルな領域を探すように遮蔽領域を決定する強化学習ネットワーク（HPTN）を用いて、正解サンプルの多様性を増幅させる。追跡器はSINTを用いているため、与えられた追跡対象の画像に対するオフライン学習も、追跡中のオンライン学習も行わない。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SINT_Robust_Visual_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://sites.google.com/view/cvpr2018sintplusplus">プロジェクト</a></li></ul></div></div><div class="slide_index">[#118]</div></div></section><section id="ID_Occlusion_Aware_Unsupervised_Learning_of_Optical_Flow"><div class="paper-abstract"><div class="title">Occlusion Aware Unsupervised Learning of Optical Flow</div><div class="info"><div class="authors">Yang Wang, Yi Yang, Zhenheng Yang, Liang Zhao, Peng Wang, Wei Xu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="" target="blank">Takahiro Itazuri</a></div><div class="item1"><div class="text"><h1>概要</h1><p>オプティカルフローのアノテーションが困難であることから、教師なし学習ベースのオプティカルフロー推定手法が提案されているが、十分な精度が出ていない。そこで問題とされている遮蔽と大きな動きに対応したネットワークを提案。教師なし学習ベースの手法では最も良い精度を出し、教師あり学習ベースの手法とのギャップを埋めた。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Occlusion_Aware_Unsupervised_Learning_of_Optical_Flow.png" alt="Occlusion_Aware_Unsupervised_Learning_of_Optical_Flow"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>２枚の画像に対して、１枚目から２枚目へのオプティカルフローと、２枚目から１枚目のオプティカルフローを推定する。２枚目の画像と前者のオプティカルフローを用いて、１枚目の画像を復元する。復元した１枚目の画像のうち遮蔽が発生していない部分に対して、本物の１枚目の画像との差を損失として用いる。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1711.05890.pdf">論文</a></li></ul></div></div><div class="slide_index">[#119]</div></div></section><section id="ID_Learning_Attentions_Residual_Attentional_Siamese_Network_for_High_Performance_Online_Visual_Tracking"><div class="paper-abstract"><div class="title">Learning Attentions: Residual Attentional Siamese Network for High Performance Online Visual Tracking</div><div class="info"><div class="authors">Qiang Wang, Zhu Teng, Junliang Xing, Jin Gao, Weiming Hu, Steve Maybank</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="" target="blank">Takahiro Itazuri</a></div><div class="item1"><div class="text"><h1>概要</h1><p>物体追跡のためのオフライン学習ベースの手法は精度とスピードにおいて高いポテンシャルがあるが、追跡対象に適応させることは困難である。一方で、オンライン学習ベースの手法は計算コストとオーバーフィッティングが問題になっている。本論文では、Siamese NetworkにおけるCross CorrelationをAttentionで重み付けしたRASNet（Residual Attentional Siamese Network）を提案し、リアルタイムを超える速度（83fps）とSOTAを実現した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/RASNet.png" alt="RASNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>Siamese NetworkにAttention Mechanismを導入した。Attention MechanismにはResidual AttentionとGeneral Attentionを含むDual Attentionと、Channel Attentionを導入した。Resiual Attentionは追跡対象に特化させるようにオンライン学習をし、Channel Attentionはチャンネルごとの特徴量の質を示している。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="http://www.dcs.bbk.ac.uk/~sjmaybank/CVPR18RASTrackCameraV3.3.pdf">論文</a></li><li><a href="https://github.com/foolwood/RASNet">GitHub</a></li></ul></div></div><div class="slide_index">[#120]</div></div></section><section id="ID_Im2Flow"><div class="paper-abstract"><div class="title">Im2Flow: Motion Hallucination from Static Images for Action Recognition</div><div class="info"><div class="authors">Ruohan Gao, Bo Xiong, Kristen Grauman</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="" target="blank">Takahiro Itazuri</a></div><div class="item1"><div class="text"><h1>概要</h1><p>人間が一枚の静止画から動き情報を推定可能であることを受け、一枚の静止画から動き情報（フロー）の事前知識を得る手法を提案。具体的には動き情報の表現方法とU-Netの構造を変形させたエンコーダ・デコーダネットワークを提案。提案手法で得たフロー情報を利用することで、行動認識の精度が向上した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Im2Flow.png" alt="Im2Flow"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>動き情報を動きの大きさと角度（角度はコサインとサインに分解）の計３チャンネルで表現する。角度は周期的な構造であるが、三角関数を用いることでこれを避けることができる。損失関数は(1)フロー自体の損失と(2)動き情報のコンテンツの損失の和で構成される。動き情報のコンテンツは、ResNetをUCF-101データセット上で行動認識にfine-tuningさせたものから取得し、推定したフローと正解のフローから得られたコンテンツの差から損失を得る。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.04109.pdf">論文</a></li></ul></div></div><div class="slide_index">[#121]</div></div></section><section id="ID_High-speed_Tracking_with_Multi-kernel_Correlation_Filters"><div class="paper-abstract"><div class="title">High-Speed Tracking With Multi-Kernel Correlation Filters</div><div class="info"><div class="authors">Ming Tang, Bin Yu, Fan Zhang, Jinqiao Wang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="" target="blank">Takahiro Itazuri</a></div><div class="item1"><div class="text"><h1>概要</h1><p>物体追跡タスクにおいて、Multi-Kernel Correlation Filter (MKCF)はKernelized Correlation Filter (KCF)のカーネルを複数にすることで識別性能を向上させているが、計算量がボトルネックとなっていた。そこで目的関数の上界を目的関数として再設定し、上から押さえるように最適化問題を解くことで、MKCFより高速（150fps）かつ高識別性能な物体追跡手法 (MKCFup)を提案した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/MKCFup.png" alt="MKCFup"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>MKCFupは従来のMKCFの最適化問題における目的関数の上界を最適化する。上界を最適化する問題に再定式化することで高速かつ高精度な追跡を実現しており、DNNを使っていない数少ない論文の1つ。Correlation FilterがDNNベースの物体追跡に利用されているように、今後DNNベースの物体追跡手法が使用する可能性がある。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Tang_High-Speed_Tracking_With_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#122]</div></div></section><section id="ID_High_Performance_Visual_Tracking_with_Siamese_Region_Proposal_Network"><div class="paper-abstract"><div class="title">High Performance Visual Tracking With Siamese Region Proposal Network</div><div class="info"><div class="authors">Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, Xiaolin Hu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="" target="blank">Takahiro Itazuri</a></div><div class="item1"><div class="text"><h1>概要</h1><p>オフラインで学習させたDNNで得た特徴量を使用した物体追跡手法は、ターゲットの動画に特有の情報を使用していないことから、相関フィルタベースの手法より良い精度が出ていなかった。提案手法は大規模な画像ペアデータを用いて学習し、同じ特徴量抽出器を２つの入力に適応させて得た特徴量の類似度を比較するSiamese NetworkとFaster R-CNNで提案されているRegion Proposal Network（RPN）を組み合わせた上で、物体追跡をlocal one-shot detectionとして定式化することで、高速かつ高精度な追跡を実現した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Siamese-RPN.png" alt="Siamese-RPN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>従来のSiamese Networkを利用した手法とは異なり、RPNを用いることで物体の変形に合わせた矩形領域を提示することによって高い精度を出すことが可能である。また物体追跡をlocal one-shot detectionとして定式化する。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#123]</div></div></section><section id="ID_End-to-End_Learning_of_Motion_Representation_for_Video_Understanding"><div class="paper-abstract"><div class="title">End-to-End Learning of Motion Representation for Video Understanding</div><div class="info"><div class="authors">Lijie Fan, Wenbing Huang, Chuang Gan, Stefano Ermon, Boqing Gong, Junzhou Huang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="" target="blank">Takahiro Itazuri</a></div><div class="item1"><div class="text"><h1>概要</h1><p>深層学習の成功に反して映像解析では未だに手作りのオプティカルフローが使用されている。通常のオプティカルフローは、それを利用したCNNと独立してしまっている点と時間的・空間的計算コストが非常に大きい点が問題である。本論文では、オプティカルフローに代わる特徴をEnd-to-Endに学習可能なネットワーク（TVNet）を提案した。End-to-Endに学習可能になることで、特定のタスクに特化した動き特徴量を学習できる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/TVNet.png" alt="TVNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>オプティカルフロー抽出手法の1つであるTV-L1をDNNにカスタマイズさせた。End-to-Endのネットワークにすることで、フロー抽出後のタスクから得られた誤差を伝搬することができるため、特定のタスクに特化した動き情報の抽出が可能となっている。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1804.00413.pdf">論文</a><a href="https://github.com/LijieFan/tvnet">GitHub</a></li></ul></div></div><div class="slide_index">[#124]</div></div></section><section id="ID_End-to-End_Flow_Correlaion_Tracking_with_Spatial-temporal_Attention"><div class="paper-abstract"><div class="title">End-to-End Flow Correlation Tracking with Spatial-temporal Attention</div><div class="info"><div class="authors">Zheng Zhu, Wei Wu, Wei Zou, Junjie Yan</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="" target="blank">Takahiro Itazuri</a></div><div class="item1"><div class="text"><h1>概要</h1><p>従来のCorrelation Filterベースの物体追跡手法は現在のフレームの見た目しか考慮できておらず、フレーム間の情報や動きの情報を考慮していなかった。本論文ではフロー情報を直接的に考慮することで時間変化に関する情報を考慮することが可能な物体追跡手法を提案した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/FlowTrack.png" alt="FlowTrack"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>通常のネットワークに対してフロー情報を追加しただけではなく、Spatial AttentionとTemporal Attentionも提案した。これにより空間情報と時間情報を効率的に考慮することが可能となった。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1711.01124.pdf">論文</a></li></ul></div></div><div class="slide_index">[#125]</div></div></section><section id="ID_Efficient_Diverse_Ensemble_for_Discriminative_Co-Tracking"><div class="paper-abstract"><div class="title">Efficient Diverse Ensemble for Discriminative Co-Tracking</div><div class="info"><div class="authors">Kourosh Meshgi, Shigeyuki Oba, Shin Ishii</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="" target="blank">Takahiro Itazuri</a></div><div class="item1"><div class="text"><h1>概要</h1><p>tracking-by-detectionベースの物体追跡手法は識別器の不完全性からオンライン自己学習するため、自己学習のループでドリフト問題が発生する。そこで学習する識別器に対する教師が必要であるという発想から、相補的に教師になるアンサンブル学習ベースの手法が提案されている。しかし、アンサンブル学習ベースの手法は、各識別器が互いに重複した領域を対象にする冗長性が発生する。本論文ではその冗長性を軽減することが可能なリアルタイム物体追跡手法（DEDT: Diversified Ensemble Discriminative Tracker）を提案する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DEDT.png" alt="DEDT"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>DEDTは高い適応性と多様性を持つ識別器群であるCommitteeモデルと長期記憶を持つAuxiliaryモデルからなり、Committeeモデルが不明確な回答を出した入力に対しては、Auxiliaryモデルが代わりに回答する。Committeeモデルは自身が不明確な回答をしたデータを用いて学習する。またこれまでのデータから不明確な回答になるようなデータを人工的に生成し、そのデータにおけるエラー率が、推定時に冗長な結果が得られたデータのエラー率より小さくなるまで繰り返し、更新することで、冗長性を回避する。一方でAuxiliaryモデルはCommitteeモデルより更新頻度が低くすることで長記憶性を持つ。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Meshgi_Efficient_Diverse_Ensemble_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#126]</div></div></section><section id="ID_Correlation_Tracking_via_Joint_Discrimination_and_Reliability_Learning"><div class="paper-abstract"><div class="title">Correlation Tracking via Joint Discrimination and Realiability Learning</div><div class="info"><div class="authors">Chong Sun, Dong Wang, Huchuan Lu, Ming-Hsuan Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="" target="blank">Takahiro Itazuri</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Correlation Filterベースの物体追跡手法は識別性と信頼性を学習するべきであるが、従来手法は識別性に着目したものが多く、Bounding Box内の予期されない顕著な領域に影響を受ける可能性がある。本論文では信頼性の高い領域に特に着目して物体追跡を行う手法（DRT）を提案した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DRT.png" alt="DRT"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>提案手法は識別性を保持するbase filterと信頼性を保持するreliability termのアダマール積を取ることで、より信頼性の高い領域に着目する。目的関数には学習サンプルの分類誤差に関する項と、局所応答に一貫性を持たせる制約項、L2ノルム正則化項からなる。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Correlation_Tracking_via_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#127]</div></div></section><section id="ID_Context-aware_Deep_Feature_Compression_for_High-speed_Visual_Tracking"><div class="paper-abstract"><div class="title">Context-aware Deep Feature Compression for High-speed Visual Tracking</div><div class="info"><div class="authors">Jongwon Choi, Hyung Jin Chang, Tobias Fischer, Sangdoo Yun, Kyuewang Lee, Jiyeoup Jeong, Yiannis Demiris, Jin Young Choi</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="" target="blank">Takahiro Itazuri</a></div><div class="item1"><div class="text"><h1>概要</h1><p>コンテキストを考慮したCorrelation Filterによる物体追跡手法を提案した。カテゴリごとに事前学習したオートエンコーダーのエキスパートを複数用意し、その中からコンテキストネットワークが1つ選択する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/TRACA.png" alt="TRACA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>リアルタイム性が重要である物体追跡タスクでは、リアルタイムにDNNを学習することは困難である。本論文では事前に各物体のカテゴリ別に学習したオートエンコーダーを用意し、その中から1つを選択することで、ある程度既に特定の物体に特化したネットワークを使用できるため、再学習の必要性を軽減することができる。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.10537.pdf">論文</a></li></ul></div></div><div class="slide_index">[#128]</div></div></section><section id="ID_A_Twofold_Siamese_Network_for_Real-Time_Object_Tracking"><div class="paper-abstract"><div class="title">A Twofold Siamese Network for Real-Time Object Tracking</div><div class="info"><div class="authors">Anfeng He, Chong Luo, Xinmei Tian, Wenjun Zeng</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="" target="blank">Takahiro Itazuri</a></div><div class="item1"><div class="text"><h1>概要</h1><p>物体追跡手法の１つであるSiamFCは効率的なオフライン学習を行うことで、非常に高い識別性能を持つが、追跡対象の見た目の変化に弱かった。そこで、見た目特徴量とセマンティックな情報を別々に抽出する２つのSiamese Networkを利用することで、追跡対象の見た目変化にも強い物体追跡手法を提案した。セマンティックな情報を抽出するネットワークは画像分類タスクで学習させることで、見た目の変化に頑健な特徴量を抽出することが可能となる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SA-Siam.png" alt="SA-Siam"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>推論フェーズでは、それぞれのネットワークで別々に追跡対象画像と探索画像の類似度を計算し、それを統合する。セマンティックな情報を抽出するネットワークは、見た目変化には頑健ではあるが、識別性能は不十分であるため、与えれた追跡対象に反応するチャンネルの重要度を増やすChennel Attentionを追加する。これによって追跡対象に適応する最低限の機能を追加している。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/He_A_Twofold_Siamese_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#129]</div></div></section><section id="ID_GroupCap_Group-based_Image_Captioning_with_Structured_Relevance_and_Diversity_Constraints"><div class="paper-abstract"><div class="title">GroupCap: Group-based Image Captioning with Structured Relevance and Diversity Constraints</div><div class="info"><div class="authors">Fuhai Chen, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>画像グループ内での関連性や相関関係などを考慮し、キャプションを出力するGroupCapの提案。まず、個々の画像でvisual tree parser(VP-Tree)を構成し、文字ベースで意味の相関を構築。次にツリーの関係から、画像間での関連性と多様性をモデル化。この制約関係をもとにLSTMでキャプション生成。これらをトリプレットロスとしてend-to-endで学習する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180622GroupCap.jpg" alt="20180622GroupCap.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>従来のイメージキャプショニングでは、単一画像に対して説明文を生成している場合がほとんど。これらはオフラインで学習し、画像間での視覚的構造関係を無視して推定している。本手法のグループベースの手法によって、グループ画像内での構造的関連性や多様性を協調して学習することでキャプションの正確性を向上させる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>MSCOCOをもとに作成した2グループキャプションデータセットを使用して評価し、優れていることを示唆。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_GroupCap_Group-Based_Image_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#130]</div></div></section><section id="ID_MoNet_Deep_Motion_Exploitation_for_Video_Object_Segmentation"><div class="paper-abstract"><div class="title">MoNet: Deep Motion Exploitation for Video Object Segmentation</div><div class="info"><div class="authors">Huaxin Xiao, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>動画中の物体にセグメンテーションを行うタスクにおいて、フレーム間処理をモーションキューによって改善するMoNetの提案。オプティカルフローを利用し、その近傍の表現を統合することにより、ターゲットフレームでの表現を強化する。これにより、時間変化におけるコンテキスト情報を活用することができ、外観変動やモーションブラー、物体の変形に頑健となる。また、動作の一致性を考慮することで、ノイズの大きいモーションキューを前景または背景に変換し、精度を向上させている。 </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180622MoNet.jpg" alt="20180622MoNet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>セグメンテーションの改良と、フレームごとの学習を行うという観点からモーションキュー(オプティカルフロー)を利用している。これによって、前景と背景の分離する制度を向上。 また、distance transform layerを提案し、動作が一致しないインスタンスと領域をフィルタリングすることができる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>実験において、モーションキュー利用の有効性と、 distance transform layerの有効性を示している。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xiao_MoNet_Deep_Motion_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#131]</div></div></section><section id="ID_DeepMVS_Learning_Multi-view_Stereopsis"><div class="paper-abstract"><div class="title">DeepMVS: Learning Multi-view Stereopsis</div><div class="info"><div class="authors">Po-Han Huang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Learning-based Multi-View Stereo の研究. 任意の枚数の画像から, 視差 Map の推定を行う（推定結果は入力の順番に依存しない）. また, ネットワークの学習のため, 新しい synthetic datasets (MVS-SYNTH dataset) を作成・公開した. ETH3D を用いた評価実験では DeMoN を上回り, COLMAP と同等の結果を達成した. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-DeepMVS-Learning-Multi-view-Stereopsis.png" alt="fukuhara-DeepMVS-Learning-Multi-view-Stereopsis.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>複数枚の画像（1枚の参照画像と複数枚の近傍画像）を入力とする, Learning-based Multi-View Stereo（MVS）の手法を提案</li><li>入力画像に対して通常の SfM(COLMAP) を用いてポーズの推定を行った後, D段階の離散的な視差の大きさ毎に近傍画像を参照画像に Warp した画像群 (plane-sweep volume) を生成</li><li>参照画像と各 plane-sweep volume に対して Patch matching を行って抽出された特徴量を encoder-decoder 型のネットワークで統合した特徴量を用いて視差 Map を推定</li><li>ネットワークを上手く学習させるためには real と synthetic の両方のデータセットが重要であるとし, 新しい synthetic datasets (MVS-SYNTH dataset) を作成・公開した</li><li>ETH3D を用いた評価実験で COLMAP[Schonberger+16] と DeMoN[Ummenhofer+17] と比較した結果, ETH3D で最も精度の高い COLMAPと同等の Photometric error と Geometric error を達成</li><li>特に複雑で復元が難しいような環境に対しては, COLMAP がノイズの多い復元結果となる一方で, 提案手法は妥当な推定をする傾向が確認された</li><li>Limitation は植物の多い領域で視差 Map の推定に失敗やすいという点や, plane-sweep volumes の計算に時間がかかる点</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.00650" target="blank">[論文] DeepMVS: Learning Multi-view Stereopsis</a></li><li><a href="https://phuang17.github.io/DeepMVS/index.html" target="blank">[Project page] DeepMVS: Learning Multi-view Stereopsis</a></li><li><a href="https://github.com/phuang17/DeepMVS" target="blank">[Code] GitHub</a></li></ul></div></div><div class="slide_index">[#132]</div><div class="timestamp">2018.6.22 5:55:55</div></div></section><section id="ID_Learning_Compact_Recurrent_Neural_Networks_with_Block-Term_Tensor_Decomposition"><div class="paper-abstract"><div class="title">Learning Compact Recurrent Neural Networks with Block-Term Tensor Decomposition</div><div class="info"><div class="authors">Jinmian Ye et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kazuma Matsui</div><div class="item1"><div class="text"><h1>概要</h1><p>RNNは強力なシーケンスモデリングツールであるが，高次元の入力を扱う場合，RNNのトレーニングはモデルパラメータが大きくなるため計算に時間がかかるという問題がある．これは，RNNがビデオや画像キャプションのアクションレコグニションなど，多くの重要なコンピュータビジョンのタスクを行うことを妨げる．この問題を解決するためにRNNのパラメータを大幅削減し，トレーニング効率を向上させるコンパクトで柔軟な構造「Block-Termテンソル分解(BTD)」を提案し，これをBlock-Term RNN (BT-RNN)と名付ける．テンポトレインRNN (TT-RNN)のような他の低ランク近似とBT-RNNを比較すると，同じランクを使用する場合，より簡潔でより良い近似が可能であり，より少ないパラメータで元のRNNに戻すことが可能である．ビデオ，画像キャプション，画像生成のアクションレコグニションを含む3つの困難なタスクに対し，BT-RNNは予測精度と収束速度の両方でTT-RNNや標準のRNNより優れていると言える．この研究において，BT-LSTMはUCF11データセットのアクションレコグニションのタスクで15.6%以上の精度向上を達成するために，標準LSTMより17,388回少ないパラメータを使用した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_Compact_Recurrent_Neural_Networks_with_Block-Term_Tensor_Decomposition.jpg" alt="Learning_Compact_Recurrent_Neural_Networks_with_Block-Term_Tensor_Decomposition.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>BTDは最適なTT-rankの設定を見つけることを困難にする代わりに次のような利点がある．・Tucker分解は異なる次元間の相関関係を表し，より良い重み分担を達成するためにコアテンソルを導入している。
・コアテンソルのランクを等しくすることができ，異なる次元での不均衡な重みの共有を避けることができ，かつ入力データの異なる順列に対して頑強なモデルを導くことができる．
・BTDは，複数のTuckerモデルの合計を使用して高次テンソルを近似し，大きなTucker分解をいくつかのより小さいモデルに分割し，ネットワークを広げ，表現能力を高めることができる．
一方で複数のTuckerモデルは、，ノイズの多い入力データに対してより堅牢なRNNモデルを導く．
結果として，BTDを使用してRNNの入力非表示重み行列の接続をプルーニングすることにより，パラメータの数が少なく，フィーチャディメンション間の相関モデリングが強化された新しいRNNモデルが提供され，モデルトレーニングが容易になり，パフォーマンスが向上した．ビデオ行動認識データセットの実験結果は，BT-RNNアーキテクチャが数オーダのパラメータを消費するだけでなく，標準的な従来のLSTMおよびTT-LSTMよりもモデル性能を向上させることを示していると言える．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.05134.pdf">論文</a></li></ul></div></div><div class="slide_index">[#133]</div><div class="timestamp">2018.6.21 18:48:30</div></div></section><section id="ID_End-to-End_Dense_Video_Captioning_with_Masked_Transformer"><div class="paper-abstract"><div class="title">End-to-End Dense Video Captioning with Masked Transformer</div><div class="info"><div class="authors">Luowei Zhou, Yingbo Zhou, Jason J. Corso, Richard Socher, Caiming Xiong</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><Kazushige>Okayasu</Kazushige></div><div class="item1"><div class="text"><h1>概要</h1><p>動画内のいつ行動が行われたかのTemporal Action Proposals(TAP)とどのような行動が行われたかのキャプションを行うタスクにおいて，self-attentionを用いて既存手法を改善する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/End-to-End_Dense_Video_Captioning_with_Masked_Transformer_1.png" alt="End-to-End_Dense_Video_Captioning_with_Masked_Transformer_1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ActivityNet CaptionsとYouCookIIでキャプションの評価を行い，METEORスコアが10.12と6.58であった．</p><p>SoTAではないが，時間的なイベントの検出とイベントのキャプショニングをEnd-to-Endに行う手法であること．また，このようなタスクで初めてのRNN-basedでは無い手法を提案したこというところが新規性．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0037.pdf">論文</a></li><li><a href="https://arxiv.org/abs/1804.00819">arxiv</a></li></ul></div></div><div class="slide_index">[#134]</div></div></section><section id="ID_Modulated_Convolutional_Networks"><div class="paper-abstract"><div class="title">Modulated Convolutional Networks</div><div class="info"><div class="authors">Xiaodi wang , Baochang Zhang</div></div><div class="slide_editor">Kazuki Tsubura</div><div class="item1"><div class="text"><h1>概要</h1><p>・CNNは画像処理の様々なタスクをこなすうえでとても有効だが，ネットワークのストレージにかなりのコストを要求するため，展開が制限される．2値化フィルタを用いたCNNの移植性向上のための新しい変調畳み込みネットワーク(MCNs)を提案する．MCNでは，end-to-endフレームワークにおけるフィルタ損失，中心損失，ソフトマックス損失を考慮した新しい損失関数であるM-フィルタを提案する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Modulated_Convolutional_Networks.png" alt="Modulated_Convolutional_Networks.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・差分</h1><p>・非二項フィルタを復元するために，M-フィルタを導入しネットワークモデルを計算するための新しいアーキテクチャを導出する．MCNは完全精度モデルとは対照的に，畳み込みフィルタの必要な記憶スペースのサイズを32倍に縮小することができ，最先端の2値化モデルよりもはるかに優れた性能を達成した．また，MCNは完全精度のResentsおよびWideResentsと同等のパフォーマンスを達成した．</p></div></div><div class="item4"><div class="text"><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Modulated_Convolutional_Networks_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#135]</div><div class="timestamp">2018.6.20 19:49:11</div></div></section><section id="ID_Ordinal_Depth_Supervision_for_3D_Human_Pose_Estimation"><div class="paper-abstract"><div class="title">Ordinal Depth Supervision for 3D Human Pose Estimation</div><div class="info"><div class="authors">Georgios Pavlakos, Xiaowei Zhou, Kostas Daniilidis</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Shusuke Shigenaka</div><div class="item1"><div class="text"><h1>概要</h1><p>3D ground truthの存在しないデータに対し人間の関節の奥行きデータの監視信号を使用することを提案。人体関節の奥行きを用いて3Dの姿勢推定をConvNetsで学習すると正確な関節座標で学習結果を得ることができる。
通常の深さ注釈をもつ2Dポーズデータセット(LSPとMPII)はConvNetsの学習に容易に組み込むことができるため、
ポーズデータセットを拡張させることにより3Dの姿勢に対する序数の深さ正確なものにし、
標準のベンチマークでstate-of-the-artを達成した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Ordinal_Depth_Supervision_for_3D_Human_Pose_Estimation.png" alt="Ordinal_Depth_Supervision_for_3D_Human_Pose_Estimation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>3D ground truthを必要としない</li><li>2Dポーズデータセットを使うことで、スタジオ以外の条件での3Dポーズ推定でも高い精度を得ることができる</li><li>Human3.6Mのデータセットではこれまで誤差が47.7だったのに対し41.8を達成しており、HumanEva-Iデータセットにおいてはこれまで誤差が24.6だったのに対し18.3と大幅に更新をしている</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://www.seas.upenn.edu/~pavlakos/projects/ordinal/">論文</a></li></ul></div></div><div class="slide_index">[#136]</div><div class="timestamp">2018.6.21 18:00:49</div></div></section><section id="ID_A_Weighted_Sparse_Sampling_and_Smoothing_Frame_Transition_Approach_for_Semantic_Fast-Forward_First-Person_Videos"><div class="paper-abstract"><div class="title">A Weighted Sparse Sampling and Smoothing Frame Transition Approach for Semantic Fast-Forward First-Person Videos</div><div class="info"><div class="authors">M. Silva, W. Ramos, J. Ferreira, F. Chamone, M. Campos</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><p>なめらかに<strong>早送り</strong>するという，ビデオ要約の新たな形を提案．</p><p>新しい適応的なフレーム選択手法を提案．重み付き最小値再構築問題として定式化．
そこに，スムーズなフレーム遷移の手法を組み合わせる．
通しで見るとなめらかに見えるようにフレームを落とす．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/A_Weighted_Sparse_Sampling_and_Smoothing_Frame_Transition_Approach_for_Semantic_Fast-Forward_First-Person_Videos.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>問題設定が面白い．流行りのビデオ要約の流れを汲みつつ，意識的に新しい枠組みを提案している．
しかも十分実行可能と思われる問題である．想定される成果の見栄えもよい．
解き方もちゃんとしている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3780.pdf">論文</a></li><li><a href="https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/cvpr2018/">プロジェクトページ</a></li><li><a href="https://github.com/verlab/SemanticFastForward_CVPR_2018">ソースコード</a></li></ul></div></div><div class="slide_index">[#137]</div><div class="timestamp">2018.6.21 17:42:48</div></div></section><section id="ID_Weakly_Supervised_Coupled_Networks_for_Visual_Sentiment_Analysis"><div class="paper-abstract"><div class="title">Weakly Supervised Coupled Networks for Visual Sentiment Analysis</div><div class="info"><div class="authors">J. Yang, D. She, Y. Lai, P.L. Rosin, M. Yang</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><p>画像で感情分析を行う研究．従来法は全体的な画像特徴からセンチメント表現を学習していたが，
本研究では局所特徴もとらえるようにした．</p><p>弱教師付き二つ組CNNによる．(1)感情に特定的にソフトマップを検出するFCNN．
画像レベルのラベルだけ必要にしたので，画素レベルアノテーションのようなアノテーション負荷が低くて済む．
(2)ロバストなクラス分類のために，深層特徴を使い，感情マップを2つ組することによって，全体・局所情報の両方を活用．
そして，これら2つを統合してEnd-to-Endで最適化できるようにする．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Weakly_Supervised_Coupled_Networks_for_Visual_Sentiment_Analysis.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>より詳細に画像を見るように設計した．その結果，6つのベンチマークで評価を行い，SOTA性能を達成．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2108.pdf">論文</a></li></ul></div></div><div class="slide_index">[#138]</div><div class="timestamp">2018.6.21 16:46:09</div></div></section><section id="ID_A_Low_Power_High_Throughput_Fully_Event-Based_Stereo_System"><div class="paper-abstract"><div class="title">A Low Power, High Throughput, Fully Event-Based Stereo System</div><div class="info"><div class="authors">A. Andreopoulos, H.J. Kashyap, T.K. Nayak, A. Amir and M.D. Flickner</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><p>著者らIBMが開発した100万個のノードが伝達しあうニューラルネットワークを模倣したプロセッサ「<a href="https://en.wikipedia.org/wiki/TrueNorth">TrueNorth</a>」を使った，
新しいカメラ「Dynamic Vision Sensor」を使ってステレオしてみた論文．</p><p>Dynamic Vision Sensorは，通常カメラのフレーム撮影方式ではなく，イベントベースに，各画素が非同期で撮影するという新たな撮影方式のセンサである．
これにTrueNorthを組み合わせれば，完全にグラフベースで，配列などのあらゆるデータ構造無しに
フォン・ノイマン型計算モデルの計算が可能である．</p><p>これにより，2000fpsの視差マップ生成を達成．通常のカメラではとらえられない急激な変化をとらえることが可能．
しかも200倍省エネ．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/A_Low_Power_High_Throughput_Fully_Event-Based_Stereo_System.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>上記参照．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>新製品の宣伝的論文っぽい．確かに面白いカメラシステムなので，今後これを軸に新たな枠組みが発生するかもしれない？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3791.pdf">論文</a></li></ul></div></div><div class="slide_index">[#139]</div><div class="timestamp">2018.6.21 16:19:51</div></div></section><section id="ID_M3_Multimodal_Memory_Modelling_for_Video_Captioning"><div class="paper-abstract"><div class="title">M3: Multimodal Memory Modelling for Video Captioning</div><div class="info"><div class="authors">J. Wang, W. Wang, Y. Huang, L. Wang, T. Tan</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><p>ビデオキャプショニングの話題．Long-Termのマルチモーダルな依存性のモデリングと
文脈的ミスアラインメントがあるのに対し，
(1)メモリモデリングするのは Long-Term系列的問題に対して 潜在的な利点がある （なにそれ），
(2)視覚的アテンションにおいてワーキングメモリは主要素，
という二点の事実を考慮した，
Multimodal Memory Modelling（M3）を提案．
LSTMの外部に視覚-テキスト間共有メモリを持ち，Long-Termな視覚-テキスト間依存性をモデル化する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/M3_Multimodal_Memory_Modelling_for_Video_Captioning.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>MSVD，MSR-VTTで評価し，BLEU，METEORにおいてSOTA性能．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>HMMのように見える．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0442.pdf">論文</a></li></ul></div></div><div class="slide_index">[#140]</div><div class="timestamp">2018.6.21 15:30:05</div></div></section><section id="ID_Going_from_Image_to_Video_Saliency_Augmenting_Image_Salience_with_Dynamic_Attentional_Push"><div class="paper-abstract"><div class="title">Going from Image to Video Saliency: Augmenting Image Salience with Dynamic Attentional Push</div><div class="info"><div class="authors">S. Gorji and J.J. Clark</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><p>画像における静的なSaliency Modelを，動的なビデオのSaliencyの予測に使う手法．この著者らは，前回に写真内に写っている人の注視（Attention）をCNNのAttentionと組み合わせるという<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Gorji_Attentional_Push_A_CVPR_2017_paper.pdf">Shared Attentionに関する論文</a>を出していたが，
今度は写真を撮る人・シーンに映っている人のShared Attentionについて取り組んだ．</p><p>マルチストリームCNN-LSTM構造を提案．これはSoTAなSaliencyをDynamic Attentional Pushに拡張する．</p><p>4つのステージからなる．Saliencyステージと，3つのAttentional Pushステージ．この複数ステージ構造は，Augmenting ConvNetに従っている．
ConvLSTMの補足（complementary）と時間変化出力組み合わせで学習．
拡張したSaliencyと，ビデオにおける「見ている人」修正パターンの間のRelative Entropyの最小化を行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Going_from_Image_to_Video_Saliency_Augmenting_Image_Salience_with_Dynamic_Attentional_Push.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>動画データセットHOLLYWOOD2，UCF-Sport，DIEMにおいて，SoTAな時空間Saliency推定性能を達成．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>発展ネタを自分で出して，しかもCVPR連続当選．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1928.pdf">論文</a></li></ul></div></div><div class="slide_index">[#141]</div><div class="timestamp">2018.6.21 12:24:09</div></div></section><section id="ID_Jointly_Localizing_and_Describing_Events_for_Dense_Video_Captioning"><div class="paper-abstract"><div class="title">Jointly Localizing and Describing Events for Dense Video Captioning</div><div class="info"><div class="authors">Y. Li, T. Yao, Y. Pan, H. Chao and T. Mei</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><p>Dense Video Captioningの話．イベントの発生時間のプロポーザルと，それぞれのイベントにおける文章生成の両者を結合的にEnd-to-Endで学習する，
Descriptiveness Regressionを提案．
シングルショット検出に組み込む．これは文章生成を経由したプロポーザル時間ごとの説明的複雑性を推論する．
これが時間定位の調節につながるらしい．
キャプショニングと検出の結合・汎用最適化をするところが他手法と異なるらしい．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Jointly_Localizing_and_Describing_Events_for_Dense_Video_Captioning.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>動画データセットActivityNetにおいてSoTAを達成．著者らはMETEORで12.96%出たのがすごいと言っている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Dense Video Captioning: イベントの時間的定位と説明文を付けるタスク．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3807.pdf">論文</a></li></ul></div></div><div class="slide_index">[#142]</div><div class="timestamp">2018.6.21 11:51:21</div></div></section><section id="ID_Audio_to_Body_Dynamics"><div class="paper-abstract"><div class="title">Audio to Body Dynamics</div><div class="info"><div class="authors">E. Shlizerman, L. Dery, H. Schoen and I. Kemelmacher-Shlizerman</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><p><strong>「音から手の動きは生成可能か？」</strong>バイオリンやピアノ演奏の音声を入力すると，アバターが演奏しているかのようにアニメーションするようなスケルトンの推定を行う手法を提案．
結論：できる．</p><p>実際ちゃんとやるにはいくつかアドホックな工夫が必要なようで，詳細はおのおの論文を確認してもらいたい．学習時に使うスケルトンデータはYouTubeのリサイタル動画からOpenPoseやMaskRCNNを駆使して生成する．
入力音声から<a href="http://grail.cs.washington.edu/projects/AudioToObama/siggraph17_obama.pdf">この手法</a>で
13次元ベクトルに変換し，さらにその時間差分や音量エネルギーを足した28次元ベクトルにする．
これから上半身のスケルトンの時系列を生成するLSTMを作り，
スケルトンにアバターを着せてアニメーションを作成する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Audio_to_Body_Dynamics.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>アプリケーション枠らしく，見た目の良さがあり，また実装上の困難と解決についてちゃんと書いているのが評価されたものと思われる．
アプリケーションとして利用するに当たって，どれだけうまくいけるのかが窺い知れる資料として
貴重に思われる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>1ページ目が既に他の論文と一線を画そうとしている．Fun to readという点で参考になるので，一度読んでみることを勧める．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0298.pdf">論文</a></li><li><a href="https://arviolin.github.io/AudioBodyDynamics/">動画など</a></li></ul></div></div><div class="slide_index">[#143]</div><div class="timestamp">2018.6.21 11:28:41</div></div></section><section id="ID_Separating_Self-Expression_and_Visual_Content_in_Hashtag_Supervision"><div class="paper-abstract"><div class="title">Separating Self-Expression and Visual Content in Hashtag Supervision</div><div class="info"><div class="authors">A. Veit, M. Nickel, S. Belongie, L. Maaten</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><p>Facebookでの研究．ユーザのこれまでのハッシュタグから，一意に同定できない意味の単語のハッシュタグでもユーザが意図した画像検索ができるようにした．
画像のDeCAFを取り，ユーザの履歴特徴，ハッシュタグ特徴を埋め込んだ３次テンソルを構成，多クラスロジスティック関数などで評価する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Separating_Self-Expression_and_Visual_Content_in_Hashtag_Supervision.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>MLPによる手法よりこちらの方が良い性能を示した．Top1で43.7%，Top10で72.12%のAccuracy．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0252.pdf">論文</a></li></ul></div></div><div class="slide_index">[#144]</div><div class="timestamp">2018.6.21 10:37:18</div></div></section><section id="ID_Human-centric_Indoor_Scene_Synthesis_Using_Stochastic_Grammar"><div class="paper-abstract"><div class="title">Human-centric Indoor Scene Synthesis Using Stochastic Grammar</div><div class="info"><div class="authors">S. Qi, Y. Zhu, S. Huang, C. Jiang, S. Zhu</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><p>3D部屋レイアウトとその2D画像との合成の話題．</p><p>Spatial And-Or Graph (S-AOG) ※ で屋内シーンを表現する．終端ノードは物体エンティティ（部屋とか家具とかその他）．</p><p>終端ノードに対し，マルコフランダム場（MRF）を用い，
人間の文脈で関係性をエンコードする．
屋内シーンデータセットから分布を学習し，
モンテカルロマルコフ連鎖（MCMC）を使って新しいレイアウトをサンプルする．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Human-centric_Indoor_Scene_Synthesis_Using_Stochastic_Grammar.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>3つの視点で有効性を確認．</p><ul><li>SOTAな部屋アレンジ手法と比較しての，視覚的リアルさ</li><li>GTに対する，アフォーダンスマップの精度</li><li>合成部屋の機能性，自然っぽさを人間の被験者で評価</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>※S-AOGは確率的文法モデルの一つ．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0116.pdf">論文</a></li></ul></div></div><div class="slide_index">[#145]</div><div class="timestamp">2018.6.20 11:41:06</div></div></section><section id="ID_Fast_Monte-Carlo_Localization_on_Aerial_Vehicles_using_Approximate_Continuous_Belief_Representations"><div class="paper-abstract"><div class="title">Fast Monte-Carlo Localization on Aerial Vehicles using Approximate Continuous Belief Representations</div><div class="info"><div class="authors">A. Dhawale, K.S, Shankar, N. Michael</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><p>ドローンのようなサイズ，重さ，力が制約されたプラットフォームでも，3D自己位置同定を高速に行えるフレームワークを提案．
点群データの混合ガウス分布（GMM）表現による圧縮をキーアイデアとしている．</p><p>デプスセンサのデータと，オンボード姿勢参照システムからピッチとロールを得る．データをGMMで表現した尤度を使って，複数仮説パーティクルフィルタにより定位．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Fast_Monte-Carlo_Localization_on_Aerial_Vehicles_using_Approximate_Continuous_Belief_Representations.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>CVPRでは，高速性・省メモリに関するトピックに興味があるかもしれない．SLAM系はICRAでは大変多く議論されている話題だが，逆にCVPRだとアプリケーション枠で
通る可能性があるかもしれない．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/4001.pdf">論文</a></li></ul></div></div><div class="slide_index">[#146]</div><div class="timestamp">2018.6.20 11:13:48</div></div></section><section id="ID_Variational_Autoencoders_for_Deforming_3D_Mesh_Models"><div class="paper-abstract"><div class="title">Variational Autoencoders for Deforming 3D Mesh Models</div><div class="info"><div class="authors">Q. Tan, L. Gao, Y. Lai and S. Xia</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><p>3Dメッシュの変形に関して，Variational AutoeEcoder(VAE)を使ってみたという研究．可能な変形の確率的潜在空間の探索を行う．
学習は簡単で，学習データも少なくて済む（どれくらい？）
事前分布を代替することで，異なる潜在変数の顕著性（Significance）を柔軟に調節可能な拡張モデルも提案．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Variational_Autoencoders_for_Deforming_3D_Mesh_Models.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>形状生成，形状補完，形状空間埋め込み，形状探索においてSoTA越え．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2701.pdf">論文</a></li></ul></div></div><div class="slide_index">[#147]</div><div class="timestamp">2018.6.20 10:42:57</div></div></section><section id="ID_Density-aware_Single_Image_De-raining_using_a_Multi-stream_Dense_Network"><div class="paper-abstract"><div class="title">Density-aware Single Image De-raining using a Multi-stream Dense Network</div><div class="info"><div class="authors">He Zhang and Vishal M. Patel</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kodai Nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>DID-MDN (density-aware multi-stream densely connected convolutional neural network-based algorithm) と呼ばれる、画像内の雨量密度推定と雨除去を行うアルゴリズムを提案。雨のストロークをより良く特徴づけるため、multi-stream densely connected de-raining networkでは異なるスケールの特徴量を効率的に活用する。また、雨密度ラベル付き画像を含むデータセットを新たに作成した。このデータセットを学習に使うことにより、state-of-the-artな手法を超えることができた。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1802.07412_fig1.png" alt="1802.07412_fig1.png"><img src="slides/figs/1802.07412_fig2.png" alt="1802.07412_fig2.png"><img src="slides/figs/1802.07412_fig3.png" alt="1802.07412_fig3.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>PSNRとSSIMにより雨除去の性能を評価した。比較に使用した手法、および、結果は右図の通り。
右図におけるTest1とTest2は、使用したテストセットが異なることを表している。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1802.07412.pdf">論文URL</a></li><li><a href="https://github.com/hezhangsprinter">github</a></li></ul></div></div><div class="slide_index">[#148]</div><div class="timestamp">2018.6.21 17:32:13</div></div></section><section id="ID_SeGAN_Segmenting_and_Generating_the_Invisible"><div class="paper-abstract"><div class="title">SeGAN: Segmenting and Generating the Invisible</div><div class="info"><div class="authors">Kiana Ehsani, Roozbeh Mottaghi and Ali Farhadi</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kodai Nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>オクルードされている物体の全体像を推定するため、SeGANを提案。SeGANは物体の見えていない領域のセグメントを生成することができる。また、occluderとoccludeeの関係も推定することができる。さらにSeNetはcategory-agnosticでありカテゴリー情報を必要としない。データセットにはDYCEを使用。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1703.10239_img2.png" alt="1703.10239_img2.png"><img src="slides/figs/1703.10239_img3.png" alt="1703.10239_img3.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>右図に示すように、他のセグメントベースラインと比べ、SeGANが見える領域、見えない領域、それらの組み合わせの全てにおいて最も良い結果を出した。ここで、SUは見える領域のセグメント、SIは見えない領域のセグメント、SFは全体像のセグメントを表している。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1703.10239.pdf">論文URL</a></li><li><a href="https://github.com/ehsanik/SeGAN">github</a></li></ul></div></div><div class="slide_index">[#149]</div><div class="timestamp">2018.6.21 15:42:10</div></div></section><section id="ID_Leveraging_Unlabeled_Data_for_Crowd_Counting_by_Learning_to_Rank"><div class="paper-abstract"><div class="title">Leveraging Unlabeled Data for Crowd Counting by Learning to Rank</div><div class="info"><div class="authors">Xialei Liu, Joost van de Weijer, Andrew D. Bagdanov</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Shusuke Shigenaka</div><div class="item1"><div class="text"><h1>概要</h1><p>群衆の画像データにおいて、ネットワークの訓練を改善するためのself-supervisedタスクを提案。タスクは集計情報とランキング情報の両方を組み合わせたマルチタスクフレームワークであり、群衆カウントのためにend-to-endで訓練できる。
群衆画像をだんだん小さくするように切り取って人数をランク付けおり、提案されたself-supervisedタスクはラベル付けのされていない群衆画像のCNNに大きく貢献した。
提案手法は群衆計測の困難なデータセットShanghaiTechとUCF CC 50においてstate-of-the-artを得ている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Leveraging_Unlabeled_Data_for_Crowd_Counting_by_Learning_to_Rank.png" alt="Leveraging_Unlabeled_Data_for_Crowd_Counting_by_Learning_to_Rank"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>困難とされている2つのデータセットでstate-of-the-artを得たこと</li><li>大人数のデータはその人数のデータより少ない数で観察というルールに基づいて計測を行っているため、大規模なトレーニングデータセットの欠如に対処することができている</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.03095">論文</a></li><li><a href="https://github.com/xialeiliu/CrowdCountingCVPR18">github</a></li></ul></div></div><div class="slide_index">[#150]</div><div class="timestamp">2018.6.21 15:36:28</div></div></section><section id="ID_Conditional_Image-to-Image_Translation"><div class="paper-abstract"><div class="title">Conditional Image-to-Image Translation</div><div class="info"><div class="authors">Jianxin Lin, Yingce Xia, Tao Qin, Zhibo Chen and Tie-Yan Liu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kodai Nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>image-to-image translationタスクで用いられるモデルは、ターゲットドメインの翻訳結果をコントールする機構がなく、出力結果が多様性に乏しい。この研究では、1. conditional image-to-image translationをいう問題を新たに設定し、2. この問題を解くためにconditional dual-GAN (cd-GAN) を提案する。
1では、複数の画像を組み合わせたtarget domainが入力されたsorce domainを変換する問題を扱う。複数の画像をどのようにして組み合わせるかで多様性に富んだ変換結果が得られる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1805.00251_img1.png" alt="1805.00251_img1.png"><img src="slides/figs/1805.00251_img2.png" alt="1805.00251_img2.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>入力は64x64とする。eA, eBは3つの畳み込み層で構成されており、各畳込み層の活性化関数にLReLUを用いる。デコレーターネットワークであるgAとgBは4つのデコンボリューション層から構成されており、はじめの3層はReLUで活性化し、4層目にはtanhで活性化する。ディスクリミネーターであるdAとdBは4つの畳み込み層と2層の全結合層から構成されており各層の活性化関数にLReLUを用いる、最後の層（2つ目の全結合層）のみsigmoidで活性化する。オプティマイザーはAdamを用い、学習率は0.0002とする。以上の設定で実験した結果を右図に示す。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://link.com/link1/">link1</a></li><li><a href="http://link.com/link2/">link2</a></li><li><a href="http://link.com/link3/">link3</a></li></ul><ol><li><a href="http://link.com/link3/">link3</a></li><li><a href="http://link.com/link3/">link3</a></li><li><a href="http://link.com/link3/">link3</a></li></ol></div></div><div class="slide_index">[#151]</div><div class="timestamp">2018.6.21 14:34:27</div></div></section><section id="ID_Empirical_study_of_the_topology_and_geometry_of_deep_networks"><div class="paper-abstract"><div class="title">Empirical study of the topology and geometry of deep networks</div><div class="info"><div class="authors">Alhussein Fawzi et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>DNN 画像クラス分類器の入力空間における位相的・幾何学的性質を実験的に分析した研究. DNN が学習している各クラスの領域は接続されたものであり, その境界は少数の大きな曲率をもつ方向と, 平坦な大多数の方向があることが確認された. また, 大きな曲率をもつ方向はデータ間で共有されており, これらの方向とネットワークの摂動に対する感度に関係性があることを確認した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Empirical-study-of-the-topology-and-geometry-of-deep-networks.png" alt="fukuhara-Empirical-study-of-the-topology-and-geometry-of-deep-networks.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>理論のみを用いた解析は困難なため, 実験を行って性質の分析を行った</li><li>DNN が学習している同じクラスの領域は接続されたものであり, その領域はほぼ凸集合になっている（凸集合に近いが実際には違う）</li><li>クラスの境界の主曲率は多数の方向で０であったが, 大きな値をもつ方向が少数存在</li><li>主曲率の値は非対称で大きな負の値を持つ方向が多い (この結果はネットワークの構造やデータセットなどを変えても共通して確認された)</li><li>主曲率の大きな値をもつ方向はデータ間で共有されていることを確認</li><li>主曲率の大きな値をもつ方向は, ネットワークが誤認識をしやすい摂動の方向となっていることを確認（adversarial perturbation との関連が確認された）</li><li>クラスの境界の主曲率の値の非対称を用いて, 元画像と adversarial perturbation を加えられた画像を識別する方法を提案 (GoogLeNet や CaffeNet を用いて行った実験では90％以上の精度を達成)</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Fawzi_Empirical_Study_of_CVPR_2018_paper.pdf" target="blank">[論文] Empirical study of the topology and geometry of deep networks</a></li><li>本研究で確認された入力空間における位相的性質と同様の性質が, weight の空間でも報告 [Freeman+ 16] されており, ２つの空間の関連性を調べることは今後の課題とされている.</li></ul></div></div><div class="slide_index">[#152]</div><div class="timestamp">2018.6.21 6:30:55</div></div></section><section id="ID_Learning_to_Find_Good_Correspondences"><div class="paper-abstract"><div class="title">Learning to Find Good Correspondences</div><div class="info"><div class="authors">Kwang Moo Yi, Eduard Trulls, Yuki Ono, Vincent Lepetit, Mathieu Salzmann and Pascal Fua</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>2枚の画像間の対応点探索を学習ベースで行う方法を提案。従来のhandcrafted特徴(SIFTなど)による手法は、特徴量により候補を決めた上でRANSACなどのアルゴリズムで対応点かそうでないかを決定する。
本研究では同様に、候補となる対応点の中から実際に対応しているペアをMulti Layer Perceptrons(MLPs)により決定する。
対応点の数は画像によって異なるので、ネットワークには対応点のペア(4変数)毎に実際に対応しているかの判定を行う。
一方で、中間層出力を全ペアの平均と分散により正規化することでglobal contextを考慮する。(Context Normalization)
学習は、ペアの判定が正しいか、判定結果を用いてessential matrixが正しく求められるかによって行う。
その際、学習データに対して対応点のアノテーションを手動で与えるのは非常に時間がかかってしまう。
そこでepipolar distanceを用いた閾値処理により対応点を取得する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Find_Good_Correspondences.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ベースラインと比較して、学習したシーン、学習していないシーンどちらにおいても高い精度ないし同等の精度を出すことに成功。59枚の学習データのみで学習した場合であっても、ベースラインと比べ高い精度を出すことに成功。
RANSACのみで対応点を決定する場合より、提案手法により候補を絞った上でRANSACにより更に候補を削るほうが17倍計算時間が早い。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.05971">論文</a></li></ul></div></div><div class="slide_index">[#153]</div><div class="timestamp">2018.6.21 01:21:08</div></div></section><section id="ID_Facelet-Bank_for_Fast_Portrait_Manipulation"><div class="paper-abstract"><div class="title">Facelet-Bank for Fast Portrait Manipulation</div><div class="info"><div class="authors">Ying-Cong Chen, Huaijia Lin, Michelle Shu, Ruiyu Li, Xin Tao, Xiaoyong Shen, Yangang Ye and Jiaya Jia</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>顔のattributeを編集するEnd-to-Endのネットワークを提案した。ドメイン間の変換を考えるのではなく、Encoderにより得られた特徴のドメイン間の差分を考えることにより特徴の付与を実現する。
ドメイン毎の特徴は、全ての学習データの平均ではなく入力画像の最近傍K枚の平均を考える。
Encoderにより入力画像から得られた特徴から、Facelet Bankというネットワークによりドメイン間の差分を求める。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Facelet-Bank_for_Fast_Portrait_Manipulation.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来手法と比較して、artifactが少なく高解像度の画像を出力することが可能になった。女性に髭を付与するなど学習データには存在しないようなものの場合、従来法では男女の違いが付与されて髭以外の変化が加わってしまう。
しかし、編集に重要な領域(髭→口周り)のみに変化を施すため従来手法よりも自然な変化が実現可能である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>比較的関連研究が多そうな研究だったが比較対象が2つと少なめ</p><ul><li><a href="https://github.com/yingcong/Facelet_Bank/">コード</a></li><li><a href="https://arxiv.org/abs/1803.05576">論文</a></li></ul></div></div><div class="slide_index">[#154]</div><div class="timestamp">2018.6.20 22:09:57</div></div></section><section id="ID_Every_Smile_is_Unique_Landmark-Guided_Diverse_Smile_Generation"><div class="paper-abstract"><div class="title">Every Smile is Unique: Landmark-Guided Diverse Smile Generation</div><div class="info"><div class="authors">Wei Wang,Xavier Alameda-Pineda, Dan Xu, Pasal Fua, Elisa Riccia and Nicu Sebe</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>1枚の顔画像から、指定した表情に変化する動画を生成する手法を提案。たとえ同じ笑顔であっても、作り笑いとそうでない場合など目の動きなど顔の変化は異なる。
そこで、指定された表情に対して複数の動画を生成する手法を提案した。
入力画像とラベルから、指定されたラベルに対して適した顔特徴点の変化を複数のネットワークによって予測する。
その際、各ネットワークの予測がお互いに類似しないように最適化することで動画を複数用意することなく予測することを可能とする。
予測した顔特徴点から各フレームの顔画像を復元することにより、動画の生成を実現する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Every_Smile_is_Unique_Landmark-Guided_Diverse_Smile_Generation.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来の動画生成に関する研究と比べ、artifactが少なく与えられた画像の人物の個人性を保った合成を実現した。ユーザースタディの結果、比較対象とした研究よりも提案手法により生成された動画のほうが圧倒的に好まれるということが分かった。
Action Unit(AU)の変化を調べたところ、提案手法により生成された動画は実際の動画に近い変化をすることが分かった。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.01873">論文</a></li></ul></div></div><div class="slide_index">[#155]</div><div class="timestamp">2018.6.19 19:37:44</div></div></section><section id="ID_Creating_Capsule_Wardrobes_from_Fashion_Images"><div class="paper-abstract"><div class="title">Creating Capsule Wardrobes from Fashion Images</div><div class="info"><div class="authors">Wei-Lin Hsiao and Kristen Grauman</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Capsule Wardrobesという、良い組み合わせが多数存在するファッションアイテムのセットを自動で作る手法を提案。ファッションアイテムのセットに対して、それで実現可能なファッションの親和性と多様性を最大化することによりセットを決定する。
注目レイヤー以外を固定して最適化することを繰り返すことでファッションアイテムの選択を行う。
ファッションの親和性を決定するために、トピックモデルをベースとした教師なし学習による全身画像からのファッションの評価方法を構築した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Creating_Capsule_Wardrobes_from_Fashion_Images.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ファッションサイトに掲載されているCapsule Wardobesと作成したものに含まれるファッションアイテムの類似度を測った結果、ベースラインと比べ提案手法により選ばれたものの方が類似度が高いという結果が得られた。提案手法である繰り返しの最適化と貪欲法による最適化結果をユーザースタディで比べたところ、提案手法のほうが好ましいと答えた人が59％いた。
また、個人の好みに応じたCapsule Wardrobesの作成が可能である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.01873">論文</a></li></ul></div></div><div class="slide_index">[#156]</div><div class="timestamp">2018.6.19 21:07:56</div></div></section><section id="ID_Anticipating_Traffic_Accidents_with_Adaptive_Loss_and_Large-scale_Incident_DB"><div class="paper-abstract"><div class="title">Anticipating Traffic Accidents with Adaptive Loss and Large-scale Incident DB</div><div class="info"><div class="authors">Hirokatsu Kataoka, Tomoyuki Suzuki, Yoshimitsu Aoki and Yutaka Satoh</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kodai Nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>交通事故予測のため, 1. loss関数としてAdaptive Loss for Earlay Anticipation (AdaLEA)と2. 予測のためのNear-miss Incident DataBase (NIDB) の提案を行った. AdaLEAにより, モデルが学習過程において, 徐々に早く危険を予測できるように学習される. モデルが交通事故を予測する速さでペナルティを与えることにより, これを実現する. NIDBは, 多くの交通ニアミス動画を含んでおり, 危険と危険要素予測の評価用アノテーションが付けられている.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1804.02675_img1.png" alt="1804.02675_img1.png"><img src="slides/figs/1804.02675_img2.png" alt="1804.02675_img2.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ベールモデルとしてDSA, LSTM, QRNN, loss関数としてEL, LEA, AdaLEAを用いて実験した.その結果, 危険予測では, mAPが6.6%上昇, ATTCが2.36sec速くなった.
また, 危険要素予測では, mAPが4.3%上昇, ATTCが0.70sec速くなった.</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1804.02675.pdf">論文URL</a></li></ul></div></div><div class="slide_index">[#157]</div><div class="timestamp">2018.6.20 20:03:48</div></div></section><section id="ID_Zero-Shot_Super-Resolution_using_Deep_Internal_Learning"><div class="paper-abstract"><div class="title">“Zero-Shot” Super-Resolution using Deep Internal Learning</div><div class="info"><div class="authors">Assaf Shocher, Nadav Cohen, Michal Irani</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Shusuke Shigenaka</div><div class="item1"><div class="text"><h1>概要</h1><p>実際の古い写真,ノイズの多い画像,生物学的データ,取得プロセスが不明または非理想的な画像のSuper-Resolution(SR)を実行を行うことができるZero-Shot SR(ZSSR)を提案．過去の画像例や事前訓練に依存することなく,Low-Resolution(LR)とその縮小版から複雑な画像特有のHR-LR関係を推論するCNNを訓練を行うことにより,
実際のLRの画像において,State-of-the-artなCNNベースのSRおよび教師なしSRよりも優れている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Zero-Shot171206087.png" alt="Zero-Shot171206087"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>SR-CNNは大規模な外部データベースの画像を事前に訓練しているのに対し,ZSSRは小さな画像から粗い解像度のテストデータを訓練．</p><p>ZSSRは同じ教師なしのSelfExSRにと比べ全てのDataSetにおいて優れている．教師あり学習でも通常のLRはあまり変わらない精度を出しており,未知LR画像で確認をするとかなり優れた精度を出している．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.06087">論文</a></li><li><a href="http://www.wisdom.weizmann.ac.il/~vision/zssr/">Zero-Shot Super-Resolution</a></li></ul></div></div><div class="slide_index">[#158]</div><div class="timestamp">2018.6.20 11:43:55</div></div></section><section id="ID_Crafting_a_Toolchain_for_Image_Restoration_by_Deep_Reinforcement_Learning"><div class="paper-abstract"><div class="title">Crafting a Toolchain for Image Restoration by Deep Reinforcement Learning</div><div class="info"><div class="authors">Ke Yu et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>強化学習（Deep Q-learning）を用いた画像復元の研究. 単一の大きなネットワークを用いる手法とは対照的に, 特定の distortion に対する復元に特化した小さなネットワークを複数集めて toolbox とし, agent が各ステップにおいて最適な tool を選択することで段階的な復元を行う. 評価実験では従来の大きな単一のCNNを用いた手法と同程度の精度を20%程度の計算量で実現した. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Crafting-a-Toolchain-for-Image-Restoration-by-Deep-Reinforcement-Learning.png" alt="fukuhara-Crafting-a-Toolchain-for-Image-Restoration-by-Deep-Reinforcement-Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>強化学習を用いて段階的に画像復元を行うフレームワークを提案</li><li>agent は action として, 各ステップにおいて特定の distortion に対する復元に特化した小さなネットワークを複数集めた toolbox の中から最適なものを選択</li><li>段階的な復元を行うと中間のステップにおいて生じる複雑な atifact を扱うため agent と tool の joint training アルゴリズムを提案 </li><li>DIV2K dataset を用いて行った評価実験では, PSNR 尺度において単一の大きなCNNを用いた場合と同程度の精度を約20％計算量で実現</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.03312" target="blank">[論文] Crafting a Toolchain for Image Restoration by Deep Reinforcement Learning</a></li><li><a href="https://github.com/yuke93/RL-Restore" target="blank">[Code] GitHub</a></li><li>どのネットワークを使うべきかという高次の意思決定を強化学習で学習するという方針が面白い. (Hierarchical Reinforcement Learning と類似の考え方)</li></ul></div></div><div class="slide_index">[#159]</div><div class="timestamp">2018.6.20 8:58:55</div></div></section><section id="ID_Reward_Learning_from_Narrated_Demonstrations"><div class="paper-abstract"><div class="title">Reward Learning from Narrated Demonstrations</div><div class="info"><div class="authors">Hsiao-Yu Tung et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>動画による教示と言語による説明を組み合わせて Reward の学習を行う研究. 言語情報によって与えられた目標の達成の可否を, 画像情報から判断する Instractable Perceptual Rewards を提案し, 学習用のデータセットを作成した. また, 評価実験では教師ありで静止画像のみから学習した場合と比較して, 優位な結果を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Reward-Learning-from-Narrated-Demonstrations.png" alt="fukuhara-Reward-Learning-from-Narrated-Demonstrations.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>言語情報によって与えられた目標の達成の可否を, 画像情報から判断する Instractable Perceptual Rewards を提案</li><li>上記の教師データとして, 動画による教示に言語による説明を付随した, Narrated Visual Demonstration (NVD) のデータセットを作成した</li><li>提案手法は hard negative mining によって少ない教師データからの効率的な学習が可能</li><li>評価実験では Visual Genome のみを用いて学習した手法 [Hu+16] と比較して優位な結果を達成</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Tung_Reward_Learning_From_CVPR_2018_paper.pdf" target="blank">[論文] Reward Learning from Narrated Demonstrations</a></li></ul></div></div><div class="slide_index">[#160]</div><div class="timestamp">2018.6.18 23:54:55</div></div></section><section id="ID_Trust_Your_Model_Light_Field_Depth_Estimation_With_Inline_Occlusion_Handling"><div class="paper-abstract"><div class="title">Trust Your Model: Light Field Depth Estimation With Inline Occlusion Handling</div><div class="info"><div class="authors">Hendrik Schilling, Maximilian Diebold, Carsten Rother, Bernd Jähne</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>LightFieldカメラからの距離画像推定の問題を提案。オクルージョンに伴う物体境界の精度や質向上に対して操作を行なったことが貢献である。従来法とは異なり、PatchMatchをベースラインとして距離画像とオクルージョン領域を同時推定を直接的に行う。同時推定を行うことで、データを全て同時に学習に用いることができ、さらに前処理のステップが不要になる。結果的には、オクルージョン領域の推定を行い物体境界をケアしただけでなく滑らかな表面再構成に成功した。公開されているLightFieldデータセットにて評価した結果、12のうち9の指標においてState-of-the-artな数値を出した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180618TrustYourModel.png" alt="180618TrustYourModel"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ライトフィールドカメラを用いた距離画像推定においてオクルージョン対策を講じた。距離画像とオクルージョン領域を同時推定する手法では既存のライトフィールドカメラにおける評価指標においてState-of-the-art。さらに、平面推定においても高度な推定を実現した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>同時推定のうまい手法を考案、副次的に平面が滑らかになるというのも面白い！</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Schilling_Trust_Your_Model_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#161]</div><div class="timestamp">2018.6.18 23:54:56</div></div></section><section id="ID_MobileNetV2_Inverted_Residuals_and_Linear_Bottlenecks"><div class="paper-abstract"><div class="title">MobileNetV2: Inverted Residuals and Linear Bottlenecks</div><div class="info"><div class="authors">Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>モバイルで動作する新規アーキテクチャMobileNetV2の提案論文、データセットを用いた複数タスクにてState-of-the-artな精度を達成した。物体検出のモデルであるSSDLiteやセマンティックセグメンテーションのモデルであるMobile DeepLabv3を考案した。これらはInverted Residual Structureと呼ばれる、ショートカットコネクションが小さなボトルネックレイヤに挟まれた構造を最小ユニットとして構成される。中間の拡張レイヤは非線形関数として軽量化されたdepthwiseの畳み込みとして実装される。右図に本論文の重要技術であるInverted Residual Blockについて示す。従来のResidual Block（左）は前後のdepthが広いが、提案のInverted Residual Blockは中ふたつがdepthが広く、前後は狭い。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180618MobileNetV2.png" alt="180618MobileNetV2"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Inverted Residual Blockの提案等によりモバイルサイズのモデルにおいても良好な認識精度のモデルを提案することに成功。認識精度とパラメータ数のトレードオフについても良好で、さらにはCPUにおいても高速に動作することを示しCVPRに採択された。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>モバイルネットv2、応用範囲が広そう。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html">Google AI Blog</a></li><li><a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet">GitHub</a></li><li><a href="https://github.com/xiaochus/MobileNetV2">GitHub2</a></li></ul></div></div><div class="slide_index">[#162]</div><div class="timestamp">2018.6.18 23:07:41</div></div></section><section id="ID_PoseFlow_A_Deep_Motion_Representation_for_Understanding_Human_Behaviors_in_Videos"><div class="paper-abstract"><div class="title">PoseFlow: A Deep Motion Representation for Understanding Human Behaviors in Videos</div><div class="info"><div class="authors">Dingwen Zhang, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>動画から人間の行動を理解するためのPoseFlowの提案。PoseFlowはオプティカルフローに代わる新しい動き表現であり、背景の動きによるノイズやオクルージョンに頑健。人間の骨格位置とマッチングの2つの問題を同時に解決するようなネットワークであるPoseFlow Net(PFN)を提案し、学習する。これにより、人体の部分のみに動きベクトルが付与された出力を得ることができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180619PoseFlow.jpg" alt="20180619PoseFlow.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>従来手法では、オプティカルフローを使ってモーションキューを探索している場合が多いが、背景の動きなども取ってしまうので“ノイズが多い動きの表現”であり、姿勢推定や行動認識のタスクにおいて支障をきたす。実験では、従来手法と比較して、姿勢推定や行動認識タスクにおいて高精度となっている。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>図のように、オプティカルフローでは背景の動きも取ってしまい、ぼんやりとした出力になっているが、PoseFlowでは人間の骨格の動きのような情報を取得することができる。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_PoseFlow_A_Deep_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#163]</div></div></section><section id="ID_Stereoscopic_Neural_Style_Transfer"><div class="paper-abstract"><div class="title">Stereoscopic Neural Style Transfer</div><div class="info"><div class="authors">Dongdong Chen, et al.</div><div class="conference">CVPR 2018</div><div class="paper_id">1802.10591</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>3D映画やAR / VRの需要に先駆けた、Stereoscopic Neural Style Transferの提案。スタイルトランスファーによって、左右視点での整合性を保持するために、style loss functionにdisparity lossを追加し、左右視点での視差制約を設けている。また、リアルタイム性を考慮したソリューションの開発に取り組み、stylization sub-networkとdisparity sub-networkの2つを共同してトレーニングできるモデルを提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180619SNST.jpg" alt="20180619SNST.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>ステレオカメラを使ったスタイルトランスファー手法。通常、図(a)のような左右視点の画像とスタイル画像を入力すると1行目のように，左視点(b)と右視点(c)のように左右の視点で差が生じる(d)。このような不一致性は、(e)のアナグリフ画像のようになり、視聴者へ左右視点での三次元的視覚疲労が生じさせる。提案手法ではこのような不一致性を抑制し、2行目のように整合性のとれたスタイルトランスファーを可能にする。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>提案手法によって、時間的および視差の整合性を考慮しており、3D映像を拡張できる。定量的および定性的評価によって、従来手法よりも高精度であることを示唆。</p><ul><li><a href="https://arxiv.org/pdf/1802.10591.pdf">論文</a></li></ul></div></div><div class="slide_index">[#164]</div></div></section><section id="ID_A_Common_Framework_for_Interactive_Texture_Transfer"><div class="paper-abstract"><div class="title">A Common Framework for Interactive Texture Transfer</div><div class="info"><div class="authors">Yifang Men, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>局所構造と視覚的豊かさの両方を保持できる、より汎用的なtexture transfer問題を解決するための提案。元画像と元画像のセマンティックマップ(aのようなセグメンテーション画像)と、変換後となるセマンティックマップの3つを入力とする。変換顔のセマンティックマップを元にスタイルトランスファーを実行する(ゴッホを痩せさせるなど)。contour key points match(CPD)やTPSアルゴリズムをベースとしたstructure propogation手法を提案している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180619ITT.jpg" alt="20180619ITT.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>タスクの多様性と、ユーザガイダンスの簡潔さをテーマに取り組んでいる。図のように、(a)簡単な絵をアートワークに変更、(b)装飾パターンの編集、(c)テキストに特殊効果を付与、(d)テキスト画像における効果を制御、(e)テクスチャの交換、などユーザのガイダンスによってさまざまなテクスチャの変換を実現できる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>他の手法と比較して、人間の視覚的にもより自然な変換ができている。</p><ul><li><a href="http://www.icst.pku.edu.cn/F/zLian/papers/CVPR18-Men.pdf">論文</a></li></ul></div></div><div class="slide_index">[#165]</div></div></section><section id="ID_Min-Entropy_Latent_Model_for_Weakly_Supervised_Object_Detection"><div class="paper-abstract"><div class="title">Min-Entropy Latent Model for Weakly Supervised Object Detection</div><div class="info"><div class="authors">Fang Wan, Pengxu Wei, Jianbin Jiao, Zhenjun Han and Qixiang Ye</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>弱教師付き学習で物体検出を行うmin-entropy latent model (MELM)の提案。MELMは、object discoveryとobject localizationの2つのサブモデルで構成され、end-to-endで学習可能。 object discoveryで、 global min-entropyと画像分類lossを最適化。local min-entropyとソフトマックスを最適化。グローバルとローカルそれぞれで物体を検出し、エントロピーを最小化し、グローバルからローカルへ物体確率を伝播。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180619Min-Entropy.jpg" alt="20180619Min-Entropy.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>弱教師付き学習による物体検出は、物体位置と検出を同時に学習するのが困難。弱教師と学習目標間に不一致が生じると物体位置にランダム性が生じ、検出器をうまく学習できない。min-entropyによって、学習中の物体位置のランダム性を計測し、物体位置を学習することができ、検出器のあいまいさを回避できる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>回帰的に学習することによって、弱教師であっても精度向上。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wan_Min-Entropy_Latent_Model_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#166]</div></div></section><section id="ID_Avatar-Net_Multi-scale_Zero-shot_Style_Transfer_by_Feature_Decoration"><div class="paper-abstract"><div class="title">Avatar-Net: Multi-scale Zero-shot Style Transfer by Feature Decoration</div><div class="info"><div class="authors">Lu Sheng, Ziyi Lin, Jing Shao and Xiaogang Wang1</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>既存手法のZero-shot style transferでは画像生成と効率のトレードオフによって，高品質な画像の生成とリアルタイムでの画像生成(style transfer)が困難．本稿ではこの問題を解決し，効率的かつ効果的な画像生成が可能なAvatar-Netを提案．提案手法では，高品質なstyle transferを可能にし，有効性および効率についても実証．さらに複数のスタイルの統合や動画のデザインを用いたアプリケーションも実装．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Avatar-Net1.png" alt="1"><img src="slides/figs/Avatar-Net2.png" alt="2"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p><img src="slides/figs/Avatar-Net3.png" alt="3"></p><ul><li>任意のスタイル画像から意味的に抽出されたスタイル特徴によってコンテンツ特徴を構成するスタイルデコレータを提案</li><li>スタイルデコレータにより，全体の特徴分布を一致させるだけでなく，装飾された特徴のスタイルパターンも保持</li><li>スタイルデコレータをマルチスケールで抽象化したスタイルを融合させるimage reconstruction networkに組み込むことで，Avatar-Netは1つのフィードフォワードパスでスタイル画像のマルチスケールのスタイルのレンダリングが可能</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#167]</div><div class="timestamp">2018.6.19 16:39:58</div></div></section><section id="ID_Real-World_Repetition_Estimation_by_Div_Grad_and_Curl"><div class="paper-abstract"><div class="title">Real-World Repetition Estimation by Div, Grad and Curl</div><div class="info"><div class="authors">Tom F. H. Runia, Cees G. M. Snoek andArnold W. M. Smeulders</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>動画中に存在する繰り返しの動作を推定する問題について考慮．既存の研究(フーリエベース)では静的および定常周期性という仮定のもとでは良好な精度であるが，現実的なシーンにおいては測定が困難．そこでウェーブレット変換を適用し，非静的かつ非定常な動画においても適切に処理できる手法を提案．また，非静的かつ非定常な動画で構成されるQUVA Repetition datasetを提案．動画内の繰り返し動作のカウント実験では深層学習による手法に比べ，良好な精度を実現．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Real-World_Repetition_Estimation1.png" alt="1"><img src="slides/figs/Real-World_Repetition_Estimation2.png" alt="2"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>流動場とその微分から，3つの基本的な運動タイプと3次元内の固有周期性の3つの運動周期性を導出</li><li>3次元の周期性の2次元的な知覚は2つの極端な視点を考慮しており，18の基本的なケースを考慮</li><li>様々な繰り返し動作の出現に対応するために，セグメント化された前景の動きに対する時間変化量Ftおよびその差異∇Ft，∇・Ftおよび∇×Ftを測定</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Runia_Real-World_Repetition_Estimation_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#168]</div><div class="timestamp">2018.6.19 15:04:59</div></div></section><section id="ID_CartoonGAN_Generative_Adversarial_Networks_for_Photo_Cartoonization"><div class="paper-abstract"><div class="title">CartoonGAN: Generative Adversarial Networks for Photo Cartoonization</div><div class="info"><div class="authors">Yang Chen, Yu-Kun Lai and Yong-Jin Liu</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>実世界の風景画(写真)を漫画スタイルの画像へ変換する手法の提案．漫画スタイル変換のためのGAN，CartoonGANを提案．ペアの画像を使用しない学習方法を採用し，そのための新規の損失関数を提案．実験では，写真のエッジや滑らかな陰影を保持したまま，アーティストのスタイルを表現することが可能であることを確認．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/CartoonGAN1.png" alt="1"><img src="slides/figs/CartoonGAN2.png" alt="2"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>画風変換には以下のような問題が存在，これにより既存の損失関数においては表現が困難</p><ul><li>漫画のスタイルは高レベルの単純化と抽象化で独特な特徴を表現</li><li>漫画は鮮明なエッジ，滑らかな色合い，比較的シンプルなテクスチャを保有この問題に対処するために以下のような損失関数を提案</li><li>semantic content loss：写真と漫画間のスタイル変換に対応するために，VGGNetの特徴マップを疎な正則化によって定式化</li><li>edge-promoting adversarial loss：鮮明なエッジを維持</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#169]</div><div class="timestamp">2018.6.19 13:57:31</div></div></section><section id="ID_Neural_Style_Transfer_via_Meta_Networks"><div class="paper-abstract"><div class="title">Neural Style Transfer via Meta Networks</div><div class="info"><div class="authors">Falong Shen, Shuicheng Yan and Gang Zeng</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>本稿ではメタネットワークを用いた1つのフィードフォワードパスによる，(style transferのための)ニューラルネットワークパラメータを自動生成する手法を提案．最新のGPU 1つで19 ms以内に任意の新しいスタイルを表現することが可能．また，生成された画像変換ネットワークの容量はわずか449 KBでありモバイルデバイス上でリアルタイムでの実行が可能．</p><img src="slides/figs/NSTviaMeta_Networks1.png" alt="1"></div></div><div class="item2"><div class="text"><p><img src="slides/figs/NSTviaMeta_Networks2.png" alt="2"><img src="slides/figs/NSTviaMeta_Networks3.png" alt="3"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>既存のstyle transferに関する研究の問題点</p><ul><li>スタイル毎にネットワークを学習する必要</li><li>推論の段階で確率的勾配降下による膨大な反復作業によって新規スタイルによる生成能力を欠く可能性</li></ul><p>以上の問題に対応するための策として</p><ul>         <li>スタイル画像を取り込み，対応する画像変換ネットワークを直接生成するメタネットワークを構築</li></ul><p>さらに</p><ul><li>最新のGPU 1枚で19 ms以内に任意の新しいスタイルを表現</li><li>生成された画像変換ネットワークの容量はわずか449 KB</li><li>メタネットワークのhidden featuresを操作することによってスタイル転送ネットワークの多様性について検証</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Neural_Style_Transfer_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#170]</div><div class="timestamp">2018.6.19 17:17:35</div></div></section><section id="ID_Learning_deep_structured_active_contours_end-to-end"><div class="paper-abstract"><div class="title">Learning deep structured active contours end-to-end</div><div class="info"><div class="authors">Diego Marcos, Benjamin Kellenberger, Lisa Zhang, Min Bai, Renjie Liao, Raquel Urtasun</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Shusuke Shigenaka</div><div class="item1"><div class="text"><h1>概要</h1><p>この論文は,隣接する建物の境界線を幾何学的特性を利用して正確に描画するDeep Structured Active Contours (DSAC)の提案である．DSACは制約条件であるActive Contour Models(ACM)と従来のポリゴンモデルを使用している．
今回はCNNを用いてインスタンスごとのACMのパラメータを学習し,
構造化された出力モデルに全てのコンポーネントを組み込む方法を示し,DSACをend-to-endで学習可能にした．
この論文は3つの困難なデータセット"building","instance","segmentation"をDSACで評価し,
state-of-the-artと比較して優れた結果を残している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_deep_structured_active_contours_end-to-end.png" alt="Learning_deep_structured_active_contours_end-to-end"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>CNNベースの方法に高度な幾何情報を利用可能にすることを目指している．<ul><li>明示的に多角形の出力を生成するCNNの作品はあまり行われていない</li></ul></li><li>CNNによる構造化学習はインスタンスレベルのセグメンテーションを扱う作業で認識されない．<ul><li>本手法は相互依存性をACMで調整することを学ぶため,損失をCNNで学習できる．</li></ul></li><li>IoUとエリア推定において従来のDSACより高い精度</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.06329">論文</a></li><li><a href="https://github.com/dmarcosg/DSAC">github</a></li></ul></div></div><div class="slide_index">[#171]</div><div class="timestamp">2018.6.19 17:47:24</div></div></section><section id="ID_TieNet_Text-Image_Embedding_Network_for_Common_Thorax_Disease_Classication_and_Reporting_in_Chest_X-ray"><div class="paper-abstract"><div class="title">TieNet: Text-Image Embedding Network for Common Thorax Disease Classiﬁcation and Reporting in Chest X-ray</div><div class="info"><div class="authors">Xiaosong Wang et al.</div><div class="conference">CVPR2018</div><div class="paper_id">1801.04334</div></div><div class="slide_editor">Takumu Ikeya</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>胸部のレントゲン写真から胸部疾病の分類及び報告を行うためのテキスト画像埋め込みネットワークの提案．</li><li>意味のあるテキストワードや画像領域を可視化するためのmultilevel attention modelsをend-to-endで学習可能なCNN-RNNアーキテクチャに統合．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/TieNet.PNG" alt="TieNet.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>分類精度を向上させるため，学習からattentionベースの画像と文字列内部表現の両方を組み合わせる手法が特徴．</li><li>提案したフレームワークは作成した評価用データセットの疾病ラベル割り当てタスクでAUCs平均0.9を達成．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1801.04334.pdf">論文</a></li></ul></div></div><div class="slide_index">[#172]</div><div class="timestamp">2018.6.19 16:12:55</div></div></section><section id="ID_Free_supervision_from_video_games"><div class="paper-abstract"><div class="title">Free supervision from video games</div><div class="info"><div class="authors">Philipp Krahenbuhl</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Masaki Miyamoto</div><div class="item1"><div class="text"><h1>概要</h1><p> 深層ネットワークでは大量のデータが必要で，ラベル付けされたデータはネットワークのデザイン同様深層ネットワークにとって重要である．しかし手作業の収集はお金と時間がかかる．そこでMicrosoftのDirectXレンダリングAPIを用いてゲームをやりながらリアルタイムでセグメンテーションやオプティカルフローなどのための正解ラベルを作成する手法を提案する．集めたデータセットは他の合成データセットより視覚的に現実世界と近いものになっている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/from_game.PNG" alt="from_game.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>このシステムはリアルタイムにすべてのラベルを計算するため直接ゲームのレンダリングパイプラインにコードを組み込んでいる．また人によるアノテーションが必要ない．さらに，様々なデザインの複数のゲームにおいてこの手法を用いることができる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2501.pdf">論文</a></li></ul></div></div><div class="slide_index">[#173]</div><div class="timestamp">2018.6.19 16:11:51</div></div></section><section id="ID_Can_Spatiotemporal_3D_CNNs_Retrace_the_History_of_2D_CNNs_and_ImageNet"><div class="paper-abstract"><div class="title">Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?</div><div class="info"><div class="authors">Kensho Hara, Hirokatsu Kataoka, Yutaka Satoh</div><div class="conference">CVPR 2018</div><div class="paper_id">arXiv:1711.09577</div></div><div class="slide_editor">Tenga Wakamiya</div><div class="item1"><div class="text"><h1>概要</h1><p>動画データセット上の比較的浅いものから非常に深いものまでの様々な3DCNNの構造を調べた．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Can_Spatiotemporal_3D_CNNs_Retrace_the_History_of_2D_CNNs_and_ImageNet.png" alt="Can_Spatiotemporal_3D_CNNs_Retrace_the_History_of_2D_CNNs_and_ImageNet.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ResNet-18の学習は，UCF-101，HMDB-51，およびActivityNetの過学習していて，Kineticsは過学習しなかった．</li><li>Kineticsのデータセットは，深い層の3DCNNで学習するために十分なデータがあり，ImageNetの2D ResNetsと同様に，最大152のResNets層の学習を可能にし，ResNeXt-101は，Kineticsのテストセットで平均78.4％の精度がある．</li><li>UCF-101およびHMDB-51上の複雑な2DアーキテクチャよりもKineticsの事前学習されたシンプルな3Dアーキテクチャが優れていて，UCF-101およびHMDB-51でそれぞれ94.5％および70.2％を達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.09577">論文</a></li></ul></div></div><div class="slide_index">[#174]</div><div class="timestamp">2018.5.29 15:59:46</div></div></section><section id="ID_Gibson_Env_Real-World_Perception_for_Embodied_Agents"><div class="paper-abstract"><div class="title">Gibson Env: Real-World Perception for Embodied Agents</div><div class="info"><div class="authors">Fei Xia, Amir R. Zamir, Zhiyang He, Alexander Sax, Jitendra Malik and Silvio Savarese</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ロボットなどのエージェントに知覚を身につけさせるためのGibsonという仮想環境を提案した。Gibsonは572の建物、1447のフロアから構築されている。
RGB-Dデータから、任意のカメラ位置でレンダリングする場合欠損が生じてしまう。
そこで、複数のカメラ位置でレンダリングした画像を組み合わせた上で、Neural Netにより欠損箇所を保管する。
得られた画像はリアルではないため、レンダリング画像とリアル画像間のドメイン変換手法Gogglesを提案した。
また、物理エンジンを組み込むことにより、実世界で起こる衝突などの判定を可能にした。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Gibson_Env_Real-World_Perception_for_Embodied_Agents.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>目的地へ向かう、階段を上るといったエージェントのタスクに加え、depth推定、シーン認識によって有効性を検証した。実世界で撮影した画像によるテストでは、他のデータセットと比べ1番精度が良かった。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://gibsonenv.stanford.edu/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#175]</div><div class="timestamp">2018.6.19 14:37:08</div></div></section><section id="ID_Multimodal_Visual_Concept_Learning_with_Weakly_Supervised_Techniques"><div class="paper-abstract"><div class="title">Multimodal Visual Concept Learning with Weakly Supervised Techniques</div><div class="info"><div class="authors">Giorgos Bouritsas, Petros Koutras, Athanasia Zlatintsi and Petros Maragos</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>従来の動画認識に関する研究は、映像情報のみを用いているものが多く字幕のようなテキストや音などの情報は利用されていない。動画認識のタスクに、映像情報に加えテキスト情報を利用するための手法を提案した。
考慮すべきこととして、映像とテキストの情報が時系列的にどのように対応しているか、同じラベルに対してテキストでは複数の表現方法が存在している、という2つの点が挙げられる。
そこで、時系列的な対応付けを行うFuzzy Sets MIL(FSMIL)とテキストがどのラベルに対応しているかを推定するProbabilistic Labels MIL(PLMIL)の2つの学習方法を提案した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Multimodal_Visual_Concept_Learning_with_Weakly_Supervised_Techniques.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>動画認識タスクとして、顔認識及びアクション認識の2つによりテストを行いベースラインと比べ精度が向上したことを確認した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Bouritsas_Multimodal_Visual_Concept_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#176]</div><div class="timestamp">2018.6.19 13:04:08</div></div></section><section id="ID_Photometric_Stereo_in_Participating_Media_Considering_Shape-Dependent_Forward_Scatter"><div class="paper-abstract"><div class="title">Photometric Stereo in Participating Media Considering Shape-Dependent Forward Scatter</div><div class="info"><div class="authors">Y. Fujimura, M. Iiyama, A. Hashimoto, M. Minoh</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>濁った水や霧の中で撮影したような，散乱光により劣化したような画像に対して適用可能な3D復元手法の提案．</p><p>形状依存の前方散乱（forward scatter）を扱うモデルを考え，ルックアップテーブル使用で解析的に求める，
それを空間的変化カーネルとして表現する．
また，前方散乱の除去を可能にする，大規模密行列を疎行列に近似する手法を提案．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Photometric_Stereo_in_Participating_Media_Considering_Shape-Dependent_Forward_Scatter.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>厳密に形状依存の表面-カメラ間前方散乱をモデル化し，その解析的解法を提案したものは初めて．</p><p>実，合成データに対して改善的性能を示した．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2871.pdf">論文</a></li></ul></div></div><div class="slide_index">[#177]</div><div class="timestamp">2018.6.21 13:38:56</div></div></section><section id="ID_Sparse_Smart_Contours_to_Represent_and_Edit_Images"><div class="paper-abstract"><div class="title">Sparse, Smart Contours to Represent and Edit Images</div><div class="info"><div class="authors">T. Dekel, D. Krishnan, C. Gan, C. Liu, W. Freeman</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>かなりスパースな輪郭線（元画像の4%程度のデータ量）から大変きれいな画像の復元ができ，更に輪郭線を調節すると大変きれいにパーツ位置を変えられる．
参照画像も変更できるので，髪を生やせるし，（効果は薄いが）人の鼻を犬っぽくできる．</p><p>まず，入力の輪郭線を工夫する．<a href="https://www.microsoft.com/en-us/research/publication/structured-forests-for-fast-edge-detection/">この手法</a>でスパースな輪郭線を取り，
輪郭線の左右の画素の色(RGB)を色値（RGB×左右＝計6値）とする．
また，画像の各色における勾配を取り，輪郭線の位置におけるRGB×XY成分＝計6値を勾配値とする．
ここからN次元特徴マップを（GANを回している最中に）学習する．
構造は<a href="https://arxiv.org/abs/1606.00915">Deeplab</a>を参考にしたDilated Conv.による簡素なネットワーク構造による．</p><p>この輪郭線特徴を入力として，2段階の復元用U-Netを生成器に，Dilated-Patch Discriminatorを判別器にしたGANを回す．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Sparse_Smart_Contours_to_Represent_and_Edit_Images.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>アプリケーションとしてかなり使い出かあるように見える．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>実験的に見て，N=3がいいらしい．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0085.pdf">論文</a></li><li><a href="https://contour2im.github.io/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#178]</div><div class="timestamp">2018.6.19 10:30:38</div></div></section><section id="ID_Document_Enhancement_using_Visibility_Detection"><div class="paper-abstract"><div class="title">Document Enhancement using Visibility Detection</div><div class="info"><div class="authors">N. Kligler, S. Katz and A. Tal</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>文書から二値化，陰影除去をするのに使えるDocument Enhancementの話．文書平面を三次元化し，文書面から凸凹を除去するという形で可視領域（Visibility）の検出をし，
それをベースに鮮鋭化するというやり方．
本手法を前処理として，二値化手法や陰影除去を適用するとSOTA性能を上回る．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Document_Enhancement_using_Visibility_Detection.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>基本方針としては，識別性を高める高次元空間への変換のやり方を考えました，という非ディープなパタレコにおけるノリ．</p><p>論文の質としては他論文と比較して若干劣るように感じられるが，「平面だけど三次元点群にするとうまくいくとは，驚きだ！」と言っていて，それがウケたのだろうか．
おそらく当初の発想も文書の凸凹を消すという発想だったと思われる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>肝心の3次元空間への射影の具体的な実装（(x, y)→(θ，φ)の部分）が読み取れませんでした．どなたか再現できたらご教授頂けますと幸いです．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2447.pdf">論文</a></li></ul></div></div><div class="slide_index">[#179]</div><div class="timestamp">2018.6.19 09:52:48</div></div></section><section id="ID_An_Efficient_and_Provable_Approach_for_Mixture_Proportion_Estimation_Using_Linear_Independence_Assumption"><div class="paper-abstract"><div class="title">An Efficient and Provable Approach for Mixture Proportion Estimation Using Linear Independence Assumption</div><div class="info"><div class="authors">Xiyu Yu, Tongliang Liu, Mingming Gong, Kayhan Batmanghelich, Dacheng Tao</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>混合分布内のラベルなしデータと少量のラベルありデータから正しく分布の重み（Weights of components）を推定し、画像分類を行う問題を提供。この問題自体をMixture Proportion Estimation（MPE）という。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180618LinearIndependenceAssumption.png" alt="180618LinearIndependenceAssumption"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>データに多数のノイズを含んでいても、少量のラベル付きデータから混合分布の割合を把握して正しく画像分類を行うことができるアルゴリズムを提案。Web画像に見られるラベルノイズが発生している学習/Semi-supervised学習、合成データ/実世界データの両者においてState-of-the-artな精度を達成した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>ラベルノイズに関する新規の問題MPEを提供した。一見すると既存の問題と思われるようなものでもまだまだ重要で提案されていない問題は残っている？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_An_Efficient_and_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#180]</div><div class="timestamp">2018.6.18 22:55:08</div></div></section><section id="ID_Geometry_Aware_Constrained_Optimization_Techniques_for_Deep_Learning"><div class="paper-abstract"><div class="title">Geometry Aware Constrained Optimization Techniques for Deep Learning</div><div class="info"><div class="authors">Soumava Kumar Roy, Zakaria Mhammedi, Mehrtash Harandi</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>勾配の最適化手法であるStochastic Gradient Descent（SGD）やRMSPropアルゴリズムをRiemannian Optimizationの設定にて一般化する手法を提案する。SGDはDNNでは一般的に用いられるが、勾配の最適化に大きな分散があり、一方でRMSPropやADAMがこの問題を解決するために提案されてきたが決定だとは言えなかった。本論文ではRiemannian Centroidsの計算や深層距離学習（Deep Metric Learning）を考慮して勾配最適化の不安定性に取り組む。詳細画像識別問題に取り組むことで提案手法の有効性を示した。右図は最適化のイメージ図であり、Riemannian多様体空間で勾配計算と誤差最適化を測ることで安定感のある最適化を実現。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180618GeometryAwareConstrainedOptimization.png" alt="180618GeometryAwareConstrainedOptimization"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>多様体空間で最適化を実現するcSGD-M/cRMSPropを提案、問題設定に対して拘束を強めてダイレクトに最適化ができる手法とした。機械学習の文脈において、PCA/DMLの拡張と位置付けられる手法を提案。同枠組みを詳細画像識別問題に適用したところ、Competitiveな結果を達成した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>発想が数学の人、〜を＊＊の枠組みで最適化するというのは得意技？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Roy_Geometry_Aware_Constrained_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#181]</div><div class="timestamp">2018.6.18 22:31:00</div></div></section><section id="ID_View_Extrapolation_of_Human_Body_from_a_Single_Image"><div class="paper-abstract"><div class="title">View Extrapolation of Human Body from a Single Image</div><div class="info"><div class="authors">Hao Zhu, Hao Su, Peng Wang, Xun Cao, Ruigang Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ある視点の人物画像からターゲットとなる視点（Novel View）の人物画像を復元するタスクを提案。従来法であるVSAP（参考文献40）では正確な視点変化に関するフローを推定することができなかったが、提案法ではまず距離画像を推定してからフロー推定することで精度を劇的に改善した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180618ViewExtrapolationHumanBody.png" alt="180618ViewExtrapolationHumanBody"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>距離画像の復元（予め形状を復元することに相当）することにより、ビューポイント変化に関するフローの推定精度を劇的に向上させ、さらにバックフローも組み合わせることでターゲット視点の人物画像復元を改善。距離画像の復元からオプティカルフローの推定を行うこのような枠組みをShape-from-Appearanceという？3次元的な情報があることで姿勢に関するバリエーションがあったとしてもロバストなビューポイント変化の人物画像推定が可能。合成データによる人物画像データセットも作成、2,000の姿勢に対して22のアピアランス変化を含む。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>以前は経由する情報をいかに少なくしてダイレクトに復元を行うか、が重要であったが、DNN時代になってから効果的な情報復元（この場合は距離画像による形状復元）を経由することにより推定精度が向上。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_View_Extrapolation_of_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#182]</div><div class="timestamp">2018.6.18 22:01:34</div></div></section><section id="ID_Geometric_robustness_of_deep_networks_analysis_and_improvement"><div class="paper-abstract"><div class="title">Geometric robustness of deep networks: analysis and improvement</div><div class="info"><div class="authors">Can Kanbak, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>幾何学的な変換に頑健なDNNを考案。従来のDNNでは例えば右図のようなアフィン変換（ここでは主に回転）に対して脆弱であり、上図では馬の種類を答えていたものが、多少の回転を与えるだけで犬の種類を答えてしまう。本論文ではManiFoolというシンプルだがスケーラブル、多様体（Manifold）ベースのアルゴリズムManiFoolを提案、幾何学的な変化に対する不変性や複雑ネットワークに対する評価を行う。さらに、Adversarial Trainingにより幾何学的な変動に頑健なモデルとなるような学習法を実装した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180618GeometricRobustnessDNN.png" alt="180618GeometricRobustnessDNN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>最小の幾何学的変換により認識を誤ってしまう問題に対して不変性を計測するManiFoolを提案したことがもっとも大きな貢献である。ImageNet等の大規模データに対して幾何学的変換とそのロバスト性を評価した最初の論文である。ManiFoolアルゴリズムをAdversarial Trainingに応用して幾何学的変換に対してロバストな学習法を提案。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>実環境（撮影時のカメラのビューポイント）を多少回転させるのではなく画像をダイレクトなアフィン変換にて回転させるからエラーが生じる？もう少し解析して欲しいような気もする。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Kanbak_Geometric_Robustness_of_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#183]</div><div class="timestamp">2018.6.18 21:28:50</div></div></section><section id="ID_Learning_Strict_Identity_Mappings_in_Deep_Residual_Networks"><div class="paper-abstract"><div class="title">Learning Strict Identity Mappings in Deep Residual Networks</div><div class="info"><div class="authors">Xin Yu, Zhiding Yu, Srikumar Ramalingam</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>自動的に冗長なレイヤを除外してくれるε-ResNetを提案し、よりコンパクトなサイズで最大限の認識パフォーマンスを実現する。ε-ResNetでは閾値εを設けて、これよりも小さい値を出力するレイヤに対して誤差を計算しないという方策を取る。提案法であるε-ResNetを実現するために、少量のReLUを加えることで実現した。CIFAR-10,-100,SVHN,ImageNetに対して単一のトレーニングプロセスで学習が成功し、なおかつ約80%ものパラメータ削減を実行した。右図は752層のε-ResNetを実装して最適化した例である。図中の赤ラインは除去されたレイヤ、青ラインは認識に対して必要と判断されたレイヤである。図の例では、CIFAR-100に対するオリジナル（ResNet-752）のエラー率が24.8%、提案法（ε-ResNet-752）のエラー率が23.8%であった。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180618EpsilonResNet.png" alt="180618EpsilonResNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ResNetを対象として、レイヤを増加させることによる冗長性を自動的に除去してくれるε-ResNetを提案した。ε-ResNetは従来の枠組みに対して4つのReLUを組み合わせ、閾値カット処理だけで実装可能である。より深い層のモデルに対して有効であり、大体80%くらいの冗長生をカットする。パラメータ数を減らしつつも超ディープなモデルにおいて多少の精度向上が見込める。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>実装が非常に簡単そうであり、すでにDNNフレームワークにおいて実装されていれば、広く使ってもらえそう。また、各タスク（e.g. 物体検出、セグメンテーション、動画認識）において気軽に使用することができれば、広がりがありそう。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Learning_Strict_Identity_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#184]</div><div class="timestamp">2018.6.18 20:51:56</div></div></section><section id="ID_Generative_Adversarial_Perturbations"><div class="paper-abstract"><div class="title">Generative Adversarial Perturbations</div><div class="info"><div class="authors">Omid Poursaeed, Isay Katsman, Bicheng Gao, Serge Belongie</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>敵対的サンプル（Adversarial Examples）を生成的に作りだすモデルを考案し、自然画像に対して摂動ノイズを与えて学習済みモデルを効果的にだます手法（GAP; Generative Adversarial Perturbations）を提案する。提案のGAPは画像に依存する/しない摂動ノイズ、いずれも生成することが可能であり、画像識別やセマンティックセグメンテーションに対して有効。また、ImageNet/Cityscapesを用いたより高解像な画像においても効果的に識別器をだますことに成功した。さらに、従来の同様の枠組みよりもより速く推論を行うことができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180618GenerativeAdversarialPertubations.png" alt="180618GenerativeAdversarialPertubations"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>より汎用的かつ画像依存性のあり/なしに関わらない摂動ノイズを、画像識別/セマンティックセグメンテーションに対して行うことができる。それでいてUniversal Perturbationsの枠組みを生成モデルにより実装、より効果的にだますことに成功。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>この論文は引用されそう？だが、ホントの意味で騙せているのかは不明である。（Adversarial Examplesの論文は、会議の前に攻略法がarXivに載せられるなどまだまだ研究が必要である）</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Poursaeed_Generative_Adversarial_Perturbations_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#185]</div><div class="timestamp">2018.6.18 20:24:14</div></div></section><section id="ID_The_Lovasz-Softmax_loss_A_tractable_surrogate_for_the_optimization_of_the_intersection-over-union_measure_in_neural_networks"><div class="paper-abstract"><div class="title">The Lovasz-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</div><div class="info"><div class="authors">Maxim Berman, Amal Rannen Triki, Matthew B. Blaschko</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>セマンティックセグメンテーションにおいて、ピクセルごとの最適化ではなく領域（Intersection-over-Union）ごとの最適化を行うことで小領域を含む領域ベースのセグメンテーションを改良する。この問題に対して、サブモデュラ凸最適化手法Lovasz（参考文献26をベースとした）を用いることで誤差計算を行う。このLovasz-Softmax Lossは従来のCross-Entropy Lossよりも領域評価jに対して頑健であることを示した（右図）。位置付け的にはLovasz Hinge Lossのマルチカテゴリに対する一般化である。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180618Lovasz-SoftmaxLoss.png" alt="180618Lovasz-SoftmaxLoss"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>セマンティックセグメンテーションにおいて特に小領域であったとしても適切に評価して誤差を計算できるLovasz-Softmax Lossを提案した。PascalVOCやCityscapesにおいてCross-Entropy Lossを用いた誤差計算よりも良好な性能を示すことが明らかとなった。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>IoUで最適化するとは？また、Jaccard indexとは何のことだろう？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Berman_The_LovaSz-Softmax_Loss_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#186]</div><div class="timestamp">2018.6.18 20:06:18</div></div></section><section id="ID_Deep_Diffeomorphic_Transformer_Networks"><div class="paper-abstract"><div class="title">Deep Diffeomorphic Transformer Networks</div><div class="info"><div class="authors">Nicki Skafte Detlefsen, Oren Freifeld, Søren Hauberg</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>顔認識において、本人認識率が向上するようにアフィン変換や形状変化（Diffeomorphic）を行うように変換を実装するネットワークDeep Diffeomorphic Transformer Networksを提案。直感的にはズームインだが、さらに形状変化を行うことが効果的であると判断してネットワークを構築した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180618DiffeemorphicTransferNetworks.png" alt="180618DiffeemorphicTransferNetworks"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>顔認識においてアフィン変換によるズームインのみならず、認証率が向上するような形状変化方法であるDiffeomorphic Transferを提案した。同処理はCNN内に実装され、Deep Diffeomorphic Transformer Networksと呼ばれ、LFW/CelebA等でState-of-the-artであった。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>ネットワークに対して内的ではなく外的に変形させて精度向上するのは意外である。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Detlefsen_Deep_Diffeomorphic_Transformer_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#187]</div><div class="timestamp">2018.6.18 19:47:55</div></div></section><section id="ID_Geometry-Aware_Scene_Text_Detection_with_Instance_Transformation_Network"><div class="paper-abstract"><div class="title">Geometry-Aware Scene Text Detection with Instance Transformation Network</div><div class="info"><div class="authors">Fangfang Wang, Liming Zhao, Xi Li, Xinchao Wang and Dacheng Tao</div><div class="conference">CVPR2018</div><div class="paper_id">167</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>幾何学的な表現を用いたEnd-to-endのシーンテキスト認識アプローチ．シーンテキストインスタンスの幾何学的構成をエンコーディングするため，幾何学的な表現を学習するInstance Transformation Network (ITN)を提案する．右図上部の（a）のように，いくつか並んだサンプルグリッド（橙色）をテキストにフィッティング（青色）する．また，（b）のように入力画像（の特徴マップ）からフィッティングのためのモデルを学習する．ネットワーク構成は，特徴抽出部，インスタンスレベルのアフィン変換を予測する部分，幾何学的表現部からなる．変換の回帰，座標の回帰，分類はマルチタスク学習となる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180619_ITN1.jpg" alt="20180619_ITN1.jpg"><img src="slides/figs/20180619_ITN2.jpg" alt="20180619_ITN2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>幾何学的表現で強いアフィン変換がかかっていても頑健なテキスト検出が可能である．データセットにはICDAR2015およびMSRA-TD500を用いて評価を行う．ベースネットワークにResNet50を用いた場合，MSRA-TD500のPrecisionは90.3，F値は80.3と非常に高精度な結果となった．ICDAR2015ではVGG16ベースの方が良い結果となり，Precisionは85.7，F値は79.5である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>幾何学的なドット列をフィッティングする手法は他にも応用が効きそう．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1653.pdf">論文</a></li></ul></div></div><div class="slide_index">[#188]</div><div class="timestamp">2018.6.19 03:01:57</div></div></section><section id="ID_Textbook_Question_Answering_under_Instructor_Guidance_with_Memory_Networks"><div class="paper-abstract"><div class="title">Textbook Question Answering under Instructor Guidance with Memory Networks</div><div class="info"><div class="authors">Juzheng Li, Hang Su, Jun Zhu, Siyu Wang and Bo Zhang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>教科書(テキストデータ＋画像)に含まれている情報に関する質問に答える、Textbook Question Answering(TQA)に関する研究。質問の答えはテキストの局所的な部分に含まれていることが多く、テキストの要約によって答えを得ることが難しい場合が多い。
本研究では、テキストや画像から得られる因果関係や構造を表したContradiction Entity-Relationship Graph(CERG)を構築し、矛盾を探すための手がかり(Guidance)とすることで局所的な情報を使用して質問に答えることを可能とする。
CERGの構築には画像特徴とテキスト特徴を使用し、質問の答えには画像特徴とテキスト特徴に加えCERGから得られたGuidanceを用いることで出力を得る。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Textbook_Question_Answering_under_Instructor_Guidance_with_Memory_Networks.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Contextが多く要約することが難しい場合、得られる情報をグラフにして記憶することが効率的であるということを示した。ベースラインやランダムに選択する場合と比べて、あらゆる質問のタイプ(truth or falseやmultiple choise)において正解率が向上していることを確認した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>一応画像情報を使用しているが、全体的にはNLP色が強いと感じた。手法としての完成度は非常に高く、評価は問題自体が新しいこともあり数値評価（従来法との比較、モデル設計の評価）及びqualitativeな比較であった。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Textbook_Question_Answering_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#189]</div><div class="timestamp">2018.6.19 00:14:07</div></div></section><section id="ID_Multi-Evidence_Filtering_and_Fusion_for_Multi-Label_Classification_Object_Detection_and_Semantic_Segmentation_Based_on_Weakly_Supervised_Learning"><div class="paper-abstract"><div class="title">Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning</div><div class="info"><div class="authors">Weifeng Ge, Sibei Yang and Yizhou Yu</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Kazuho Kito</div><div class="item1"><div class="text"><h1>概要</h1><p>マルチレベルの物体認識，検出，セマンティックセグメンテーションのための弱教師カリキュラム付き学習のパイプラインを提案。このパイプラインは物体位置の中間点と訓練画像のピクセルのラベルの結果をを入手し、結果を用いて教師付きのやり方で特定のタスクの深層学習で訓練する。その全体のプロセスは4つのステージを含む、訓練画像の物体位置を含み、物体のインスタンスのフィルタリングと結合し、訓練画像のピクセルラベリングをし、特定のタスクのネットワークでトレーニングをする。訓練画像からキレイな物体のインスタンスを入手することで、物体のインスタンスのフィルタリング、結合、クラスファイリングのための新しいアルゴリズムを複数の解決策から集める。このアルゴリズムは、検出された物体のインスタンスをフィルタリングするため、metric learningと密度ベースのクラスタリングの両方を組み込んでいる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Multi-Evidence_Filtering_and_Fusion_for_Multi-Label_Classification.PNG" alt="Multi-Evidence_Filtering_and_Fusion_for_Multi-Label_Classification.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>マルチレベルの画像の分類においてstate-of-the-artを達成．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1802.09129.pdf">論文</a></li></ul></div></div><div class="slide_index">[#190]</div><div class="timestamp">2018.6.18 23:24:56</div></div></section><section id="ID_ShuffleNet_An_Extremely_Efficient_Convolutional_Neural_Network_for_Mobile_Devices"><div class="paper-abstract"><div class="title">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</div><div class="info"><div class="authors">Xiangyu Zhang et al.</div><div class="conference">CVPR2018</div><div class="paper_id">1707.01083</div></div><div class="slide_editor">Takumu Ikeya</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>モバイルデバイス向けに特別に設計した非常に計算効率の良いCNNアーキテクチャである“ShufflNet”を開発した．このアーキテクチャではpointwise group convolutionとchannel shuffleという2つの新しい演算を使用し，精度を落とすことなく，計算コストを大幅に削減した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Shuffle_Net_An_Extremely_Efficient_Convolutional_Neural_Network_for_Mobile_Device.png" alt="Shuffle_Net_An_Extremely_Efficient_Convolutional_Neural_Network_for_Mobile_Device.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ImageNetによる分類とMS COCOによる物体検出のタスクではほかのアーキテクチャよりも高い性能を示した．</li><li>40MFLOPの計算資源の制約のもと,ImageNet分類タスクで他のモバイルデバイス向けアーキテクチャよりもtop-1 エラーが7.8%低い結果が得られた．</li><li>既存のアーキテクチャよりも高精度で計算効率が非常に良い“ShufflNet”というアーキテクチャを提案した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1707.01083.pdf">論文</a></li></ul></div></div><div class="slide_index">[#191]</div><div class="timestamp">2018.6.18 21:27:54</div></div></section><section id="ID_What_have_we_learned_from_deep_representations_for_action_recognition"><div class="paper-abstract"><div class="title">What have we learned from deep representations for action recognition?</div><div class="info"><div class="authors">Christoph Feichtenhofer et al.</div><div class="conference">CVPR2018</div><div class="paper_id">1801.01415</div></div><div class="slide_editor">Takumu Ikeya</div><div class="item1"><div class="text"><h1>概要!</h1><ul><li>動画中の行動を認識するためにtwo stream modelが学習したものを視覚化することで時空間表現がどのように働いているか調査した研究．</li><li>単純に形状特徴と動作特徴を分割するよりも，cross-stream fusionは正しい時空間特徴を学習することが可能．</li><li>ネットワークはクラス特有の局所表現だけでなく，様々なクラスに対応できる汎用表現を学習することが可能．</li><li>ネットワークの階層全体を通して，特徴はより抽象的になり，ある動作の区別にとって重要でないデータに対する不変性が増加．</li><li>視覚化は、学習された表現を確認するだけでなく，学習データの独自性を明らかにし，systemの失敗例の説目に利用可能.</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/What_have_we_learned_from_deep_representations_for_action_recognition.PNG" alt="What_have_we_learned_from_deep_representations_for_action_recognition.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ランダムに初期化されたノイズ画像とノイズ動画の入力から開始するモデルの時空間の入力を直接最適化する.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1801.01415.pdf">論文</a></li></ul></div></div><div class="slide_index">[#192]</div><div class="timestamp">2018.6.18 21:04:41</div></div></section><section id="ID_A_Perceptual_Measure_for_Deep_Single_Image_Camera_Calibration"><div class="paper-abstract"><div class="title">A Perceptual Measure for Deep Single Image Camera Calibration</div><div class="info"><div class="authors">Y. Hold-Geoffroy, K. Sunkavalli, J. Eisenmann, M. Fisher, E. Gambaretto, S. Hadap and J.F. Lalonde</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>単画像におけるカメラパラメータのキャリブレーションの話．事前知識なしに非コントロール環境でもちゃんと動くように，
DCNNによるキャリブレーションパラメータの直接推測手法を提案する．</p><p>ImageNet学習済みDenseNetの最終層を３つの分離したヘッドに置き換え，それぞれ水平角度推定，水平線の中心からの距離，縦方向の場を表すように改造する．
これを，大規模パノラマ画像データセットから自動生成したサンプルにより学習する．</p><p>評価については，実際人がおかしさを感じるかどうかによるので，AMTで聞いてみた結果から導いた人の誤差モデルをもとに語ってみる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/A_Perceptual_Measure_for_Deep_Single_Image_Camera_Calibration.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>結果はそれなりにできている．が，それなりっぽく見えてしまうので，人間の感じ方もちゃんと調べて載せた！
というのが評価されているように思う．</p><p>ネットワーク構造の簡単な調整で達成できたところが，DNNの手に掛かれば様々な問題が如何様にも解ける感じを醸し出していておもしろい．</p><p>アプリケーション枠狙いにするためか，アプリケーション例をいくつか掲載している．論文自体，他のアプリケーション系論文と比べて，読んでいて飽きない感じがする．
合わせ技一本，という感じがする．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>速読したからかもしれないが，不思議な構成の論文だった．論点が2つあるからだろうか．違和感は感じるが，なんとかうまく収めている感じもする．</p><p>NVidiaにGPUを寄付してもらったらしい．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2618.pdf">論文</a></li></ul></div></div><div class="slide_index">[#193]</div><div class="timestamp">2018.6.18 19:26:49</div></div></section><section id="ID_SplineCNN_Fast_Geometric_Deep_Learning_with_Continuous_B-Spline_Kernels"><div class="paper-abstract"><div class="title">SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels</div><div class="info"><div class="authors">Matthias Fey, Jan Eric Lenssen, Frank Weichert, Heinrich M¨uller</div><div class="conference">CVPR2018</div></div><div class="slide_editor">KazukiTsubura</div><div class="item1"><div class="text"><h1>概要</h1><p>グラフなどの不規則な構造をした幾何学的入力のためのディープニューラルネットワークの変形であるスプラインベースの畳み込みニューラルネットワーク(SplineCNN)．スペクトル領域内でフィルタリングするのではなく，純粋に空間領域で特徴集計をする．SplineCNNを使用することで，手作業による特徴記述子の代わりに入力として幾何学的構造を使用することで，深いアーキテクチャの完全なend-to-endの学習が可能になる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Fast_Geometric_Deep_Learning_with_Continuous_B-Spline_Kernels1.png" alt="Fast_Geometric_Deep_Learning_with_Continuous_B-Spline_Kernels1"></p></div></div><div class="item3"><div class="text"><h1>新規性・差分</h1><p>グラフやmeshesのような不規則な構造をした様々な点で利用でき，空間上における入力の幾何学的関係を発見する．手作業による特徴記述子を使用せずにend-to-endの学習が可能になり，また，最先端の幾何学的な学習と同等である．</p><ul><li><a href="https://arxiv.org/pdf/1711.08920.pdf">論文</a></li><li><a href="https://github.com/rusty1s/pytorch_geometric">Github</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/Fast_Geometric_Deep_Learning_with_Continuous_B-Spline_Kernels2.png" alt="Fast_Geometric_Deep_Learning_with_Continuous_B-Spline_Kernels2"></p></div></div><div class="slide_index">[#194]</div><div class="timestamp">2018.6.18 18:45:52</div></div></section><section id="ID_Learning_and_Using_the_Arrow_of_Time"><div class="paper-abstract"><div class="title">Learning and Using the Arrow of Time</div><div class="info"><div class="authors">Donglai Wei et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>DNN を用いて動画中の時間の流れている方向（Arrow of Time）を学習する研究. 人工的な信号を含むキューは Arrow of Time の学習に悪影響を及ぼすことを示し, それらの影響を取り除いた大規模 dataset を作成した. 評価実験では映画中の逆再生部分を検出するというタスクにおいて人間とほぼ同程度の精度を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Learning-and-Using-the-Arrow-of-Time.png" alt="fukuhara-Learning-and-Using-the-Arrow-of-Time.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Arrow of Time を学習する DNN アーキテクチャとして Temporal Class-Activation Map Network (T-CAM) を提案</li><li>T-CAM は数フレーム分の optical flow を入力から Arrow of Time を推測</li><li>人工的な信号である camera Motion や black framing を含むキューは Arrow of Time の推定を容易にし, ネットワークの学習に悪影響を与えてしまうことを実験により示した</li><li>上記の人工的な信号を取り除いた Arrow of Time を学習するための大規模データセット, Flickr-AoT と Kinetics-AoT を作成</li><li>提案手法を用いて行った映画の逆再生部分を検出する実験では, 人間（80%）とほぼ同等（76%）の結果を達成</li><li>また, Arrow of Time が flow-based の行動認識において self-supervised pre-training に有用であることを示した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://people.csail.mit.edu/donglai/paper/aot18.pdf" target="blank">[論文] Learning and Using the Arrow of Time</a></li><li><a href="https://www.youtube.com/watch?v=1zfZhXkOzCw" target="blank">[動画] YouTube</a></li></ul></div></div><div class="slide_index">[#195]</div><div class="timestamp">2018.5.17 12:19:55</div></div></section><section id="ID_Missing_Slice_Recovery_for_Tensors_Using_a_Low-rank_Model_in_Embedded_Space"><div class="paper-abstract"><div class="title">Missing Slice Recovery for Tensors Using a Low-rank Model in Embedded Space</div><div class="info"><div class="authors">T. Yokota, B. Erem, S. Guler, S.K. Warfield and H. Hontani</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>テンソルがスライス方向に欠けてしまった場合の復元についての論文．このケースでは，よく行われる核ノルム利用やその他正則化手法ではムリ．
遅れ／シフトに不変な構造を捉えることが重要になることから，
「高次元空間への低ランクモデルの埋め込み」を行うことで解決する．
時系列の遅延埋め込みを，テンソルにおける「複数方向遅延埋め込み変換」
を行い，不完全なテンソルを高次不完全ハンケルテンソルへと変換する．
その後，この高次テンソルをタッカー展開の枠組みで低ランク化することで
復元が行われる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Missing_Slice_Recovery_for_Tensors_Using_a_Low-rank_Model_in_Embedded_Space.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>伝統的に行われてきた行列・テンソル解析系の論文．情報学部出身の読者になるべく分かりやすいように丁寧に書いているように見受けられる．
画像で言えば，伝送エラーなどで行の一部分や下半分が吹き飛んでしまった時などに使える復元手法．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>きちんと読み手への導入は行われているものの，読み下すには，テンソル分解程度の数学の知識が必要．ついでに，カオスのような時系列システムも知っているとわかりやすい（図中の説明での事例がそれ）．
まとめ人にとっては数学の復習になったので，ぜひ論文を読んでみていただきたい．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0672.pdf">論文</a></li></ul></div></div><div class="slide_index">[#196]</div><div class="timestamp">2018.6.18 11:01:34</div></div></section><section id="ID_Sim2Real_Viewpoint_Invariant_Visual_Servoing_by_Recurrent_Control"><div class="paper-abstract"><div class="title">Sim2Real Viewpoint Invariant Visual Servoing by Recurrent Control</div><div class="info"><div class="authors">Fereshteh Sadeghi et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>ロボットアームを用いたビジュアルサーボについての研究. DNN を用いた視点に依存しないビジュアルサーボの能力を学習する Recurrent Convolutional Neural Network Controller を提案. 様々な視点, 光源環境, 物体の種類や位置に置けるタスクをシミュレーション上で学習することで, 未知の視点において自動でキャリブレーションを行うことが可能. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Sim2Real-Viewpoint-Invariant-Visual-Servoing-by-Recurrent-Control.png" alt="fukuhara-Sim2Real-Viewpoint-Invariant-Visual-Servoing-by-Recurrent-Control.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>コントローラーは目的物体のクエリ画像, 現在の観測画像, 1つ前の行動, 現在の内部状態から次の行動と内部状態を決定する</li><li>LSTM を用いてネットワークが過去の行動の結果を参照できるようにすることで Jacobian (action と motion との関係) についての事前知識無しでの学習を可能とした</li><li>ロス関数にはとった行動によって目的物体との距離がどのように変化したかと, 長期的な行動の価値を学習するための Q-関数 (行動状態価値関数) を用いる</li><li>少数のアノテーション付きシークエンスがあれば, シミュレーション上で学習結果を実際のロボットへ転移することが可能（追加で学習が必要なのは画像特徴の部分のみのため）</li><li>実際のロボットに学習結果を転移して行った評価実験では, 物体へロボットアームを到達させるタスクにおいて, 単一物体の場合は 94.4%, 二つの場合は 70.8% を達成した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.05472" target="blank">[論文] Sim2Real Viewpoint Invariant Visual Servoing by Recurrent Control</a></li><li><a href="https://www.youtube.com/watch?v=oLgM2Bnb7fo" target="blank">[動画] YouTube</a></li></ul></div></div><div class="slide_index">[#197]</div><div class="timestamp">2018.6.18 4:18:55</div></div></section><section id="ID_Multi-Oriented_Scene_Text_Detection_via_Corner_Localization_and_Region_Segmentation"><div class="paper-abstract"><div class="title">Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation</div><div class="info"><div class="authors">Pengyuan Lyu, Cong Yao, Wenhao Wu, Shuicheng Yan and Xiang Bai</div><div class="conference">CVPR2018</div><div class="paper_id">982</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>コーナー検出とセグメンテーションを用いた高速かつ高精度なテキスト検出手法．テキスト検出時，ボックスのコーナー点を局所化し，テキスト領域を相対位置でセグメンテーションする．画像を入力すると，DSSDベースのNWで特徴抽出をし，コーナー点検出とコーナー位置に基づくセグメンテーションを出力する．コーナー点はサンプリングおよびグループ化され複数の候補ボックスとなる．セグメンテーション結果とあわせてスコア付けしてNMSする．長いテキストを自然に検出でき，複雑な後処理をする必要もない．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180617_Multi-Oriented_Scene_Text_Detection1.jpg" alt="20180617_Multi-Oriented_Scene_Text_Detection1.jpg"><img src="slides/figs/20180617_Multi-Oriented_Scene_Text_Detection2.jpg" alt="20180617_Multi-Oriented_Scene_Text_Detection2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Deepベースのテキスト検出は，テキストを物体の一種として扱いb-boxの回帰を行うか，テキスト部分を直接抽出する手法である．前者はアスペクト比によっては検出できず，後者は複雑な後処理を必要とする．本手法はその2つを組み合わせて，両者の欠点を補う．SynthText，ICDAR2015，2013，MSRA-TD500，MLTおよびCOCO-Textのデータセットで評価して，ほとんどがSOTAを達成した．とくに，ICDAR2015では84.3%（F-measure），MSRA-TD500では81.5%を達成した．10.4FPSで動作する．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>非常にシンプルながらも高精度なテキスト検出． DSSDのデコーダ部分の特徴マップからセグメンテーションを行う最近よくある手法をテキスト検出に応用している．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1688.pdf">論文</a></li><li><a href="https://arxiv.org/abs/1802.08948">arXiv</a></li></ul></div></div><div class="slide_index">[#198]</div><div class="timestamp">2018.6.17 23:16:27</div></div></section><section id="ID_Low-Latency_Video_Semantic_Segmentation"><div class="paper-abstract"><div class="title">Low-Latency Video Semantic Segmentation</div><div class="info"><div class="authors">Yule Li, et al.</div><div class="paper_id">1804.00389</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>動画によるセマンティックセグメンテーションにおいて、精度を向上させつつ、処理速度を上げる手法の提案。2つのコンポーネントを組み込んだフレームワークで構成している。1つ目は、時間変化に伴って空間的な畳み込み処理を変化させ、特徴を適応させる特徴伝播モジュール。2つ目は、精度予測に基づいて、計算を動的に割り当てるスケジューラ。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180617LLVSS.jpg" alt="20180617LLVSS.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>動画のセマンティックセグメンテーションには、高スループットやコスト、低遅延などの問題があり、 自律運転などにおいて重要となる。時間的変化に適応させた処理によって精度向上、処理速度向上を図る。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>CityscapesとCamVidにおいて、最新の手法と競合する精度で、遅延を360msから119msに抑えられる結果に。</p><ul><li><a href="https://arxiv.org/pdf/1804.00389.pdf">論文</a></li></ul></div></div><div class="slide_index">[#199]</div></div></section><section id="ID_VirtualHome_Simulating_Household_Activities_via_Programs"><div class="paper-abstract"><div class="title">VirtualHome: Simulating Household Activities via Programs</div><div class="info"><div class="authors">Xavier Puig et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>家の中の環境をシミミュレーションするための仮想環境 VirtualHome を作成した. また, 家の中で典型的に起こる様々な行動を自然言語とプログラムの形式で表現し, それらを仮想環境上でシミミュレーションした動画を組みにした VirtualHome Activity Dataset を公開した. 加えて, LSTM を用いて動画やテキストからプログラム形式の表現を生成する手法を提案した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-VirtualHome-Simulating-Household-Activities-via-Programs.png" alt="fukuhara-VirtualHome-Simulating-Household-Activities-via-Programs.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>VirtualHome には様々な種類の間取りや物体（平均357個）があり, Agent も複数の種類が用意されている</li><li>dataset では家の中で行われる様々な行動に対して, 名前と自然言語形式での行動の説明と行動をプログラムの形式が与えられている</li><li>VirtualHome 上でプログラムをシミュレーションすることで作成された動画には, Agent の姿勢やフロー, 物体のクラスなど様々な情報が与えられている</li><li>LSTM を用いた encoder-decoder 型のネットワークに強化学習を適用し, 動画やテキストからプログラム形式の表現を生成する手法を提案</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Puig_VirtualHome_Simulating_Household_CVPR_2018_paper.pdf" target="blank">[論文] VirtualHome: Simulating Household Activities via Programs</a></li><li><a href="http://www.virtual-home.org/" target="blank">[Project page] VirtualHome: Simulating Household Activities via Programs</a></li></ul></div></div><div class="slide_index">[#200]</div><div class="timestamp">2018.6.16 16:40:55</div></div></section><section id="ID_Visual_Question_Generation_as_Dual_Task_of_Visual_Question_Answering"><div class="paper-abstract"><div class="title">Visual Question Generation as Dual Task of Visual Question Answering</div><div class="info"><div class="authors">Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang and Xiaogang Wang</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像に関する質問に答えるVisual Question Answering(VQA)と与えられた答えになる質問を作るVisual Question Generation(VQG)を同時に扱うInvertible Question Answering Network(iQAN)を提案した。質問が与えられている場合は答えを、答えが与えられている場合は質問を推定することで学習をする。
その際、2つのタスクを独立した問題ではなく逆問題であると考え、質問と答え及びそれぞれを表現する特徴量間の変換に使用する重みを共有する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Visual_Question_Generation_as_Dual_Task_of_Visual_Question_Answering.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>VQAに関しては、従来手法と比べて精度を向上することが可能となった。また、VQGによって生成した質問と答えのペアをVQAの学習に使用すると精度が向上することが分かり、VQGによってデータ数を増やすことが可能であると結論付けた。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://cvboy.com/publication/cvpr2018_iqan/">著者ホームページ</a></li></ul></div></div><div class="slide_index">[#201]</div><div class="timestamp">2018.6.5 00:51:56</div></div></section><section id="ID_Teaching_Categories_to_Human_Learners_with_Visual_Explanations"><div class="paper-abstract"><div class="title">Teaching Categories to Human Learners with Visual Explanations</div><div class="info"><div class="authors">Oisin Mac Aodha, Shihan Su, Yuxin Chen, Pietro Perona and Yisong Yue</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像に写っているもののカテゴリをコンピュータが人間に教えるためのシステムEXPLAINを提案。カテゴリを分類する上でどこに注目すればいいのか(例：蝶の種類を見分けるにはどこに注目すれば良いか）を提示することで人間がカテゴリを学習することを支援する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Teaching_Categories_to_Human_Learners_with_Visual_Explanations.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来の手法ではカテゴリを表すラベルを提示するのみであったが、重要領域を提示することでより効率的に人間が学習することを可能とした。ユーザースタディにより人に学習してもらった内容に関するテストをしたところ、EXPLAINの方が短い時間で高い正答率を出すという結果を得られた。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.06924">link1</a></li></ul></div></div><div class="slide_index">[#202]</div><div class="timestamp">2018.6.10 23:58:13</div></div></section><section id="ID_Face_Aging_with_Identity-Preserved_Conditional_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">Face Aging with Identity-Preserved Conditional Generative Adversarial Networks</div><div class="info"><div class="authors">Zongwei Wang, Xu Tang, Weixin Luo and Shenghua Gao</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>人間の年齢変化顔を合成するIdentity-Preserved Conditional Generative Adversarial Networks (IPCGANs)を提案。合成画像が満たすべき特徴を、(1)目的の年齢に近づいている(2)変化前の人物と同一人物か(3)リアルな画像かの3つとした。
(1)(2)については、Generatorによって生成した画像を年齢推定及び同一人物性を評価するネットワークによって評価する。
(3)はDiscriminatorにリアルかどうかを判定させることで最適化を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Face_Aging_with_Identity-Preserved_Conditional_Generative_Adversarial_Networks.PNG" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ユーザースタディにより、Image Quality, Age Classification, Face Verificationの3つの観点を評価し、DNNベースの手法と比較してFace VerificationとImage Qualityの2つの観点で高い評価を得た。VGG-faceによりinception scoreを求め、比較対象の手法より高いスコアを得た。
また、計算時間についても劇的に良化した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Face_Aging_With_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#203]</div><div class="timestamp">2018.6.15 18:52:34</div></div></section><section id="ID_Emotional_Attention_A_Study_of_Image_Sentiment_and_Visual_Attention"><div class="paper-abstract"><div class="title">Emotional Attention: A Study of Image Sentiment and Visual Attention</div><div class="info"><div class="authors">Shaojing Fan, Zhiqi Shen, Ming Jiang, Bryan L. Koenig, Juan Xu, Mohan S. Kankanhalli and Qi Zhao</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像に潜んでいる感情と注目を集める領域の関連を調査した。アイトラッキングのデータと、画像中に写っている感情に関連する物体(笑顔など)をアノテーションしたEMOtional attention dataset(EMOd)を構築した。
また、画像中の注目領域を抽出するDNNモデルであるCASNetを提案した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Emotional_Attention_A_Study_of_Image_Sentiment_and_Visual_Attention.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>EMOdを用いて分析した結果、感情に関連する物体の方が人々の視線を集めることが判明した。その中でも、人間が関連する（笑顔など）場合がより視線を集めることが分かった。
従来のSaliencyを求める手法よりもCASNetの方が多くの指標で高いスコアを獲得した。
また、感情に関連する物体の方がより注目を集めるという結果を出力したことからEMOdの分析結果を反映していることを確認した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://nus-sesame.top/emotionalattention/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#204]</div><div class="timestamp">2018.6.5 12:28:58</div></div></section><section id="ID_Categorizing_Concepts_with_Basic_Level_for_Vision-to-Language"><div class="paper-abstract"><div class="title">Categorizing Concepts with Basic Level for Vision-to-Language</div><div class="info"><div class="authors">Hanzhang Wang, Hanli Wang and Kaisheng Xu</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Vision and Languageのタスクに、Cognition分野で提唱されているbasic levelという概念を基にしたBasic Concept(BaC)を導入した。basic levelとは人間が幼少期に行う抽象化であり、本研究では物体のクラスを類似したもの同士を1つにまとめる。
始めに、MSCOCOのキャプションとImageNetのクラスをマッチングすることで、Salient Concept(SaC)というBaCに候補を決定する。
続いて、物体のクラス分類におけるConfusion Matrixを求め、混同されるクラス同士を1つにまとめることでBaCを決定する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Categorizing_Concepts_with_Basic_Level_for_Vision-to-Language.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Vision and Languageのタスクとして、Image CaptioningとVQAによって検証を行った。Image Captioningについては、ベースラインと比較してほとんどの指標において精度が向上し、向上しなかった指標についてもベースラインと大差ない数値を記録した。
VQAについては、ObjectとLocationについて精度の向上を確認した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Categorizing_Concepts_With_CVPR_2018_paper.html">論文</a></li></ul></div></div><div class="slide_index">[#205]</div><div class="timestamp">2018.6.15 16:40:44</div></div></section><section id="ID_Multi-Level_Fusion_Based_3D_Object_Detection_From_Monocular_Images"><div class="paper-abstract"><div class="title">Multi-Level Fusion Based 3D Object Detection From Monocular Images</div><div class="info"><div class="authors">Bin Xu et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>一枚のRGB画像から３次元物体認識を行う研究. region-based な２次元の物体検出器を３次元に拡張する一般的なフレームワークを提案し, end-to-end のネットワークで２次元と３次元の物体位置と物体のクラスを同時に推定することが可能. KITTI dataset を用いた評価実験では state-of-the-art の結果を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Multi-Level-Fusion-Based-3D-Object-Detection-From-Monocular-Images.png" alt="fukuhara-Multi-Level-Fusion-Based-3D-Object-Detection-From-Monocular-Images.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>end-to-end のネットワークで単一のRGB画像から物体のクラスと２次元, ３次元の物体位置, ３次元の物体の方向などを同時に推定</li><li>RGB画像に MonoDepth を用いて推定した Depth 画像を連結したものを CNN に入力し, Faster-RCNN と同様の方法で Region Proposal を生成</li><li>また, Depth 画像から Point Cloud (XYZ Map)を推定</li><li>上記の２つを連結したものを全結合層に通して, 物体位置と物体のクラスの推定を行う</li><li>KITTI dataset を用いた評価実験では　Mono3D, 3DOP, Deep3DBox などと比較して優位な結果を達成した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2380.pdf" target="blank">[論文] Multi-Level Fusion Based 3D Object Detection From Monocular Images</a></li></ul></div></div><div class="slide_index">[#206]</div><div class="timestamp">2018.6.15 1:01:55</div></div></section><section id="ID_Conditional_Probability_Models_for_Deep_Image_Compression"><div class="paper-abstract"><div class="title">Conditional Probability Models for Deep Image Compression</div><div class="info"><div class="authors">Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, Luc Van Gool</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像復元の問題は復元エラー（distortion）とエントロピー（rate）とのトレードオフであるが、本論文ではこのトレードオフをできる限り解消し、画像圧縮を行うAutoEncoderを提案する。著者らはコンテキストモデルから直接的に潜在表現のエントロピーを復元するモデルを考案して同問題に取り組んだ。AutoEncoderには条件付き確率モデルを学習した3D-CNNを適用。実験ではSSIMを用いて従来の畳み込みによるAutoEncoderモデルよりも良好な精度を実現した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180614DeepImageCompression.png" alt="180614DeepImageCompression"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>3D-CNNにより条件付き学率モデルを学習したAutoEncoderモデルを考案したことが新規性であり、JPEG(2000)などよりも良い圧縮法であることを示し、Rippel&Bourdevらのモデルと同等レベルの精度を達成した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>画像圧縮、超解像の違いがいまいちよくわからなくなってきた。評価方法の違い？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Mentzer_Conditional_Probability_Models_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://relational.github.io/">著者</a></li></ul></div></div><div class="slide_index">[#207]</div><div class="timestamp">2018.6.14 08:55:34</div></div></section><section id="ID_Improved_Lossy_Image_Compression_With_Priming_and_Spatially_Adaptive_Bit_Rates_for_Recurrent_Networks"><div class="paper-abstract"><div class="title">Improved Lossy Image Compression With Priming and Spatially Adaptive Bit Rates for Recurrent Networks</div><div class="info"><div class="authors">Nick Johnston, Damien Vincent, David Minnen, Michele Covell, Saurabh Singh, Troy Chinen, Sung Jin Hwang, Joel Shor, George Toderici</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Recurrent/Convolutional Neural Networks（RNN/CNN）を用いた非可逆画像圧縮の手法を提案し、BPG(4:2:0), WebP, JPEG2000, JPEGよりも性能のよいものを提案した。3つの改善、(1)ニューラルネットにより空間的分散を効果的に捉えて情報量の劣化を防ぐ、(2)エントロピーコーディングの上に空間適応的ビット配置アルゴリズムを適用して効率的な画像圧縮とする、(3)SSIMによりピクセルごとの損失を計算して最適化することで圧縮数値を改善する、を加えて圧縮方法を提案。KodakやTecnickのカメラを用いてコーデックの評価を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180614SpatiallyAdaptiveBitRates.png" alt="180614SpatiallyAdaptiveBitRates"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来の圧縮方法であるBPG(4:2:0), WebP, JPEG2000, JPEGなどよりも効率の良い圧縮方法を提案した。また、手法的にもCNN/RNNを応用し、さらに後処理として画質を改善するSpatially Adaptive Bit Rate (SABR)を提案したことが評価された。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>（数十年前からある問題という意味で）過去の問題と現在の手法が合わさって新規性を出している論文。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Johnston_Improved_Lossy_Image_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#208]</div><div class="timestamp">2018.6.14 08:27:59</div></div></section><section id="ID_Deep_Density_Clustering_of_Unconstrained_Faces"><div class="paper-abstract"><div class="title">Deep Density Clustering of Unconstrained Faces</div><div class="info"><div class="authors">Wei-An Lin, Jun-Cheng Chen, Carlos D. Castillo, Rama Chellappa</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>unconstrainedな顔に対してクラスタリングを行うDeep Density Clustering(DDC)を提案。顔画像をDNNによって単位超級面空間に射影する。続いて、各サンプル2点の類似度を測定する際に、
その2点の近傍に位置するサンプルを考慮することでクラスタの密度を推定することが可能となるため、これに基づいてクラスタリングを行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Density_Clustering_of_Unconstrained_Faces.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>YTF, LFW, IJB-Bデータセットを使用して評価。それぞれのデータセットには同一人物の画像が複数枚もつ。</li><li>評価指標はBCubed precision、Bcubed F-measure、NMIで評価。 </li><li>提案手法と同等の精度を持つ既存手法のJULE、DEPICTはクラスタ数を指定する必要があるが、提案手法ではクラスタ数を指定する必要がない。</li><li>クラスタリングの際の閾値の変更に対して、既存手法に比べてクラスタ数の変動が小さい。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3208.pdf">論文</a></li><li> <a href=" http://openaccess.thecvf.com/content_cvpr_2018/Supplemental/3208-supp.pdf">Supplementary material</a></li></ul></div></div><div class="slide_index">[#209]</div></div></section><section id="ID_Pose-Guided_Photorealistic_Face_Rotation"><div class="paper-abstract"><div class="title">Pose-Guided Photorealistic Face Rotation</div><div class="info"><div class="authors">Yibo Hu, Xiang Wu, Bing Yu, Ran He, Zhenan Sun</div><div class="conference">CVPR 2018 Spotlight</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>入力顔画像に対して任意の画像を生成するネットワークを提案。顔向きのコンディションとしてランドマークのヒートマップを与え、U-Netによって画像を生成し、2つのdiscriminatorを用いることで画像を生成。
1つ目のdiscriminatorは入力画像をコンディションとして生成画像or正解画像を識別し、
2つ目のdiscriminatorはランドマークのヒートマップをコンディションとして生成画像or正解画像を識別する。
また人物IDを保存するためにLight CNNによる特徴量によるロスをとる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Pose-Guided_Photorealistic_Face_Rotation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ランドマークのヒートマップ、2つのdiscriminator、IDを保存するロスを用いて入力顔画像を任意の向きに回転させた画像を生成。</li><li>337IDそれぞれに対して20の照明環境と15種類の顔向きをもつMulti-PIEで検証。</li><li>トレーニングには使用していないLFWで画像を生成したところ、既存手法による画像よりも見た目の良い画像が得られた。</li><li>face verification、face recognitionにおいてSoTAを達成。</li><li>ablation studyの結果、IDのロスがface recognitionに最も影響が高いことを確認。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>既存手法のように顔向きの角度を使うのではなくヒートマップを与えることでU-netの学習がしやすい、という上手い方法。</li><li>IDのロスに使用する特徴量が最後のFC層に加えてプーリング層からも取得されておりIDについてはMS-Celeb-1Mでプリトレインした後Multi-PIEへとファインチューニングしているなど、かなり微調整を感じる論文。</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1097.pdf">論文</a></li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/Supplemental/1097-supp.pdf">Supplementary material</a></li></ul></div></div><div class="slide_index">[#210]</div></div></section><section id="ID_Unsupervised_Training_for_3D_Morphable_Model_Regression"><div class="paper-abstract"><div class="title">Unsupervised Training for 3D Morphable Model Regression</div><div class="info"><div class="authors">Kyle Genova, Forrester Cole, Aaron Maschinot, Aaron Sarna, Daniel Vlasic, William T. Freeman</div><div class="conference">CVPR 2018 Spotlight</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>それぞれ単独の実画像データセットと3D Morphable Model(3DMM)データセットを使用し、画像から3DMMを生成する手法を提案。トレーニングには実画像データセットVGG-Face、3DMMデータセットBasel Face 3DMMを使用。
IDが保たれることを念頭にネットワークを構築。Batch Distribution Lossでは、
Basel Face 3DMMのパラメタ分布が平均０、標準偏差1のガウス分布であるため、
実画像によって生成される3DMMのシェイプ、テクスチャパラメタがどちらも平均0、標準偏差1となるようにロスをとる。
Loopback Lossは画像/生成された3DMMのdecoderによる特徴量の差分を取り、よりリアルな3DMMかつ、
より現実的な3DMMパラメタを得ることを目的としている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Unsupervised_Training_for_3D_Morphable_Model_Regression.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>画像、3DMMの対応がないデータセットを用いて、教師なしで画像から3DMMを生成する手法を提案。</li><li>Batch Distribution Loss、Loopback Loss、Multi-view Identity Lossを学習することで教師なしであることを緩和している。</li><li>MICC Florence 3D Faceデータセットで検証し、Mean error、Faceクラスタリング、Earth mover’s distanceによる実画像と生成3DMMの顔類似度のそれぞれにおいてSoTA。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>Basel Face 3DMMのパラメタ分布が平均０、標準偏差1のガウス分布という仮定はどこから来ている？</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1349.pdf">論文</a></li></ul></div></div><div class="slide_index">[#211]</div></div></section><section id="ID_Aligning_Infinite-Dimensional_Covariance_Matrices_in_Reproducing_Kernel_Hilbert_Spaces_for_Domain_Adaptation"><div class="paper-abstract"><div class="title">Aligning Infinite-Dimensional Covariance Matrices in Reproducing Kernel Hilbert Spaces for Domain Adaptation</div><div class="info"><div class="authors">Zhen Zhang, Mianzhi Wang, Yan Huang, Arye Nehorai</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ソースドメイン(SD)とターゲットドメイン(TD)のそれぞれのreproducing kernel Hilbert space(RKHS)における共分散を最適化することでdomain adaptation(DA)を行う手法。
既存のカーネルベースのDAはSDとTDのRKHS上の統計的分布の類似度に大きく依存することに着目。
共分散を最適化する方法としてkernel whitening-coloring map(KWC)とkernel optimal 
transport map(KOT)があり、これをRKHS上で計算で可能なように式変形を行うことでDAを行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Aligning_Infinite-Dimensional_Covariance_Matrices_in_Reproducing_Kernel_Hilbert_Spaces_for_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>SDとTDのRKHS上の共分散を最適化することでDAを行う。</li><li>複数のDAのベンチマークデータセットにおいてKWC、KOTのいずれかがSoTAを達成。</li><li>SoTAと比較して実行時間が短く、KWCは4分の１、KOTは10分の1程度。</li><li>Out-of-Sampleによる推定においてもSoTAを達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>248パターンのDAを検証しており、本論文に載っていたのは34パターン</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3383.pdf">論文</a></li><li> <a href="http://openaccess.thecvf.com/content_cvpr_2018/Supplemental/3383-supp.pdf">Supplementary material</a></li></ul></div></div><div class="slide_index">[#212]</div></div></section><section id="ID_Cross-Dataset_Adaptation_for_Visual_Question_Answering"><div class="paper-abstract"><div class="title">Cross-Dataset Adaptation for Visual Question Answering</div><div class="info"><div class="authors">Wei-Lun Chao, Hexiang Hu, Fei Sha</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>VQAのデータセットにおけるバイアスを調査した上で、VQAにおけるdomain adaptation(DA)を提案。提案手法では選択肢の中から解答を選択するVQAを扱う。VQAデータセットは画像、質問、解答選択肢＝正解＋誤答の要素からなる。
それぞれの要素を組み合わせた入力を用いて、その入力がどのデータセットに所属しているのかを調査した結果、
画像はほぼ無相関であることがわかり、質問と解答によってデータセット間にバイアスが生じていることを確認。
この結果に基づき、以下のようにDAを提案。ターゲットドメイン(TD)に質問/解答選択肢のみがある場合、
ソースドメイン(SD)の質問/正解(誤答は任意性があるため使用しない)の特徴量が持つ分布とTDの質問のDNNによる
特徴量が持つ分布のJensen-shannon Divergence(JSD)が小さくなるように学習。TDが質問と正解(＋誤答)を持つ場合、
SDが持つ質問・正解の特徴量分布とTDの質問・正解のDNNによる特徴料が持つJSDが小さくなるように学習。
さらにSDで事前学習を行った質問-正解識別をTDでfine-tuningを行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Cross-Dataset_Adaptation_for_Visual_Question_Answering.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>事前実験より与える情報によって、入力データがどちらのデータセットに所属しているかの識別率の変化を確認。画像、質問、正解解答、解答群(正解+不正解)を与え、与える要素を増やすほど識別率が高くなった。
この結果から、データセットによってバイアスがあることを確認。</li><li>質問に対する正答率を複数のデータセットにおいて既存手法であるADDA、CORALと比較した結果SoTAを達成。TDが解答選択肢のみ、質問と正解を持つ場合において高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集 </h1><ul><li>TDの正解、誤答のみを使用し質問を使用せずにDAを行った方が高い状況がいくつも確認できる。これはつまり質問と解答の相関がすでにSDで学習できており、SDの質問がノイズになってしまっているとを示唆している。</li><li>VQAをDAしてみた、という実験的な論文であり比較している手法もDAのベンチマークの手法なので、まだまだ新規性を出すことができそう。</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/4249.pdf">論文</a></li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/Supplemental/4249-supp.pdf">Supplementary material</a></li></ul></div></div><div class="slide_index">[#213]</div></div></section><section id="ID_Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Monocular_Video_Using_3D_Geometric_Constraints"><div class="paper-abstract"><div class="title">Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints</div><div class="info"><div class="authors">Reza Mahjourian et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>教師なし学習で単眼の動画から Depth と Ego-Motion の推定を行う研究. 連続するフレーム間における 3D Geometry の一貫性を教師信号の代わりに利用して学習を行う. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuahra-Unsupervised-Learning-of-Depth-and-Ego-Motion-from-Monocular-Video-Using-3D-Geometric-Constraints.png" alt="fukuahra-Unsupervised-Learning-of-Depth-and-Ego-Motion-from-Monocular-Video-Using-3D-Geometric-Constraints.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>連続するフレーム間における 3D Geometry の一貫性を用いることで, 教師なし学習で単眼の動画から Depth と Ego-Motion の推定を行うことを可能とした</li><li>連続するフレームから推定された Point Cloud に対して Iterative Closest Point (ICP) を計算し, その Residual と Transform の大きさを 3D Loss として課す</li><li>3D Loss に加えて推定された Depth の滑らかさと, 推定結果を用いて復元した画像の誤差 (2種類) も Loss として課す</li><li>KITTI dataset と mobile phone カメラで撮影した動画を用いて行った評価実験では　Trajectory と Depth の両方において先行研究よりも優位な結果を達成した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.05522" target="blank">[論文] Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints</a></li></ul></div></div><div class="slide_index">[#214]</div><div class="timestamp">2018.5.28 18:59:55</div></div></section><section id="ID_A-Network-Architecture-for-Point-Cloud-Classification-via-Automatic-Depth-Images-Generation"><div class="paper-abstract"><div class="title">A Network Architecture for Point Cloud Classification via Automatic Depth Images Generation</div><div class="info"><div class="authors">RiccardoRoveri et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Point Cloud データのクラス分類についての研究. 順序不定の 3D Point Cloud データを 2D Depth 画像に変換し, ResNet でクラス分類を行う. 評価実験では PointNet より優位な結果となった.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-A-Network-Architecture-for-Point-Cloud-Classification-via-Automatic-Depth-Images-Generation.png" alt="fukuhara-A-Network-Architecture-for-Point-Cloud-Classification-via-Automatic-Depth-Images-Generation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Network は３つのモジュールで構成されており, joint training が可能</li><li>１つ目のモジュールは PointNet を用いて PointCloud から有用な view direction を推定する</li><li>２つ目のモジュールは Gausiaan Interporation （Roveri＋18 の拡張版）によって推定された view direction からの Depth 画像を生成する</li><li>３つ目のモジュールは ResNet50 を用いて Depth 画像から Image Based Classification を行う</li><li>ModelNet40 benchmark を用いて行った shape のクラス分類の評価実験では instance-based accuracy と class average accuracy の両方で PointNet よりも優位な結果となった</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3120.pdf" target="blank">[論文] A Network Architecture for Point Cloud Classification via Automatic Depth Images Generation</a></li><li>3D の問題を既によく研究されている 2D 画像のクラス分類へと帰着させることで, 既存の強力な手法を用いる戦略</li></ul></div></div><div class="slide_index">[#215]</div><div class="timestamp">2018.6.13 5:54:55</div></div></section><section id="ID_GraphBit_Bitwise_Interaction_Mining_via_Deep_Reinforcement_Learning"><div class="paper-abstract"><div class="title">GraphBit: Bitwise Interaction Mining via Deep Reinforcement Learning</div><div class="info"><div class="authors">Yueqi Duan et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Deep binary descriptor においてバイナリを生成する際に0と1の境界に位置する曖昧なビット (ambiguous bit) の問題に取り組んだ研究. 強化学習によって学習したビット間の implicit な関係性を付加することで曖昧性を緩和する GraphBit を提案. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-GraphBit-Bitwise-Interaction-Mining-via-Deep-Reinforcement-Learning.png" alt="fukuhara-GraphBit-Bitwise-Interaction-Mining-via-Deep-Reinforcement-Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Binary descriptor における曖昧なビット (ambiguous bit) の問題を緩和するためにビット間の関係性を付加した GraphBit を提案</li><li>CNNからの出力された正規化された特徴量（binary descriptor）に対して Grpah 構造を付加する</li><li>ビット間の相互関係をマイニングする過程をマルコフ過程として定式化し, 強化学習（Policy Gradient）で学習</li><li>State は現在の Graph の構造</li><li>Atction は GraphBit に新しいエッジを１つ追加するか, 既存のエッジを１つ削除</li><li>Reward は t ステップと t+1 ステップにおけるロス関数の減少度合いから計算</li><li>CIFAR-10, Brown, HPatches dataset を用いた評価実験では mean average precision (mAP) の評価尺度でそれぞれ平均 9.64%, 8.84%, 3.22% の精度の向上を達成した </li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper.pdf" target="blank">[論文] GraphBit: Bitwise Interaction Mining via Deep Reinforcement Learning</a></li></ul></div></div><div class="slide_index">[#216]</div><div class="timestamp">2018.6.13 2:43:55</div></div></section><section id="ID_Deep_Progressive_Reinforcement_Learning_for_Skeleton-based_Action_Recognition"><div class="paper-abstract"><div class="title">Deep Progressive Reinforcement Learning for Skeleton-based Action Recognition</div><div class="info"><div class="authors">Yansong Tan et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Skeleton-based action recognition の研究. 強化学習によって与えられた動画から最適な keyframe の組を選択する frame distillation network (FDNet) と graph-based convolution によって keyframe の skeleton 情報から行動認識を行う Graph-based CNN (GCNN) を提案. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Deep-Progressive-Reinforcement-Learning-for-Skeleton-based-Action-Recognition.png" alt="fukuhara-Deep-Progressive-Reinforcement-Learning-for-Skeleton-based-Action-Recognition.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>与えられた動画のシークエンスから最適な keyframe の組を選択する過程をマルコフ過程として定式化し, 強化学習 (policy gradient) を適用した</li><li>State として Skeleton 動画全体と現在選択されてる keyframe の組の情報を使用</li><li>Action は各 keyframe を1フレーム前後にずらすか, そのままかの３つ</li><li>Reward は学習済みの GCNN を用いて計算</li><li>また, keyframe から行動認識を行う際は gggraph-based convolution を用いることによって人間の関節の依存関係を考慮している</li><li>NTU, SYSU, UT dataset を用いて評価実験では state-of-the-art とほぼ同等か, 優位な結果を示した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1736.pdf" target="blank">[論文] Deep Progressive Reinforcement Learning for Skeleton-based Action Recognition</a></li></ul></div></div><div class="slide_index">[#217]</div><div class="timestamp">2018.6.12 13:53:55</div></div></section><section id="ID_Learning_Superpixels_with_Segmentation-Aware_Affinity_Loss"><div class="paper-abstract"><div class="title">Learning Superpixels with Segmentation-Aware Affinity Loss</div><div class="info"><div class="authors">Wei-Chih Tu, Ming-Yu Liu, Varun Jampani, Deqing Sun, Shao-Yi Chien, Ming-Hsuan Yang, Jan Kautz</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Hiroaki Aizawa</div><div class="item1"><div class="text"><h1>概要</h1><p>superpixel segmentationのためにピクセルの類似性(pixel affinities)を学習するdeep learningベースの手法を提案。pixel affinitiesが同一物体に属する2つの隣接画素の尤度を測る。これまで、groundtruthがないこと、superpixelsのインデックスが交換可能であること、superpixelsの手法は微分不可であることからdeep learningベースのsuperpixelアルゴリズムは試みられていなかった。論文では、segmentation誤差から類似性を学習するsegmentation-aware loss(SEAL)と、pixel affinitiesを出力するPixel Affinity Net(PAN)を提案し、superpixelsとdeep learningを統合する。既存の手法より物体境界を保持したままsuperpixelsを計算することが可能になった。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_Superpixels_with_Segmentation_Aware_Affinity_Loss.PNG" alt="Learning_Superpixels_with_Segmentation_Aware_Affinity_Loss.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>superpixels + deep learningが新しい。実験では単純なpretrained modelによる特徴量や、edge検出によるsuperpixelsとの統合はうまくいかないことを示している。手法に関しては、superpixelsを直接出力するのではなく、pixel affinitiesを計算、graph-basedのアルゴリズム(ERS)を経由し出力、そしてSEALを計算する。これにより、pixel affinitiesを出力するPANへ誤差を逆伝播することができる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>より効果的に細部の情報をsuperpixelsとして保持することができるため、semantic segmentationの改善や計算量の削減につながるだろう。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Tu_Learning_Superpixels_With_CVPR_2018_paper.pdf">paper</a></li><li><a href="https://sites.google.com/site/wctu1009/cvpr18_superpixel">proj_page</a></li></ul></div></div><div class="slide_index">[#218]</div><div class="timestamp">2018.6.12 12:14:08</div></div></section><section id="ID_Generating_Synthetic_X-ray_Images_of_a_Person_from_the_Surface_Geometry"><div class="paper-abstract"><div class="title">Generating Synthetic X-ray Images of a Person from the Surface Geometry</div><div class="info"><div class="authors">Brian Teixeira, Vivek Singh, Terrence Chen, Kai Ma, Birgi Tamersoy, Yifan Wu, Elena Balashova and Dorin Comaniciu</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>人間の三次元輪郭形状から，見えない体の内側を解析してしまおうという話．本論文では，X線画像を生成する．
さらに，X線画像はパラメタライズしておくことで，体のキーポイントの調節によるマニピュレーションも可能．</p><p>構造的には，2つのネットワークからなる．(1)部分画像といくつかのパラメータから，画像全体を生成するように学習，
(2)全体画像が得られるような(1)のパラメータの推定．
これら2つのネットワークを，一貫性が出てくるように反復的に学習させる．</p><p>生成した画像を使ってみて，画像補間に使ってみた．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Generating_Synthetic_X-ray_Images_of_a_Person_from_the_Surface_Geometry.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>体表面を計測しておくなどして，体表面形状のデータがあれば，X線画像をある程度任意に生成できる．逆に，体表面形状をいじることでそれに対応したX線画像も作れる．
学習データとして活用することができる可能性がある．</p><p>構造はGAN風だが，いい感じに変形している感じがウケているかもしれない．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>この時点での一番の貢献は，それっぽいX線画像が自動生成できる事だろう．<a href="http://smpl.is.tue.mpg.de/">SMPL</a>と組み合わせていろいろやることを想定しているだろうか．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0500.pdf">論文</a></li></ul></div></div><div class="slide_index">[#219]</div><div class="timestamp">2018.6.12 10:10:47</div></div></section><section id="ID_Fully_Convolutional_Adaptation_Networks_for_Semantic_Segmentation"><div class="paper-abstract"><div class="title">Fully Convolutional Adaptation Networks for Semantic Segmentation</div><div class="info"><div class="authors">Yiheng Zhang, Zhaofan Qiu, Ting Yao, Dong Liu, Tao Mei</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>スタイル特徴量を用いて画像の見た目を変換するネットワークとドメイン間で不変な特徴量を得るネットワークを用いて、domain adaptationを行うことで教師無しでセマンティックセグメンテーションを行うFully Convolutional 
Adaptation Networks (FCAN)を提案。画像の見た目を変換するAppearance Adaptation Networks (AAN)では
ホワイトノイズから画像を生成し、ソースドメインの特徴量マップ、ターゲットドメインのもつ<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf">スタイル特徴量</a>が小さくなるように学習を行うことで、画像をもう一方のドメインの見た目になるように変換する。
ドメイン間で不変な特徴量を得るRepresentation Adaptation Networks (RAN)ではsemantic classificationと、
それぞれのドメインにから得られた特徴量マップに対するadversarial lossと、
ASPPによって得られた特徴量マップに対してピクセルごとにadversarial lossを適用。
ドメインとして実画像とゲーム画像で検証している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Fully_Convolutional_Adaptation_Networks_for_Semantic_Segmentation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>style transferと同様の考え方でドメイン間の画像変換を行いsemantic classification、特徴量マップ、dilated convolutional layerから得られた特徴量マップに対する各ピクセルに対してadversarial lossをとることで教師無しでセマンティックセグメンテーションを行う。</li><li>GTA5とCityscapesを用いて、セマンティックセグメンテーションの精度をstate-of-the-artと比較した結果、19クラスのうち17クラスで最も高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1761.pdf">論文</a></li></ul></div></div><div class="slide_index">[#220]</div></div></section><section id="ID_Re-weighted_Adversarial_Adaptation_Network_for_Unsupervised_Domain_Adaptation"><div class="paper-abstract"><div class="title">Re-weighted Adversarial Adaptation Network for Unsupervised Domain Adaptation</div><div class="info"><div class="authors">Qingchao Chen, Yang Liu, Zhaowen Wang, Ian Wassell, Kevin Chetty</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>Unsupervised Domain Adaptationを行うため、ドメイン間の特徴量分布を一致させるoptimal transportベースのEM distanceを導入し、ターゲットドメイン(T)のラベル分布をソースドメイン(S)のラベル分布に対してラベルごとに重み付けした分布で表現する手法を提案。
domain discriminatorをOTベースのEM distanceをロス関数とすることでドメイン間の特徴量分布を近づける。
一方でベイズの定理より、ドメイン間のラベルの事前分布と特徴量の事後分布は比例関係にありラベルは低次元かつ離散的であるので
ドメイン間で類似度が高いと仮定し、Tにおけるラベルの事前分布をSのラベルの事前分布の重みを変更したもので表す。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Re-weighted_Adversarial_Adaptation_Network_for_Unsupervised_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ドメイン間で特徴量分布をOTベースのEM distanceの学習で、Tのラベル分布をSのラベル分布の重みを変更したもので表現することで、それぞれのdomain shiftを解消する手法を提案。</li><li>手書き文字データセットMNIST、USPS、SVHN、MINST-Mデータセット、19のラベルを持つ実画像、デプス画像のドメインを持つNYU-Dデータセットで検証。state-of-the-artと比較した結果、多くの状況で最も高い精度を達成。</li><li>Sのラベル分布の重みの変更による有効性、ラベルごとの特徴量が分離できているかどうかも議論している。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1224.pdf">論文</a></li></ul></div></div><div class="slide_index">[#221]</div></div></section><section id="ID_Unsupervised_Deep_Generative_Adversarial_Hashing_Network"><div class="paper-abstract"><div class="title">Unsupervised Deep Generative Adversarial Hashing Network</div><div class="info"><div class="authors">Kamran Ghasedi Dizaji, Feng Zheng, Najmeh Sadoughi, Yanhua Yang, Cheng Deng, Heng Huang</div><div class="conference">CVPR 2018 Spotlight</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>教師無しで画像をバイナリに符号化するハッシュ関数であるHashGANを提案。ハッシュ関数が満たすべき条件は画像が変換されて同じハッシュ値を返すこと、異なる画像には異なるハッシュ値を与えることである。
既存の教師無しハッシュ関数は過学習のために精度がよくなかった。提案手法であるHashGANはgenerator、discriminator、
encoderからなる。学習はGAN loss、encoderによって生成されるハッシュ値のエントロピーが小さくなるように、
出現するハッシュ値が同じになるように、画像の変換によるハッシュ値が不変となるように、画像ごとのハッシュ値が固有となるように、
合成画像をエンコードした際のハッシュ値のL2ロス、実画像と合成画像を入力とした際のdiscriminatorの最後の層に対して
feature matchingを行う。またdiscriminatorはデータ固有の情報を識別し、encoderはデータ固有の情報を抽出しようとするため、
両者の目的が一致しているのでパラメタを共有して学習を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Unsupervised_Deep_Generative_Adversarial_Hashing_Network.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>GAN、discriminatorとパラメタを共有しているencoder、ハッシュ関数が満たすべきロス関数を導入したHashGANを提案。</li><li>image retrieval、image clusteringで手法の優位性を検討。image retrievalでは既存のunsupervised hash functionとの比較を行い、最も高い精度を達成。image clusteringではstate-of-the-artと同等の精度を達成。</li><li>ablation testにより、特にadversarial loss, feture matching, L2ロス、画像変換によるハッシュの不変性の考慮の影響が大きいことがわかった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>教師無し学習でもタスク特化の手法であり、ハッシュ関数の性質をよく考察した上でモデルを設計している。</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0909.pdf">論文</a></li></ul></div></div><div class="slide_index">[#222]</div></div></section><section id="ID_Supervision-by-Registration_An_Unsupervised_Approach_to_Improve_the_Precision_of_Facial_Landmark_Detectors"><div class="paper-abstract"><div class="title">Supervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors</div><div class="info"><div class="authors">Xuanyi Dong, Shoou-I Yu, Xinshuo Weng, Shih-En Wei, Yi Yang, Yaser Sheikh,</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ランドマークのGT有り顔画像とラベルなし顔動画を用いて、現在フレームに対して直接推定されたランドマークと、トラッキングによって前フレームから推定されたランドマークの位置の誤差を学習することで顔画像に対してランドマークを推定する手法を提案。
人間によるランドマークのアノテーションは正確でないため、この誤差が学習や推定精度に影響を与えてしまう。
これに対して本論文ではランドマークの推定器に最適化によって計算されるオプティカルフローを教師情報として与える
Supervision by Registration(SBR)を提案。ランドマーク位置を推定するCNNに対して、
Lukas-Kanade法によるトラッキング結果とランドマークの推定位置が同じになるように学習を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Supervision-by-Registration_An_Unsupervised_Approach_to_Improve_the_Precision_of_Facial_Landmark_Detectors.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>人間のアノテーションよりも、より正確であるオプティカルフローを教師情報として使用することで顔画像に対するランドマークの推定手法を提案。</li><li>300-W、AFLWにおいてランドマーク推定手法であるCPMのアルゴリズムをSBRで学習させると、SBRを使用しない場合よりも精度が向上。</li><li>動画に対するランドマーク推定はstate-of-the-artに及ばなかった。ターゲットとなる人物をデータセットに含んでおくPersonalized Adaptation Modeling(PAM)を行うことで、state-of-the-artと同等の精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>画像のランドマークを推定するために動画から得られるオプティカルフローを使用する、という発想の飛躍が面白い！最適化による正確な教師情報とCNNによる合わせ技。</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0191.pdf">論文</a></li></ul></div></div><div class="slide_index">[#223]</div></div></section><section id="ID_Environment_Upgrade_Reinforcement_Learning_for_Non-differentiable_Multi-stage_Pipelines"><div class="paper-abstract"><div class="title">Environment Upgrade Reinforcement Learning for Non-differentiable Multi-stage Pipelines</div><div class="info"><div class="authors">Shuqin Xie et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>微分不可能な multi-stage pipline において joint optimization を可能にする environment upgrade reinforcement learning (EU-RL) を提案. ２段階の Instance segmentation と pose estimation のタスクで評価実験を行い, どちらも優位な結果を示した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Environment-Upgrade-Reinforcement-Learning-for-Non-differentiable-Multi-stage-Pipelines.png" alt="fukuhara-Environment-Upgrade-Reinforcement-Learning-for-Non-differentiable-Multi-stage-Pipelines.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>微分不可能な multi-stage pipline の学習において問題であった上流への feedback が出来ないという点と end-to-end な最適化が出来ない点に取り組んだ研究</li><li>強化学習の agent が下流の出力を受けて上流の出力に変更を与える, environment upgrade reinforcement learning (EU-RL) を提案</li><li>強化学習の手法として actor-critic を Temporal Difference　(TD) learning で学習</li><li>State として１段階目（例えば物体認識）からの出力と２段階目からの出力（例えば semantic segmentation）を使用</li><li>Action として１段階目からの出力結果を変更する操作の集合を使用（物体認識ならBounding Boxの位置の変更やスケールなど）</li><li>Reward は２段目の出力の精度の向上度合いによって計算</li><li>Instance segmentation と pose estimation のタスクで評価実験を行い, どちらも優位な結果を示した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/1643.html" target="blank">[論文] Environment Upgrade Reinforcement Learning for Non-differentiable Multi-stage Pipelines</a></li><li>強化学習の応用先としても, アイデアとしても面白い. 今回の論文では２段階の pipeline についてのみ議論が行われていたが, 今後は３段以上の pipeline でも同様の議論が行われていく？</li></ul></div></div><div class="slide_index">[#224]</div><div class="timestamp">2018.5.11 23:32:55</div></div></section><section id="ID_Deep_Reinforcement_Learning_of_Region_Proposal_Networks_for_Object_Detection"><div class="paper-abstract"><div class="title">Deep Reinforcement Learning of Region Proposal Networks for Object Detection</div><div class="info"><div class="authors">Aleksis Pirinen and Cristian Sminchisescu</div><div class="conference">CVPR2018</div><div class="paper_id">872</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Region proposal network（RPN）と深層強化学習（DRL）を組み合わせたdrl-RPNを提案する．通常のRPNがRoIを貪欲に選択するのに対し，DRLで学習されたsequential attention mechanismを用いて選択することで，最終検出タスクに最適化される．また，時間経過とともにクラス固有の特徴を蓄積し，分類スコアに良い影響を与えて検出精度が高めることを示す．また，学習をいつ停止するか自動的に判断する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180611_drlRPN.jpg" alt="20180611_drlRPN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>RPNにDRLを導入して，attentionに即したRoIを選択できるようにした．VOC2007を用いた評価では，通常のRPNがmAP74.2%なのに対し，drl-RPNは76.4%を達成した．MSCOCOでも各指標・各セットで数%の精度向上が見られた．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>またまた高精度なRoIを検出するタイプの手法．ついにRLまで使うことになった．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Pirinen_Deep_Reinforcement_Learning_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#225]</div><div class="timestamp">2018.6.11 23:18:20</div></div></section><section id="ID_A_Closer_Look_at_Spatiotemporal_Convolutions_for_Action_Recognition"><div class="paper-abstract"><div class="title">A Closer Look at Spatiotemporal Convolutions for Action Recognition</div><div class="info"><div class="authors">Du Tran et al.</div><div class="conference">CVPR2018</div><div class="paper_id">1711.11248</div></div><div class="slide_editor">Takumu Ikeya</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>動画解析のための時空間畳み込みの各手法が行動解析に及ぼす影響を調査した．</li><li>Residual learningのフレームワークでは3D CNNsが2D CNNsよりも精度において優れていることを実験的に示した．</li><li>3D Convolution filterを空間と時間へ分割することで精度が向上することを示した．</li><li>新たな時空間畳み込みブロックの構造として”R(2+1)D”を提案した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/A_Closer_Look_at_Spatiotemporal_Convolutions_for_Action_Recognition.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>新規の畳み込みブロックとして時空間の畳み込みブロックを時間と空間に分割する"R(2+1)D"を提案した．</li><li>"R(2+1)D"はSports-1M，Kinetics,UCF101,HMDB51のデータセットでSOTAを達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1711.11248.pdf">論文</a></li></ul></div></div><div class="slide_index">[#226]</div><div class="timestamp">2018.6.11 19:39:34</div></div></section><section id="ID_GeoNet_Geometric_Neural_Network_for_Joint_Depth_and_Surface_Normal_Estimation"><div class="paper-abstract"><div class="title">GeoNet: Geometric Neural Network for Joint Depth and Surface Normal Estimation</div><div class="info"><div class="authors">Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasu andJiaya Jia</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>単眼の画像から深さ(depth)と表面の法線マップ(surface normal maps)を同時に予測する幾何ニューラルネットワーク(GeoNet)を提案．NYU v2 dataset、ではGeoNetが幾何学的に一貫した深度マップと法線マップを予測できることを確認．surface normal maps推定でSOTA、また既存のdepth推定方法と同等の精度を達成．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180609_GeoNet1.png" alt="1"><img src="slides/figs/20180609_GeoNet2.png" alt="2"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>GeoNetは2つのストリームのCNNの上に構築されており、depthとsurface normal maps間の幾何学的な関係を構築．これによってdepthとsurface normal mapsを効率的に予測するための基礎となるモデルを構築し，高い一貫性と一致精度を達成することが可能．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://xjqi.github.io/">著者</a></li><li><a href="http://www.cs.toronto.edu/~rjliao/papers/CVPR_2018_GeoNet.pdf">論文</a></li></ul></div></div><div class="slide_index">[#227]</div><div class="timestamp">2018.6.9 13:54:32</div></div></section><section id="ID_MiCT_Mixed_3D2D_Convolutional_Tube_for_Human_Action_Recognition"><div class="paper-abstract"><div class="title">MiCT: Mixed 3D/2D Convolutional Tube for Human Action Recognition</div><div class="info"><div class="authors">Yizhou Zhou, Xiaoyan Sun, Zheng-Jun Zha and Wenjun Zeng</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>2D CNNと3D CNNの畳み込みモジュールを統合した行動認識のためのネットワークMixed Convolutional Tube(MiCT)を提案．3つの有名なベンチマークデータセット(UCF101，Sport1M，HMDB-51)においてMiCT-Netが元の3D CNNのみの手法より著しく優れていることを確認．UCF101とHMDB51での行動認識でSOTAの手法と比較し、MiCT-Netは最高の性能を発揮．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180609_MiCTNet1.png" alt="1"><img src="slides/figs/20180609_MiCTNet2.png" alt="2"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>2D CNNにおける手法を十分にリスペクトし，3D Convと融合した新規のネットワークを構築</li><li>MiCT-Netによって時空間融合の各ラウンドにおける学習の複雑さを軽減しつつ、より深くより有益な特徴マップを生成可能</li><li>UCF101とHMDB51においてSOTA<img src="slides/figs/20180609_MiCTNet3.png" alt="3"></li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://www.microsoft.com/en-us/research/uploads/prod/2018/05/Zhou_MiCT_Mixed_3D2D_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#228]</div><div class="timestamp">2018.6.9 15:14:28</div></div></section><section id="ID_Jerk-Aware_Video_Acceleration_Magnification"><div class="paper-abstract"><div class="title">Jerk-Aware Video Acceleration Magnification</div><div class="info"><div class="authors">Shoichiro Takeda, Kazuki Okami, Dan Mikami, Megumi Isogai and Hideaki Kimata</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>高速で大きな動きに対して加速度法の出力を頑健にするための、ジャーク(振動，ぶれ)の新規利用方法について言及．微小な変化は時間的スケールでの高速な大きな動きよりも滑らかであるという観点・観測に基づき、高速で大きな動きの下でのみ微妙な変化を通過させるジャークフィルタを設計．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180609_jerkVAM1.png" alt="1"><img src="slides/figs/20180609_jerkVAM3.png" alt="3"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ジャークフィルタを加速度法に適用することで、最先端のものより優れた結果を確認．<img src="slides/figs/20180609_jerkVAM2.png" alt="2"></p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2767.pdf">link1</a></li></ul></div></div><div class="slide_index">[#229]</div><div class="timestamp">2018.6.9 17:37:53</div></div></section><section id="ID_Recurrent_Pixel_Embedding_for_Instance_Grouping"><div class="paper-abstract"><div class="title">Recurrent Pixel Embedding for Instance Grouping</div><div class="info"><div class="authors">Shu Kong, Charless Fowlkes</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Hiroaki Aizawa</div><div class="item1"><div class="text"><h1>概要</h1><p>Instance segmentationのような画素単位のグループ分け問題を行うEnd-to-Endで学習可能な枠組みを提案。同じグループの画素はcosine similarityが高くなるように、異なるグループはmargin以下の値になるように超球面上に回帰(Spherical Embedding Module)し、そこでRNNによるMean-shift clusteringを実行すること(Recurrent Grouping Module)で実現。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Recurrent_Pixel_Embedding_for_Instance_Grouping.PNG" alt="Recurrent_Pixel_Embedding_for_Instance_Grouping.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>既存のregion proposalやbboxによる組み合わせたinstance segmentationの手法とは大きく異なり新しい。またこれをRNNでMean-shift clusteringを表現することで実現し、End-to-Endな学習を可能としている。加えてhyperparameterの設定に関する理論的分析も提供。instance segmentationやsemantic segmentationだけでなく、様々なpixel-levelのドメインタスクへ応用可能。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>手法もシンプルでかつ効果的で応用先も広い。Fig.11の結果からsemantic segmentationにおいてもinstanceの情報が効果的に利用できそうで試してみたい。</p><ul><li><a href="https://arxiv.org/abs/1712.08273">arxiv</a></li><li><a href="https://www.ics.uci.edu/~skong2/SMMMSG.html">project_page</a></li><li><a href="https://github.com/aimerykong/Recurrent-Pixel-Embedding-for-Instance-Grouping">GitHub</a></li></ul></div></div><div class="slide_index">[#230]</div><div class="timestamp">2018.6.11 07:49:04</div></div></section><section id="ID_Learning_a_Discriminative_Feature_Network_for_Semantic_Segmentation"><div class="paper-abstract"><div class="title">Learning a Discriminative Feature Network for Semantic Segmentation</div><div class="info"><div class="authors">Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, Nong Sang</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Hiroaki Aizawa</div><div class="item1"><div class="text"><h1>概要</h1><p>Semantic Segmentationにおけるintra-class inconsistencyとinter-class indistinctionの問題を、Discriminative Feature Network(DFN)によって対処。intra-class inconsistencyは図の牛の一部を馬と誤認識するような現象。inter-class indistinctionは、図のコンピュータのように外見が似ている対象の区別することが難しい現象。前者の問題をmulti-scaleかつglobal contextな情報を抽出するChannel Attention Block(CAB)を持つSmooth Networkにより、後者の問題をbottom-upなBorder Networkにより緩和する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_a_Discriminative_Feature_Network_for_Semantic_Segmentation.PNG" alt="Learning_a_Discriminative_Feature_Network_for_Semantic_Segmentation.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Semantic Segmentationをpixel単位のラベル付けだけではなく、物体の1つのカテゴリに対して一貫したセマンティックラベル付けをするタスクとして考えた。それゆえのBorder Networkと考える。上記の2つの問題は、必要な情報が異なるゆえ、対処の仕方をCABとU-Net構造に似たSmooth NetworkとBottom-upなBorder Networkとうまく分解している。PASCAL VOC 2012でmean IoU 86.2%、Cityscapesで80.3%を達成。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>実験で各モジュールの効果を検証していたが何が効いているのかよくわからない。直感的にはBorder NetworkとSmooth Networkの分離は良いアイデアと感じたが、この分離による効果は1%未満。</p><ul><li><a href="https://arxiv.org/abs/1804.09337">arxiv</a></li></ul></div></div><div class="slide_index">[#231]</div><div class="timestamp">2018.6.11 07:32:22</div></div></section><section id="ID_SemStyle_Learning_to_Generate_Stylised_Image_Captions_using_Unaligned_Text"><div class="paper-abstract"><div class="title">SemStyle: Learning to Generate Stylised Image Captions using Unaligned Text</div><div class="info"><div class="authors">A.Mathew, L.Xie and X.He</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1805.07030</div></div><div class="slide_editor">Kota Yoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>書面上のコミニュケーションをする上で文書のスタイルは魅力と明快さに影響する．同一の画像からスタイルの異なるキャプションを生成するという研究．様々なスタイルの単語の選択肢とは異なる構文をもつ文章をデコードするための統一された言語モデルを開発した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SemStyle_Learning_to_Generate_Stylised_Image_Captions_using_Unaligned_Text.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Semanticな用語を用いて文章の柔軟性を備えたキャプションの生成</li><li>スタイルと記述両方のコーパスを用いて文章レベルのスタイルを模倣するための学習</li><li>SemStyleのキャプションが画像の意味を保持し、記述的で、スタイルもシフトできていることを示した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>連続する写真からより豊富なキャプションを生成できる可能性を秘める</li><li><a href="https://arxiv.org/abs/1805.07030">Paper</a></li></ul></div></div><div class="slide_index">[#232]</div><div class="timestamp">2018.6.10 14:13:49</div></div></section><section id="ID_Reinforcement_Cutting-Agent_Learning_for_Video_Object_Segmentation"><div class="paper-abstract"><div class="title">Reinforcement Cutting-Agent Learning for Video Object Segmentation</div><div class="info"><div class="authors">Junwei Han et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Video Object Segmentation (VOS) を強化学習によって行う研究. Object Segmentation では主に物体の領域とそれらの(周辺との)関係性が重要であるという推量に基づいて, VOS をマルコフ過程として定式化し, Deep Q-Learning を適用した. 評価実験では, state-of-the-art とほぼ同等の結果を達成した. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Reinforcement-Cutting-Agent-Learning-for-Video-Object-Segmentation.png" alt="fukuhara-Reinforcement-Cutting-Agent-Learning-for-Video-Object-Segmentation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Video Object Segmentation (VOS) をマルコフ過程 (MDP) として定式化した </li><li>State は動画の現在のフレームの特徴量と過去 k（論文では k=4） フレーム分の action のヒストリーを使用</li><li>Action は object searching (9次元) と context embedding (3次元) を使用</li><li>Reward は ground truth のマスクと推定されたマスクの IoU の差で評価</li><li>強化学習は Deep Q-Learning (DQN) を使用</li><li>DAVIS dataset と YouTube-Objects dataset を用いた評価実験では, state-of-the-art とほぼ同等の結果を達成した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/3342.html" target="blank">[論文] Reinforcement Cutting-Agent Learning for Video Object Segmentation</a></li><li> <a href="https://davischallenge.org/" target="blank">[Dataset] DAVIS dataset</a></li><li><a href="https://data.vision.ee.ethz.ch/cvl/youtube-objects/" target="blank">[Dataset] YouTube-Objects dataset</a></li><li>Future work として同様の手法が　Semantic Segmentation, Object Localization, Saliency Estimation, 3D Shape Learning などに適用できる可能性を示唆</li></ul></div></div><div class="slide_index">[#233]</div><div class="timestamp">2018.6.9 17:29:55</div></div></section><section id="ID_SeedNet_Automatic_Seed_Generation_with_Deep_Reinforcement_Learning_for_Robust_Interactive_Segmentation"><div class="paper-abstract"><div class="title">SeedNet: Automatic Seed Generation with Deep Reinforcement Learning for Robust Interactive Segmentation</div><div class="info"><div class="authors">Gwangmo Song et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>インタラクティブセグメンテーションに強化学習を適用した研究. 入力画像と初期 seed から自動で新しい seed を順次生成する SeedNet を提案. 評価実験では state-of-the-art の結果を達成すると共に, 教師あり手法と比較しても優位な結果を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-SeedNet-Automatic-Seed-Generation-with-Deep-Reinforcement-Learning-for-Robust-Interactive-Segmentation.png" alt="fukuhara-SeedNet-Automatic-Seed-Generation-with-Deep-Reinforcement-Learning-for-Robust-Interactive-Segmentation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Interactive Segmentation のタスクをマルコフ過程として定式化し, 強化学習（Deep Q-Learning）を用いて学習を行った</li><li>State には入力画像の画素情報と seed の位置とラベル,　mask 画像を用いる (seed の位置を state に陽に加えることによって, 生成される mask が seed 位置の変化についてロバストになるらしい)</li><li>Action は state の情報から新しい seed の位置とラベルの決定（自由度を削減するために 20x20 のグリッド上から位置を選択, seed の数が10点になった段階で終了）</li><li>Reward は生成された Mask と Ground Truth の Mask の IoU（exp 型を提案）に加えて, SeedNet によって追加された新 seed のラベルと位置が適切かの２点を考慮して決定</li><li>MSRA10K dataset を用いた評価実験では state-of-the-art の結果に加えて, 初期の seed 位置についてロバストであることが確認された</li><li>また, 教師あり学習を用いた手法 [Long+15], [Xu+16] と比較しても優位性が確認された</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Song_SeedNet_Automatic_Seed_CVPR_2018_paper.pdf" target="blank">[論文] SeedNet: Automatic Seed Generation with Deep Reinforcement Learning for Robust Interactive Segmentation</a></li><li>強化学習を新タスクに適用してみました系列の論文</li><li>他の同系列の論文に見られる傾向と同じく, MDPによる定式化と Reward の計算方法を主な貢献としている</li><li>特に本論文は, 教師ありでは学習するのが難しい問題を上手く見つけている（seed の打ち方は user によって千差万別なのでトレーニングデータを作るのが難しい）</li></ul></div></div><div class="slide_index">[#234]</div><div class="timestamp">2018.6.10 21:50:55</div></div></section><section id="ID_Adversarial_Complementary_Learning_for_Weakly_Supervised_Object_Localization"><div class="paper-abstract"><div class="title">Adversarial Complementary Learning for Weakly Supervised Object Localization</div><div class="info"><div class="authors">Xiaolin Zhang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>弱教師ありの Object Localization の研究. 2つの Classifier を並列に配置し, 片方の classifier で注目された領域を他方の入力から取り除いておくことで, それぞれが異なる領域に反応するような構造となっている. 評価実験では ILSVRC dataset の localization　のタスクで 45.15% (new state-of-the-art) の誤差率を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Adversarial-Complementary-Learning-for-Weakly-Supervised-Object-Localization.png" alt="fukuhara-Adversarial-Complementary-Learning-for-Weakly-Supervised-Object-Localization.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>全結合層の最後に畳み込み層を1つ追加することで, CAM [Zhou＋16] と同等の object localization maps を事後処理無しで得られることを数式で示した</li><li>画像から畳み込み層によって抽出した特徴量を, 並列に配置した　classifier に入力する</li><li>片方の classifier から出力された object localization map で注目されていた領域を消去したものを, 他方の入力とすることで両方の classifier を異なる領域に反応させる</li><li>ILSVRC dataset 等を用いて行った評価実験では Localization と Classification の両タスクにおいて, state-of-the-art [Zhou+16, Singh+17] と同等か優位な結果を達成した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.06962" target="blank">[論文] Adversarial Complementary Learning for Weakly Supervised Object Localization</a></li></ul></div></div><div class="slide_index">[#235]</div><div class="timestamp">2018.6.9 00:32:55</div></div></section><section id="ID_Feature_Selective_Networks_for_Object_Detection"><div class="paper-abstract"><div class="title">Feature Selective Networks for Object Detection</div><div class="info"><div class="authors">Yao Zhai, Jingjing Fu, Yan Lu, Houqiang Li</div><div class="conference">CVPR2018</div><div class="paper_id">538</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>物体検出時に用いるRegion-of-Interest（RoI）を，sub-regionとアスペクト比の差を用いて再構成するFeature selective netsを提案．画像全体に対してsub-regionのattention bank（すべてのattention mapを記憶するbank）とアスペクト比のattention bankを生成する．Attention mapはbankから選択的にpoolされ，RoIの改善に使用される．処理の手順は(1)CNNから得られた特徴マップをRPNに入力しRoIを得て，(2)特徴マップのチャンネル数を削減してRoIプーリングを行い，圧縮されたRoI特徴を得る．(3)削減される前のRoIをregion-wise attention生成モジュールに入力する．特徴マップを用いてアスペクト比attention bankとsub-region attention bankを得る．(4)各bankにselective RoIプーリングを行う．そして，(2)と(4)で得られたRoI特徴と各attention mapを結合して検出サブネットワークに入力する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180610_FeatureSelectiveNets1.jpg" alt="20180610_FeatureSelectiveNets1.jpg"><img src="slides/figs/20180610_FeatureSelectiveNets2.jpg" alt="20180610_FeatureSelectiveNets2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>RoIをattentinを用いて補正する．VGGだけではなくGoogLeNetやResNetにも適用可能である．VOC2007を用いた評価では，mAP: 82.9%, 76.8%, 74.3% （Res101, GoogLe, VGG-16）を達成し，Faster R-CNNの78.8%, 74.8%, 73.2%（上記と同順）よりも高精度である．さらに，検出サブネットワークをシンプルにしているため，Faster R-CNNよりも高速な検出が可能である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Attentionを用いた物体検出が増えてきている．Mask R-CNNみたいにRoIに注目する手法も多い？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2123.pdf">論文</a></li><li><a href="https://arxiv.org/abs/1711.08879">arXiv</a></li></ul></div></div><div class="slide_index">[#236]</div><div class="timestamp">2018.6.10 23:42:54</div></div></section><section id="ID_Pseudo_Mask_Augmented_Object_Detection"><div class="paper-abstract"><div class="title">Pseudo Mask Augmented Object Detection</div><div class="info"><div class="authors">Xiangyun Zhao, Shuang Liang, Yichen Wei</div><div class="conference">CVPR2018</div><div class="paper_id">530</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Bounding boxでの物体検出でグラフカットを用いて擬似的なマスク（セグメンテーション）のrefinementを行う．インスタンスセグメンテーションの学習を行うことで擬似的な物体マスクを推定できるようにネットワークパラメータを最適化する．フレームワークは検出ネットワークと擬似的なマスクのrefinementを行うグラフカットベースのモジュールからなる．RoIを入力として，ベースネットワークの特徴マップからインスタンスセグメンテーションを行い，それをグラフカットモジュールに入力して擬似的なマスクを得る．インスタンスセグメンテーションの結果はbounding boxの修正にも用いられる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180610_PseudoMaskAug1.jpg" alt="20180610_PseudoMaskAug1.jpg"><img src="slides/figs/20180610_PseudoMaskAug2.jpg" alt="20180610_PseudoMaskAug2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>流行りの物体検出＋セグメンテーションの手法．マスクを単に特徴マップから得て終わりではなく，グラフカットでrefineする部分は新しいところ．グラフカットを数iter行うことで，よりきれいなマスクを得ることができる．VOC2007/2012を用いた物体検出の精度はmAP74.4%（VGG-16）で，Faster R-CNN（70.4%）やHyperNet（71.4）よりも良い．VOC2012SDSを用いたセグメンテーションの精度は58.5/67.6(マスクレベルスコア/物体検出スコア)%であり，iterを繰り返すことで精度が向上することが確認されている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>セグメンテーションタスクの精度向上のためグラフカットでマスクのrefineを繰り返し行うのは面白いと思った．Iter0とiter3でマスクの結果を比較するとかなりきれいになっている．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Pseudo_Mask_Augmented_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://arxiv.org/abs/1803.05858">arXiv</a></li></ul></div></div><div class="slide_index">[#237]</div><div class="timestamp">2018.6.10 21:35:27</div></div></section><section id="ID_Scalable_Dense_Non-Rigid_Structure-From-Motion_A_Grassmannian_Perspective"><div class="paper-abstract"><div class="title">Scalable Dense Non-Rigid Structure-From-Motion: A Grassmannian Perspective</div><div class="info"><div class="authors">Suryansh Kumar, Anoop Cherian, Yuchao Dai, Hongdong Li</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>複数画像を使用した非剛体のSfM (Non-Rigid Structure-from-Motion)に関する研究である。右図は非剛体の表面形状復元結果の一例であり、顔のように時系列的に変化する形状を、多様体の概念をSfMに導入することにより問題解決を図っている。非剛体の形状変化を、空間的・時間的な部分空間としてすいていすることでSfMを実行する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180610NRSfM.png" alt="180610NRSfM"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>非剛体物体の表面形状復元に関するSfM問題を、グラスマン多様体（Grassman Manifold）の問題と捉えて解決している点が新規性として挙げられる。柔軟に表面形状復元ができている様子は動画にて確認可能である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>DynamicFusionからこの手の問題は出て来たのだが、どのような違いがある/どのように展開されているのか？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Kumar_Scalable_Dense_Non-Rigid_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://suryanshkumar.github.io/">著者</a></li><li><a href="https://www.youtube.com/watch?v=KSLrDnOI2m4">YouTube</a></li></ul></div></div><div class="slide_index">[#238]</div><div class="timestamp">2018.6.10 16:43:01</div></div></section><section id="ID_A_Papier-Mch_Approach_to_Learning_3D_Surface_Generation"><div class="paper-abstract"><div class="title">A Papier-Mâché Approach to Learning 3D Surface Generation</div><div class="info"><div class="authors">Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>2次元画像、もしくは3次元点群からメッシュや分解構造を生成し、テクスチャありのメッシュや3次元プリント物体を出力する。この枠組みはAtlasNetと呼ばれ、同タスクのPrecision向上と一般化の面で性能改善を行い、3次元形状を集めたデータベースであるShapeNet上で形状をAuto-Encoding、単眼画像からの形状復元を行った。その他、AtlasNetを用いてモーフィング、パラメトライゼーション、超解像、形状マッチング、共セグメンテーションを実施した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180610AtlasNet.png" alt="180610AtlasNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>3D表面形状生成器であるAtlasNetを構築したことが最も大きな新規性である。形状に関するパラメータを学習可能にした。さらに、AtlasNetをGitHub上で公開して使用できる形式にしている。復元したメッシュ形状も、提案手法がもっともノイズが少なく、良好な復元結果となった。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>数年前は型崩れの多い3次元形状を出力するGeneratorであったが、徐々によくなりつつある。この研究もまだ過程にしか過ぎない？</p><ul><li><a href="https://arxiv.org/pdf/1802.05384.pdf">論文</a></li><li><a href="http://imagine.enpc.fr/~groueixt/atlasnet/">Project</a></li><li><a href="https://github.com/ThibaultGROUEIX/AtlasNet">GitHub</a></li></ul></div></div><div class="slide_index">[#239]</div><div class="timestamp">2018.6.10 16:03:52</div></div></section><section id="ID_Improving_Occlusion_and_Hard_Negative_Handling_for_Single-Stage_Pedestrian_Detectors"><div class="paper-abstract"><div class="title">Improving Occlusion and Hard Negative Handling for Single-Stage Pedestrian Detectors</div><div class="info"><div class="authors">Junhyug Noh, et al.</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>歩行者検出におけるオクルージョンやハードネガティブを改善するための提案。本提案手法は、シングルステージ物体検出手法に適応可能。オクルージョン処理のために、ベースモデルの出力テンソルを更新してパートスコアを推定し、オクルージョン認識スコアを算出する。ハードネガティブの混同を軽減するために、 average grid classifiersをpost-refinement classifiersとして導入。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180690IOHN.jpg" alt="20180690IOHN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>SqueezeDetやYOLOv2、SSD、DSSDを含むシングルステージ物体検出手法に適応でき、オクルージョンやハードネガティブを改善する。本論文では歩行者検出におけるオクルージョンにフォーカスを当てているが、一般物体検出にも適応できる可能性がある。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>CaltechPedestrianとCityPersonsデータセットで評価。4つのモデルのパフォーマンス向上を確認。重度のオクルージョン設定において、最良のパフォーマンス。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0620.pdf">論文</a></li></ul></div></div><div class="slide_index">[#240]</div></div></section><section id="ID_Iterative_Learning_with_Open-set_Noisy_Labels"><div class="paper-abstract"><div class="title">Iterative Learning with Open-set Noisy Labels</div><div class="info"><div class="authors">Yisen Wang, et al.</div><div class="paper_id">1804.00092</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>ノイズのあるラベルを含んだデータセットを使い、CNN学習を高精度に行うための新しい反復学習フレームワークの提案。反復的なノイズラベル検出、特徴学習、および再重み付けの3段階のフレームワークでノイズの多いラベルを検出しつつ、識別器を反復的に学習。再重みづけでは、クリーンなラベルの学習を重視し、ノイズの場合には低減させる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180690NoisyLabels.jpg" alt="20180690NoisyLabels.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>綺麗なラベルアノテーション付き大規模データセットによる学習は非常に重要だが、人の手間がかなりかかる他、ヒューマンエラーを含む可能性が否めない。本研究では、あえてノイジーなデータセットに挑戦することで、これらの問題を解決する。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>データセットの収集コストや信頼性の問題に伴って、自ら良いデータを選択して学習する需要が高まっている印象。</p><ul><li><a href="https://arxiv.org/pdf/1804.00092.pdf">論文</a></li></ul></div></div><div class="slide_index">[#241]</div></div></section><section id="ID_Hand_PointNet_3D_Hand_Pose_Estimation_using_Point_Sets"><div class="paper-abstract"><div class="title">Hand PointNet: 3D Hand Pose Estimation using Point Sets</div><div class="info"><div class="authors">Liuhao Ge, et al.</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>正規化されたポイントクラウドを入力として、複雑な手構造を捕捉し、手の姿勢の低次元表現を正確に回帰させることができるHand PointNetの提案。Oriented Bboxでポイントクラウドを正規化し、ネットワーク入力をよりロバストにする。その後、階層的なPointNetに入力し特徴抽出。PointNetを細分化することにより、指先に対する推定精度を向上させる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180610HandPointNet.jpg" alt="20180610HandPointNet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>CNNを用いた従来の奥行き画像における3次元手姿勢推定手法とは異なり、本研究では三次元点群に着目している。データは、奥行き画像をポイントクラウドデータに変換してから使用している。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>3つのハンドポーズデータセットにて実験し、リアルタイム性に優れていることを示唆。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Ge_Hand_PointNet_3D_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#242]</div></div></section><section id="ID_Toward_Driving_Scene_Understanding_A_Dataset_for_Learning_Driver_Behavior_and_Causal_Reasoning"><div class="paper-abstract"><div class="title">Toward Driving Scene Understanding:A Dataset for Learning Driver Behavior and Causal Reasoning</div><div class="info"><div class="authors">Vasili Ramanishka, et al.</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>自動車の運転シーン理解のためのデータセットであるHonda Research Institute Driving Dataset(HDD)の提案。本データセットはサンフランシスコ・ベイエリアにて、様々なセンサーを備えた自動車を人間が運転したデータが104時間分含まれる。センサはグラスホッパーカメラ、LiDAR、ダイナミックモーションアナライザ、Vehicle Controller Area Network (CAN)の4つ。これらのデータから運転者の行動を基にアノテーションを付加している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180610HDD.jpg" alt="20180610HDD.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>様々なセンサを用いて、大規模データを収集しただけでなく、ヒューマンファクタや認知科学に基づいてアノテーションを行っている。アノテーションは、Goal-oriented action, Stimulus-driven action, Cause, Attentionの4つ。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>LSTMを用いたベースラインにおいて、センサを増やすことによって表現力の向上が見られた。評価が難しいアノテーションデータが含まれ、チャレンジングなデータセット。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3855.pdf">論文</a></li></ul></div></div><div class="slide_index">[#243]</div></div></section><section id="ID_A_High-Quality_Denoising_Dataset_for_Smartphone_Cameras"><div class="paper-abstract"><div class="title">A High-Quality Denoising Dataset for Smartphone Cameras</div><div class="info"><div class="authors">Abdelrahman Abdelhamed, Stephen Lin, Michael S. Brown</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>スマートフォンで撮影したノイズの多い画像で構成したデータセットSmartphone Image Denoising Dataset (SIDD)の提案。 5つの代表的なスマホカメラを使用し、様々な照明条件下で約30,000枚のノイズの多い画像を収集。ノイズの多い画像だけでなく、ノイズを除去した画像をground truthとして提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180610SIDD.jpg" alt="20180610SIDD.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>過去10年間で、撮影される画像は一眼レフやコンデジから、スマートフォンに切り替わったことに着目。しかし、口径やセンサ―サイズが小さいため、スマホの写真はノイズを多く含んでいる。このような、ノイズを多く含んだスマホ画像を集めることで新たなデータセットを提案する。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>やはりノイズを含むスマホ画像でのトレーニングよりも、高品質な画像でトレーニングした方が、CNNで高い精度を得た。現在のタスクにおいて「スマホの画像だから精度が出ない」というのはあまり考えにくいが、日常的なアプリケーションには有用なデータセットではないか。</p><ul><li><a href="http://www.cse.yorku.ca/~mbrown/pdf/sidd_cvpr2018.pdf">論文</a></li></ul></div></div><div class="slide_index">[#244]</div></div></section><section id="ID_Fast_and_Furious_Real_Time_End-to-End_3D_Detection_Tracking_and_Motion_Forecasting_with_a_Single_Convolutional_Net"><div class="paper-abstract"><div class="title">Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting with a Single Convolutional Net</div><div class="info"><div class="authors">Wenjie Luo, Bin Yang, Raquel Urtasun</div><div class="conference">CVPR2018</div><div class="paper_id">437</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>3Dセンサで得られた点群から3D物体検出や追跡を行う新しいDNN「Fast and Furious（FaF）」を提案．検出と追跡，さらに短期の経路予測を同時に推論でき，Sparse dataやオクルージョンに頑健な検出ができる．3D点群と時間の4Dテンソルを入力として，空間と時間に対して3D畳み込みを行う．4DテンソルはEarly FusionまたはLate Fusion（図中ではLater）で時間情報を結合している．これらは精度と効率のトレードオフ関係にある．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180610_FaF1.jpg" alt="20180610_FaF1.jpg"><img src="slides/figs/20180610_FaF2.jpg" alt="20180610_FaF2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>物体検出から追跡，さらに経路予測までend-to-endで行えるモデル．全体の検出時間はわずか30ms以下である．約55万フレームからなるLiDARのデータセットを作成し，車両に3D bboxとトラッキング用IDをラベリングして学習および評価に用いる．物体検出の結果はSSDのIoU 77.92mAPを上回る83.10mAPである（Late Fusionを用いることで1.4mAP向上している）．追跡もHungarianと同等以上の性能で，経路予測もL2距離0.33メートル未満で10フレーム予測可能である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>タイトルが某カーアクション映画みたいでカッコいい．内容も名前負けしておらずよく作り込まれておりOralで採択されている．インパクトのあるタイトルは大切．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3013.pdf">論文</a></li></ul></div></div><div class="slide_index">[#245]</div><div class="timestamp">2018.6.10 03:15:09</div></div></section><section id="ID_Low-Shot_Learning_from_Imaginary_Data"><div class="paper-abstract"><div class="title">Low-Shot Learning from Imaginary Data</div><div class="info"><div class="authors">Yu-Xiong Wang, et al.</div><div class="paper_id">1801.05401</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>人間の想像力に着目することで、メタ学習におけるLow-Shot Learningを可能にするアーキテクチャの提案。コンピュータビジョンに幻覚(想像)を抱かせることで、少ないデータから新しい視覚的概念を学習させる。アプローチとしては、メタ学習を取り入れており、 meta-learnertとhallucinator(幻覚者)を組み合わせて共同で最適化。hallucinatorは、通常のトレインセットとノイズベクトルから幻覚トレーニングセットを出力する。通常のトレーニングセットに加えて、幻覚トレーニングセットを学習することで精度向上を図る。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180609imagin.jpg" alt="20180609imagin.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>人間は新しい視覚的情報を素早く学習できる。これは、「物体がさまざまな視点から見たときにどのように見えるかを想像できるから」と仮定。そのうえで、人間の想像力をモデルとし、システムに組み込むことでLow-Shot Learningを可能にしている。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>AIに幻覚を見せられる時が来た模様。さまざまなメタ学習手法に組み込むことができ、精度を向上させられるらしい。</p><ul><li><a href="https://arxiv.org/pdf/1801.05401.pdf">論文</a></li></ul></div></div><div class="slide_index">[#246]</div></div></section><section id="ID_Multi-View_Harmonized_Bilinear_Network_for_3D_Object_Recognition"><div class="paper-abstract"><div class="title">Multi-View Harmonized Bilinear Network for 3D Object Recognition</div><div class="info"><div class="authors">Tan Yu, Jingjing Meng, Junsong Yuan</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>3次元物体認識を実行するMulti-view Harmonized Bilinear Network (MHBN)を提案する。異なるビューの特徴量を学習するために基本的にはパッチベースでマッチングを行う。Polynomial Kernel/Bilinear Poolingの関係性を記述するために、畳み込みによる3次元物体表現とBilinear Poolingを実行する。MHBNの枠組みはEnd-to-Endでの学習が可能である。構造は右図のように示され、畳み込みにより特徴マップ（3次元物体表現）を生成、最後にBilinear Poolingを通り抜けて識別を実行。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609MultiviewHarmonized.png" alt="180609MultiviewHarmonized"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>3次元物体認識の場面においてSoTA。ModelNet40, ModelNet10ではそれぞれ94.7 (Instance)/93.1 (Class), 95.0 (Instance)/95.0 (Class)である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>3次元物体認識ではホントの意味での大規模DBはないのだろうか？ModelNetにしてもShapeNetにしてもCADをベースにしている？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Multi-View_Harmonized_Bilinear_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://cse.buffalo.edu/~jsyuan/">著者</a></li><li><a href="http://vis-www.cs.umass.edu/mvcnn/">MVCNN</a></li></ul></div></div><div class="slide_index">[#247]</div><div class="timestamp">2018.6.9 22:40:37</div></div></section><section id="ID_Disentangled_Person_Image_Generation"><div class="paper-abstract"><div class="title">Disentangled Person Image Generation</div><div class="info"><div class="authors">Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc Van Gool, Bernt Schiele, Mario Fritz</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>アピアランス/ビューポイント/背景など、分解された（Disentangled）人物画像の生成を行うための研究である。この目的のため、2ステージの生成手法を考案した（右図を参照）。1ステージ目はリアルの埋め込み特徴（Embedding Features）を獲得する学習を行い、前景/背景や姿勢などを表現。次に2ステージ目は敵対的学習により生成的特徴学習を行いガウシアンノイズから中間表現にマッピング、特徴変換を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609DisentangledPersonImageGeneration.png" alt="180609DisentangledPersonImageGeneration"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>姿勢ベースの人物画像を生成し、人物再同定（Person Re-Identification; ReID）の学習に適用。人物画像生成自体も誤差が少なく、ReIDのためのにおいても良好な精度を実現した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>学習画像がコントロールできるということで注目される技術。ある程度の知見を学習しておけば、そのうちリアル画像のデータがいらない時代になる？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Ma_Disentangled_Person_Image_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://github.com/charliememory/Disentangled-Person-Image-Generation">GitHub</a></li></ul></div></div><div class="slide_index">[#248]</div><div class="timestamp">2018.6.9 21:59:40</div></div></section><section id="ID_Learning_Pose_Specific_Representations_by_Predicting_Different_Views"><div class="paper-abstract"><div class="title">Learning Pose Specific Representations by Predicting Different Views</div><div class="info"><div class="authors">Georg Poier, David Schinagl, Horst Bischof</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>異なるビューポイントの距離画像入力から、低次元の潜在表現を利用して手部領域追跡の学習を実行する研究である。ビューポイント推定の誤差をフィードバックして、教師なしでも手部の姿勢推定に必要な潜在表現を獲得する。これにより、必要なのは対象となるビューポイントではなく、第二のビューポイントのみであり、ラベルあり/ラベルなしの場合においても効果的に学習することができる（Semi-supervised Learningの枠組みで学習可能）。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609PoseSpecificRepresentation.png" alt="180609PoseSpecificRepresentation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>あるビューポイントの距離画像が手に入れば、異なるビューポイントに関する手部領域の姿勢推定が可能になるSemi-supervised Learningを提案。異なるビューポイントの低次元潜在表現を学習し、3Dの関節位置を推定することができる。NYU-CS dataset/MV-hands datasetにてState-of-the-artな精度を達成。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>中間表現（本論文の場合には低次元潜在空間）を学習して、異なるドメイン間の学習に応用したい。このような問題は意外と簡単にできるのだろうか？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Poier_Learning_Pose_Specific_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#249]</div><div class="timestamp">2018.6.9 19:19:49</div></div></section><section id="ID_Fine-grained_Video_Captioning_for_Sports_Narrative"><div class="paper-abstract"><div class="title">Fine-grained Video Captioning for Sports Narrative</div><div class="info"><div class="authors">Huanyu Yu, Shuo Cheng, Bingbing Ni, Minsi Wang, Jian Zhang, Xiaokang Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><Kazushige>okayasu</Kazushige></div><div class="item1"><div class="text"><h1>概要</h1><p>Fine-grainedなスポーツ動画キャプショニング</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Fine-grained_Video_Captioning_for_Sports_Narrative.png" alt="Fine-grained_Video_Captioning_for_Sports_Narrative"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>youtubeから2Kのスポーツ動画とキャプションからなるFine-grained Sports Narrative dataset(FSN)の提案</li><li>スポーツビデオのキャプショニングの新しい評価指標Fine-grained Captioning Evaluation(FCE)の提案</li><li>スポーツビデオのキャプショニングの新しいフレームワークの提案(骨格情報とオプティカルフローで詳細な動作のエンコード，オプティカルフローと選手のローカライズ結果で人物間のインタラクションをエンコードそれらのエンコードされたベクトルを階層的RNNで言語化)</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0668.pdf">論文</a></li></ul></div></div><div class="slide_index">[#250]</div><div class="timestamp">2018.3.24 13:04:44</div></div></section><section id="ID_GANerated_Hands_for_Real-Time_3D_Hand_Tracking_From_Monocular_RGB"><div class="paper-abstract"><div class="title">GANerated Hands for Real-Time 3D Hand Tracking From Monocular RGB</div><div class="info"><div class="authors">Franziska Mueller, Florian Bernard, Oleksandr Sotnychenko, Dushyant Mehta, Srinath Sridhar, Dan Casas, Christian Theobalt</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>RGBのみの動画入力からリアルタイムに3次元手部関節位置推定を実行する手法を提案。YouTubeのようなコントロールされていない場面においても3次元手部関節位置推定を行うことができる。本論文では3次元のハンドモデルとCNNを組み合わせることによりトラッキングを実行しており、GANによる生成ベース（手の3次元合成データをリアルに変換していることに相当）の手法によりオクルージョンやビューポイントの違いに頑健である。GANはAdversarial LossとCycle-consistency Loss、さらには幾何学的な整合性を保つためにGeometric Consistency Lossを最適化するよう学習。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609GANeratedHand.png" alt="180609GANeratedHand"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>GANをベースとして合成データからリアル画像を生成、同データで学習したモデルは、RGB-onlyな3次元ハンドトラッキングにおいてState-of-the-artである。敵対的学習を用いたデータ生成手法、YouTube等のあまり校正されていないデータにおいても良好な精度を実現していることが採択された理由であると考える。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>3Dデータを自由に生成できることは、次世代のアイディアを実現するための大きなポイントである。3次元トラッキングのみならず面白いこと考えたい。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Mueller_GANerated_Hands_for_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#251]</div><div class="timestamp">2018.6.9 18:52:55</div></div></section><section id="ID_A_Certifiably_Globally_Optimal_Solution_to_the_Non-Minimal_Relative_Pose_Problem"><div class="paper-abstract"><div class="title">A Certifiably Globally Optimal Solution to the Non-Minimal Relative Pose Problem</div><div class="info"><div class="authors">Jesus Briales, Laurent Kneip, Javier Gonzalez-Jimenez</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>キャリブレーション済みの２カメラにおける相対姿勢の推定問題を解くための全体最適化法（Globally Optimal Solution）を提案する。局所最適解ではなく、グローバルな最適化が計算できることが新規性である。本論文では、凸最適化の問題においてあらかじめ定義された問題（Shor's Convex Relaxation）としてQuadratically Constrained Quadratic Program (QCQP)を扱うことを実施する。ここに対して、理論的かつ実験的な解答法を提示したことが本論文の貢献である。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609GloballyOptimalSolution.png" alt="180609GloballyOptimalSolution"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>２カメラの相対姿勢問題の解決のために従来の凸最適化手法を適用して、理論的かつ実験的に解決できることを示したことが新規性であり、CVPRに採択された理由である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>（あまり深く読めていないのと、知識が足りなくて自信がないです。。）</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Briales_A_Certifiably_Globally_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#252]</div><div class="timestamp">2018.6.9 17:36:01</div></div></section><section id="ID_LiDAR-Video_Driving_Dataset_Learning_Driving_Policies_Effectively"><div class="paper-abstract"><div class="title">LiDAR-Video Driving Dataset: Learning Driving Policies Effectively</div><div class="info"><div class="authors">Yiping Chen, et al. </div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>LiDERで取得したポイントクラウド、車載カメラ映像、および一般ドライバーの運転動作からなるLiDAR-Videoデータセットの提案。運転動作は、ハンドルの傾きと自動車の走行速度情報によるもの。また、これらのデータを使い、自律走行における運転手段を決定するためのPolicy Learningを提案。 これは、DNN+LSTMで構成されるアーキテクチャである。3種類のデータの対応時間を登録することでどのように運転するかをベンチマークする。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180690LiDERVIDIO.jpg" alt="20180690LiDERVIDIO.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>自律走行において、これまではカメラとレーザースキャナー、運転動作を組み合わせたデータやアプローチがなかった。本論文ではデータベースを構築したうえで、自律走行に対するアプローチを提案している。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>単一のデータよりも3つのデータを組み合わせることで精度が向上していることを示唆。また、DNN単体よりも長いtermで処理できるDNN+LSTMの方が精度向上につながることも示唆。</p><ul><li><a href="https://drive.google.com/file/d/1d9OlwHOAz0EgoiMTFsNWaievOArKO5EC/view">論文</a></li></ul></div></div><div class="slide_index">[#253]</div></div></section><section id="ID_Collaborative_and_Adversarial_Network_for_Unsupervised_domain_adaptation"><div class="paper-abstract"><div class="title">Collaborative and Adversarial Network for Unsupervised domain adaptation</div><div class="info"><div class="authors">Weichen Zhang, Wanli Ouyang, Wen Li, Dong Xu</div><div class="conference">CVPR 2018 Spotlight</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>CNNの浅い層ではドメイン固有の特徴量を、深い層ではドメインに不変な特徴量を取得することでdomain adaptationを行うCollaborative and Adversarial Network(CAN)を提案。
従来の<a href="http://jmlr.org/papers/volume17/15-239/15-239.pdf">Domain Adversarial Training of Neural Network(DANN)</a>ではドメインに不変な特徴量を学習することができるものの、ターゲットドメイン固有の特徴量を得ることが難しいという問題があった。
提案手法では、CNNの浅い層では低次の特徴量を、深い層では高次の特徴量を取得することができることに着目し、
CNNのそれぞれのブロックに対するdomain discriminatorに対して、浅いブロックではソースドメインとターゲットドメインを識別可能となるように、
深いそうでは識別が不可能となるように学習を行う。ソースドメインに対してはクラスの識別も行う。
またテストデータに対してpseudo labelingを行うIncremental CAN(iCAN)も提案。
ターゲットドメインのサンプルのうち、高いconfidenceでソースドメインであると判定され、
かついずれかのラベルに対するconfidenceが高いものに対してpseudo labelingを行うことで、データセットを拡張しdomain shiftを解消する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Collaborative_and_Adversarial_Network_for_Unsupervised_domain_adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>CNNの浅いブロックで得られる特徴量に対してはドメイン識別が可能なように、深いブロックで得られる特徴量に対してはドメイン識別が不可能なように学習を行うCANを提案。
またターゲットドメインに対してpseudo labeingを行うiCANも提案。</li><li>実験で使用したのはpretrained RenNet50であり、10層目、22層目、40層目、49層目のそれぞれに対してdomain discriminatorを適用。41~49層からなるブロックからドメインに不変な特徴量を得るように学習を行った。</li><li>Office31、ImageCLEF-DAを用いたクラス識別においてstate-of-the-artと比較した結果、最も高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>シンプルな発想だが面白い手法！似たアイディアで画像の生成もできないだろうか？</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Collaborative_and_Adversarial_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#254]</div></div></section><section id="ID_Look_at_Boundary_A_Boundary-Aware_Face_Alignment_Algorithm"><div class="paper-abstract"><div class="title">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</div><div class="info"><div class="authors">Author</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔の境界線を事前分布として使用することで、顔のランドマークを推定する手法を提案。既存手法でジゼ情報として使用されている顔のパーツは情報が離散的であり、
顔に対するセマンティックセグメンテーションであるface parsingは鼻に対する精度が良くない。
一方で顔の境界線は定義がはっきりしており、かつ顔の形状から推定することが可能。
提案手法では顔の境界線をstacked hourglassをベースとして、オクルージョンに対して頑健になるようにmessage passing layer、
推定精度の向上のためにadversarial netを導入している。推定された顔の境界線を元に、顔のランドマークを推定する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Look_at_Boundary_A_Boundary-Aware_Face_Alignment_Algorithm.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>事前実験によって顔の境界線を用いたランドマーク推定がstate-of-the-artよりも優っていることを確認した上で手法を提案。</li><li>300W, COFW, AFLWなどのデータセットにおいてstate-of-the-arttと比較した結果、全ての場合において提案手法が優位となった。また境界線のGTを使用したランドマーク推定をOracleとして示しており、
Oracleによる推定精度が最も高くなった。</li><li>WIDER FaceデータセットをベースにしたWider Facial Landmarks in-the-wild(WFLW)データセットを構築しており、10000枚の画像に対して98点のランドマーク、オクルージョン、メイク、照明環境、ブラー、表情のアノテーションを持つ。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>事前実験やOracleによって精度向上の理由が明確になっていルため、手法の優位性がはっきりと伝わってくる。</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1330.pdf">論文</a></li><li><a href="https://wywu.github.io/projects/LAB/LAB.html">Project page(Supplementary material, Demo, Code)</a></li></ul></div></div><div class="slide_index">[#255]</div></div></section><section id="ID_Revisiting_knowledge_transfer_for_training_object_class_detectors"><div class="paper-abstract"><div class="title">Revisiting knowledge transfer for training object class detectors</div><div class="info"><div class="authors">Jasper Uijlings, Stefan Popov, Vittorio Ferrari</div><div class="paper_id">1708.06128</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>ソースクラスのBBoxアノテーションを使って、弱教師付きのトレーニング画像からターゲットの物体検出器を学習する知識転移手法の提案。まず、ソーストレインセットでproposal generatorをトレーニングし、それをターゲットトレインセットに適用。次に、画像のクラスラベル(Bboxなし)を使用し、知識転移でMultiple Instance Learning(MIL)を実行。 MILによって、物体検出器をトレーニングするために使用する、ターゲットクラス用のBBoxを生成。最後に、ターゲットの物体検出器をターゲットテストセットに適用。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180609MKT.jpg" alt="20180609MKT.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>物体候補とクラスを段階的に知識伝達していくフレームワーク。これにより、固有のクラスやジェネリックなクラスに渡る、広い知識伝達を可能にすることができる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>段階的な知識伝達によって、良質な物体候補を出力できる。</p><ul><li><a href="https://arxiv.org/pdf/1708.06128.pdf">論文</a></li></ul></div></div><div class="slide_index">[#256]</div></div></section><section id="ID_Fight_Ill-Posedness_With_Ill-Posedness_Single-Shot_Variational_Depth_Super-Resolution_From_Shading"><div class="paper-abstract"><div class="title">Fight Ill-Posedness With Ill-Posedness: Single-Shot Variational Depth Super-Resolution From Shading</div><div class="info"><div class="authors">Bjoern Haefner, Yvain Quéau, Thomas Möllenhoff, Daniel Cremers</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>距離空間/距離画像の超解像を行う（Super-Resolution）を行う技術を提案。従来はShape-from-shadingにより行って来たが、形状の複雑性（誤りを含む）が存在していたため、これを改善する手法を提案した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609FightIllPosed.png" alt="180609FightIllPosed"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>距離画像における超解像を行うための最適化手法を提案した。結果は図に示すとおりである。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Haefner_Fight_Ill-Posedness_With_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://github.com/YapengTian/Single-Image-Super-Resolution">Single-Image-Super-Resolution</a></li></ul></div></div><div class="slide_index">[#257]</div><div class="timestamp">2018.6.9 13:58:31</div></div></section><section id="ID_Multistage_Adversarial_Losses_for_Pose-Based_Human_Image_Synthesis"><div class="paper-abstract"><div class="title">Multistage Adversarial Losses for Pose-Based Human Image Synthesis</div><div class="info"><div class="authors">Chenyang Si, Wei Wang, Liang Wang, Tieniu Tan</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>人物の姿勢を事前情報として、ある視点の人物画像の入力からビューポイントを変更した人物画像を合成する手法を提案する。右図では3ステージのフレームワークについて示しており、最初のステージでは角度情報を挿入した姿勢変換、次のステージでは角度変化した人物にアピアランスを挿入、最後に背景を自然に挿入するステージ、という感じで変換が進んで行く。どう枠組みを実行するため、特にステージ２ではAdversarial Lossが、ステージ３ではForeground/Global Adversarial Lossを適用して誤差を計算する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609PoseHumanSynthesis.png" alt="180609PoseHumanSynthesis"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>評価は生成した画像のPSNR（シグナル・ノイズ比）、正解値との誤差SSIMを計算して、提案手法がもっとも優れた数値を出していることを明らかにした（SSIM: 0.72, PSNR: 20.62）。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>データセットの環境が固定だからできる？背景モデルの空間が非常に小さいので変換した際にもテクスチャが崩れずに生成できる？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Si_Multistage_Adversarial_Losses_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#258]</div><div class="timestamp">2018.6.9 13:47:06</div></div></section><section id="ID_Cross-Modal_Deep_Variational_Hand_Pose_Estimation"><div class="paper-abstract"><div class="title">Cross-Modal Deep Variational Hand Pose Estimation</div><div class="info"><div class="authors">Adrian Spurr, Jie Song, Seonwook Park, Otmar Hilliges</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>2次元画像と3次元手部モデルを同様の空間で扱うことができるCross-modal latent spaceを提案して、手部姿勢推定を実行する。別々にクラスタリングするのではなく、同一の空間で扱う（2DRGB-3D空間関係なく、同じ姿勢は同じような空間位置に投影される）方がマッチングの際にも便利。この特徴空間を学習するためにVariational Auto-Encoder（VAE）の枠組みで、Cross-modalのKL-divergenceを学習する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609CrossModalLatentSpace.png" alt="180609CrossModalLatentSpace"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>2D-3Dの共通空間を学習することで、2D画像からダイレクトに手部の3D関節点推定に成功した。距離画像との単一空間も学習可能とした。同一空間上で扱えるようにして、かつ従来法よりも精度向上が見られたため、CVPRに採択された。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>異なるモダリティを同一の枠組みで行ってしまう（2d-3dを同じ空間で）学習は他にもありそう？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Spurr_Cross-Modal_Deep_Variational_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#259]</div><div class="timestamp">2018.6.9 13:24:52</div></div></section><section id="ID_Progressive_Attention_Guided_Recurrent_Network_for_Salient_Object_Detection"><div class="paper-abstract"><div class="title">Progressive Attention Guided Recurrent Network for Salient Object Detection</div><div class="info"><div class="authors">Xiaoning Zhang, et al.</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>マルチレベルのコンテクスト情報を選択的に統合する、顕著性のためのProgressive Attention Guided Recurrent Networkの提案。Attention Moduleを複数組み込み、その出力をステップ形式で統合していく。高レベルのfeatureを使って、低レベルのfeatureをガイドするイメージ。また、ネットワーク全体を最適化するためのmulti-path recurrent feedbackを提案。これにより、上部の畳み込み層からのセマンティック情報を、浅い層に転送することができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180609PAGRN.jpg" alt="20180609PAGRN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>顕著性推定のための学習方法の提案。 従来のFCNベースの方法では、情報を区別せずに多レベルの畳み込み特徴を直接適用してしまうため、精度が上がらないと指摘。複数の層、複数のAttention Module出力を使い、コンテキスト情報を統合するので強力な特徴を抽出できる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>6種類のデータベースで精度評価。従来手法と比較して、ほぼ全てで最良の結果。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1235.pdf">論文</a></li></ul></div></div><div class="slide_index">[#260]</div></div></section><section id="ID_Scale-Transferrable_Object_Detection"><div class="paper-abstract"><div class="title">Scale-Transferrable Object Detection</div><div class="info"><div class="authors">Peng Zhou, et al.</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>マルチスケールに対応した物体検出器であるScale-Transferrable Object Detection(STDN)の提案。STDNは DenseNet-169をベースとし、複数の物体スケールに対応するためのsuper-resolution layersを搭載。このsuper-resolution layersによってアップサンプリングすることで高解像度のfeature mapを得られるので小さな物体に対応し、大きな物体にはpooling層で対応する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180690STDN.jpg" alt="20180690STDN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>従来の物体検出手法では、様々なサイズのfeature mapを組み合わせるなどして、スケールに対応していたが、やはり小さな物体は苦手。本手法では、super-resolution layersという新たな手法によって改善を図る。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>PASCAL VOCやMS COCOなどで精度向上を示している。個人的には、物体検出が苦手とする小さな物体に着目したデータセットなどを用意したうえで精度を比較してみたい。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1376.pdf">論文</a></li></ul></div></div><div class="slide_index">[#261]</div></div></section><section id="ID_Weakly_and_Semi_Supervised_Human_Body_Part_Parsing_via_Pose-Guided_Knowledge_Transfer"><div class="paper-abstract"><div class="title">Weakly and Semi Supervised Human Body Part Parsing via Pose-Guided Knowledge Transfer</div><div class="info"><div class="authors">Hao-Shu Fang, Guansong Lu, Xiaolin Fang, Jianwen Xie, Yu-Wing Tai, Cewu Lu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>人物姿勢推定において「似たような姿勢はほぼ同じセグメント結果を保有する」という前提で弱教師付き/半教師あり学習を実行する。ある対象画像が入力された際にはほぼ同じ姿勢のデータをDBから検索して知識を転用（Pose-guided Knowledge Transfer）学習を実行する。その際に姿勢による拘束条件（Morphological Constraints）を入れ込むことでピクセルベースの姿勢のセグメンテーションを実行。モデルは全層畳み込みネット（Fully Convolutional Networks; FCN)を適用。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609WeakSemiPoseParsing.png" alt="180609WeakSemiPoseParsing"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>弱教師付き学習（類似の姿勢を検索して対応づける）/半教師付き学習（少量のデータがあれば学習を実行）、いずれの手法でも姿勢学習を実行することができる。その上でデータ量を確保することに成功し、PASCAL-Part datasetにてmAPが3ポイント向上した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>より少量のアノテーションで、かつ複数の枠組みで（本論文の場合は弱教師付き学習/半教師あり学習）学習が実行できる枠組みが増えてきた。そればかりか、教師あり学習のみよりも精度の高いものができあがりつつある。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Fang_Weakly_and_Semi_CVPR_2018_paper.pdf">論文</a></li><li><a href="http://www.stat.ucla.edu/~jxie/">著者</a></li><li><a href="whttps://github.com/MVIG-SJTU/WSHPww">GitHub</a></li></ul></div></div><div class="slide_index">[#262]</div><div class="timestamp">2018.6.9 09:39:15</div></div></section><section id="ID_Occluded_Pedestrian_Detection_Through_Guided_Attention_in_CNNs"><div class="paper-abstract"><div class="title">Occluded Pedestrian Detection Through Guided Attention in CNNs</div><div class="info"><div class="authors">Shanshan Zhang, et al.</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>オクルージョンに頑健な、Faster R-CNNベースの歩行者検出手法の提案。歩行者検出について解析することで、CNN特徴の各チャンネルがそれぞれ異なる身体部分を活性化していることに着目。(実際にチャンネルごとにアテンションを取ることで確認)各チャンネルが異なる身体部位を表現しているならば、オクルージョン発生時に身体部位の特定の組み合わせを定式化することができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180608OPDTGA.jpg" alt="20180608OPDTGA.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>歩行者検出器におけるCNN特徴について解析することで、歩行者に特化した物体検出を可能にしている。Faster R-CNNにAttention Networkを追加したアーキテクチャを提案。これにより、上位featureの重みパラメータを調節。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>アーキテクチャをあまり複雑化せずに精度を向上させている。動物や虫などでも、CNNチャンネルごとに異なる身体部位を表現しているのだろうか。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2198.pdf">論文</a></li></ul></div></div><div class="slide_index">[#263]</div></div></section><section id="ID_FaceID-GAN_Learning_a_Symmetry_Three-Player_GAN_for_Identity-Preserving_Face_Synthesis"><div class="paper-abstract"><div class="title">FaceID-GAN: Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis</div><div class="info"><div class="authors">Yujun Shen, Ping Luo, Junjie Yan, Xiaogang Wang, Xiaoou Tang</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>IDを保った任意の顔向き画像をGANで生成するために、実画像ドメインと合成画像ドメインのそれぞれのIDを識別するclassifierを導入したFaceID-GANを提案。従来のGANではgeneratorとdiscriminatorが競い合うだけでclassifierは補助的な機能を果たしていたが、
提案手法におけるclassifierは実画像に対しては実画像ドメインのID番号を、
合成画像に対しては合成画像ドメインのID番号を識別させる、というようにデータセットに含まれるN個のラベルに対して、
2Nのラベル識別を行う。
他にも実画像のIDを表す特徴量と合成画像のIDを表す特徴量のコサイン類似度をロス関数として使用することで、
異なるドメインに属する特徴量の類似度を高める。generatorには顔の形状特徴量、顔向き特徴量、ランダムノイズを入力とする。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/FaceID-GAN_Learning_a_Symmetry_Three-Player_GAN_for_Identity-Preserving_Face_Synthesis.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>実画像、合成画像のそれぞれのドメインにおいてID識別を行うclassifierをGANに導入することで、generator VS. discriminator & classifier の構図を持つFaceID-GANを提案。</li><li>CASIA-WebFace494414枚(10575人のID)の画像でトレーニングを行い、LFW, IJB-A, CelebA, CFPで検証した。</li><li>state-of-the-artと横顔を入力とした正面顔画像生成、水平方向の視点移動、face verificationの精度を比較した結果、最も高い精度を達成した。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_FaceID-GAN_Learning_a_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://www.youtube.com/watch?v=EDahz2ppTYY">Demo</a></li></ul></div></div><div class="slide_index">[#264]</div></div></section><section id="ID_Unsupervised_Sparse_Dirichlet-Net_for_Hyperspectral_Image_Super-Resolution"><div class="paper-abstract"><div class="title">Unsupervised Sparse Dirichlet-Net for Hyperspectral Image Super-Resolution</div><div class="info"><div class="authors">Ying Qu, Hairong Qi, Chiman Kwan</div><div class="conference">CVPR 2018 Spotlight</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>高解像度かつ短いスペクトルバンド幅で撮影された画像であるhyper resolution hyperspectral image(HR HSI)を、HR HSIの正解データなしで、広いスペクトルバンド幅で撮影された高解像度画像(HR MSI)と、短いスペクトルバンド幅で撮影された低解像度画像(LR HSI)を用いて生成する手法を提案。
高解像度かつ短いスペクトルバンド幅で写真を撮影することはハードウェア的に困難であり、データセットの構築も難しい。
提案手法ではHR MSIとLR HSIをトレーニングデータとして2つのencoder-decoderを用いる。
HR MSIとLR HSIにはそれぞれ独立のエンコーダーが適用されるが、LR HSIから得られるスペクトル情報を共有するため、
デコーダーは共有する。またスペクトル係数の総和は1という物理的な制約を実現するために潜在変数がディリクレ分布に従うようにする。
また推定されたスペクトルに対し得てスペクトル空間上の角度の差が小さくなるように学習を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Unsupervised_Sparse_Dirichlet-Net_for_Hyperspectral_Image_Super-Resolution.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>CAVE、Harvardデータセットにて検証を行い、state-of-the-artとRMSE、SAM(スペクトル空間のベクトル類似性)比較して最も高い精度を達成。</li><li>教師無し学習が行えた理由として、古くから取り扱われている問題設定であったため、問題の性質をよく知っていたことがあげられる。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.05042">論文</a></li></ul></div></div><div class="slide_index">[#265]</div></div></section><section id="ID_3D_Semantic_Segmentation_with_Submanifold_Sparse_Convolutional_Networks"><div class="paper-abstract"><div class="title">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</div><div class="info"><div class="authors">Benjamin Graham, Laurens van der Maaten, Martin Engelcke</div><div class="conference">CVPR 2018</div><div class="paper_id">1248</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>スパース性が持ったデータ(ポイントクラウドなど)をより効率的で畳み込むsparse convolutional operationsを提案した．また，提案operationsを用いて新たな高次元スパースデータを有効的に処理できるsubmanifold sparse convolutional networks(SSCNs)を提案した．</li><li>従来の問題点：従来のCNNをsparse dataに用いたら計算及びメモリーの効率が良くない問題点がある．また，従来のスパースデータのためのネットワークは主に”full convolution”を行うためスパースデータをdilateしてしまう問題点がある．また，従来のCNNは層が深まることにより，active sitesが大幅に増加してしまうような“submanifold dilation problem”がある．</li><li>以上の様々な問題から，“ネットワークの異なる層で同じレベルのactive sitesのスパース性を保つ”をベースな考えとした新たなconvolution operations:SSCを提案した．こういうような性質から，SSCを用いたらより深い層構造持ったネットワークの学習を可能にした</li><li>具体的なssc：①プーリーングとstrided畳み込み操作と合併②入力のactive sitesだけに対して畳み込みし，active sitesを出力．Ground stateの入力を0と取り扱い畳み込みを廃棄のような設定がある</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SSCN-3D-SemanticSegmentation.png" alt="SSCN-3D-SemanticSegmentation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>提案のSSCがスパース性持ったデータの高効率CNNを可能にした．また，計算量とメモリー消耗の大幅削減及び深い層ネットワークの構築などに用いられる．</li><li>ShapeNetデータセットにおいて，SSCNを用いた3Dシーン及び物体パーツセマンティックセグメンテーションが従来手法(PointCNN,PointNet,Pd-Networkなど含め)より良い精度を達成した．更に，SSCNsの計算効率がより良い</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>論文がとても読みやすかった．しかし想像力が貧乏なので，うまくまとめられない．発表ビデオやコードで具体的なsparse convolutional operations操作を勉強したい</p></li><li><p>ポイントクラウドのようなスパースデータに相性が良いので，SSCNsを用いて３次元処理を行う文章がこれから出てきそう</p></li><li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3108.pdf">論文</a></p></li><li><p><a href="https://github.com/facebookresearch/SparseConvNet">コード</a></p></li></ul></div></div><div class="slide_index">[#266]</div><div class="timestamp">2018.6.7 19:19:22</div></div></section><section id="ID_Im2Struct_Recovering_3D_Shape_Structure_from_a_Single_RGB_Image"><div class="paper-abstract"><div class="title">Im2Struct: Recovering 3D Shape Structure from a Single RGB Image</div><div class="info"><div class="authors">Chengjie Niu, Jun Li, Kai Xu</div><div class="conference">CVPR 2018</div><div class="paper_id">578</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>1枚のRGB画像から3次元形状構造(直方体で物体パーツを表示し，構造をパーツ間の接続性や対称性などの関係で表す)を復元するネットワーク構造を提案した．</li><li>従来1枚のRGB画像からボリューメトリックの復元が広く研究されている．しかし従来の様々な手法より復元された物体はトポロジーや構造が崩れる問題点が多く存在する（特に入力モデルの構造欠損がある場合）．提案手法は画像から形状構造復元を行うため，従来の体積復元の更なる精度向上や3次元形状構造の編集や高レベル画像編集など様々なところに応用できる．</li><li>提案手法のネットワークは①構造マスクを推定するネットワーク②再帰的オートエンコーダーを用いた直方形階層の構造復元ネットワークで構成される．具体的①はskip連結付きなマルチスケールCNNを用いた．②は①の抽出特徴及び元画像の特徴から再帰的なデコーダーを用いた．学習データは3D CADモデルからレンダリング及び構造抽出により作成した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Im2Struct.png" alt="Im2Struct"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>提案手法が初めての1枚RGB画像から詳細3次元形状構造を復元する手法と指摘した．</li><li>提案の形状構造復元手法がパーツ間の連結や対称性など関係の復元を学習するので，復元された形状の構造の妥当性と汎用性が保証できる．</li><li>構造駆動型3次元体積補間及び構造awareなインタラクティブ画像編集の2つのアプリを開発し，提案手法により復元された形状構造の有効性および妥当性を示した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>画像からの3次元形状構造復元がvolume復元と比べパラメータ数が圧倒的少ないので，問題自体の難しさも低い．しかし実応用を考えると，構造復元がかなり応用場面が多いと思う．問題設定がとても良いと思う</p></li><li><p>逆に今までどうしてやる人がなかったのが分からない</p></li><li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3941.pdf">論文</a></p></li></ul></div></div><div class="slide_index">[#267]</div><div class="timestamp">2018.6.7 16:59:09</div></div></section><section id="ID_3D-RCNN_Instance-level_3D_Object_Reconstruction_via_Render-and-Compare"><div class="paper-abstract"><div class="title">3D-RCNN: Instance-level 3D Object Reconstruction via Render-and-Compare</div><div class="info"><div class="authors">Abhijit Kundu, Yin Li, James Rehg</div><div class="conference">CVPR 2018</div><div class="paper_id">436</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>RGB画像からインスタンスレベルの物体full3次元形状及び姿勢を行う”inverse graphics”なend-to-endなネットワーク構造の提案． 物体のカテゴリ検出の結果が与えられたことを仮定し，画像中の物体2次元観測から物体の3次元パラメータの推定を行う．</li><li>提案手法の主な貢献としては①3次元表示：物体の3次元形状がクラス内で共通性が高いことから，大量なCADモデルから低次元なclass-specificな形状priorsを学習する．②2D-3Dマッピングを効率的行える新たなshape,poseの表示を提案した．(例:egocentricではなくallocentric視点を用いるなど)③提案手法を2D監督信号で学習可能にする予測した3次元形状を2次元にレンダリングし2次元のgtと比較することをベースとしたRender-Compareロス関数を提案した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/3D-RCNN-3D-Object-Reconstruction.png" alt="3D-RCNN-3D-Object-Reconstruction"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来のシーン理解は主にシーンに対しセマンティックセグメンテーションや物体検出などを行う．3次元空間のreasoningなどのタスクにおいては3次元のrepresentationが必要となる．また，従来の画像から3次元情報復元に関する研究は主に簡単なシーンから一つの物体に対し推定を行う．提案手法はより複雑なシーンの2次元画像から全部の物体インスタンスに対し3次元情報を推定できるため，自動運転の車・人の3次元情報推定などの様々な複雑なタスクに用いられる．</li><li>ジョイント物体検出と姿勢推定、バウンディングボクス領域内の物体三次元姿勢推定の2つのタスクにおいて，Pascal 3D+,KITTIデータセットでstate-of-the-artな精度を達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>今後”analysis by synthesis”,”inverse graphics”などの概念の引用が増やしそう</p></li><li><p>かなり様々なところで工夫をしている．</p></li><li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Kundu_3D-RCNN_Instance-Level_3D_CVPR_2018_paper.pdf">論文</a></p></li></ul></div></div><div class="slide_index">[#268]</div><div class="timestamp">2018.6.7 16:52:32</div></div></section><section id="ID_Optimizing_Video_Object_Detection_via_a_Scale-Time_Lattice"><div class="paper-abstract"><div class="title">Optimizing Video Object Detection via a Scale-Time Lattice</div><div class="info"><div class="authors">Kai Chen et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>動画中の物体検出において精度とコストの柔軟な trade-off が可能となる Scale-Time Lattice を提案. Propagation and Refinement Unit を用いて時間とスケールについての upsampling を階層的に行う. ImageNet VID dataset を用いた評価実験では先行研究と同等の精度の結果を Realtime で得られた.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Optimizing-Video-Object-Detection-via-a-Scale-Time-Lattice.png" alt="fukuhara-Optimizing-Video-Object-Detection-via-a-Scale-Time-Lattice.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Propagation and Refinement Unit は入力された 2つのフレームの中間の時間のフレームでの推定結果を Motion History Image [Bobick+ 2001] を用いて推定し, その結果をもとにより大きなスケールでの推定を行う.</li><li>Propagation と Refinement を２段階行ったあとは, 残りの全フレームに対して線形補間を行う.</li><li>1段階目の入力となる Keyframe は, まず粗く一様にサンプリングした後, Keyframe 間の Propagation　の容易さ（物体の大きさが小さく, 動きが早いほど難しい）を評価し閾値を超えたら新しい中割りの Keyframe を動的に追加する.</li><li>ImageNet VID dataset を用いた評価実験の結果は 20fps のとき 79.6mAP, 62fps のとき 79.0 fps と先行研究([Feichtenhofer+ 17]が 5fps で 79.8mAP)と同等の高い推定精度を維持したまま Realtime での動作も可能であることが確認された.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.05472" target="blank">[論文] Optimizing Video Object Detection via a Scale-Time Lattice</a></li><li><a href="http://mmlab.ie.cuhk.edu.hk/projects/ST-Lattice/" target="blank">[Project page] Optimizing Video Object Detection via a Scale-Time Lattice</a></li></ul></div></div><div class="slide_index">[#269]</div><div class="timestamp">2018.6.3 14:41:55</div></div></section><section id="ID_Distort-and-Recover_Color_Enhancement_using_Deep_Reinforcement_Learning"><div class="paper-abstract"><div class="title">Distort-and-Recover: Color Enhancement using Deep Reinforcement Learning</div><div class="info"><div class="authors">Jongchan Park et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>強化学習(DQN)を用いて automatic color enhancement を行う研究. 編集後の画像のみを利用して学習を行う方法（distort-and-recover scheme）を提案し, この学習方法の場合は従来の教師あり学習の手法よりも, 強化学習を用いる方が適していることを検証した. また, 評価実験では先行研究と同等か優位な結果を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Distort-and-Recover-Color-Enhancement-using-Deep-Reinforcement-Learning.png" alt="fukuhara-Distort-and-Recover-Color-Enhancement-using-Deep-Reinforcement-Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>color enhancement の工程をマルコフ過程としてモデル化し, 強化学習(DQN)を用いて解いた.</li><li>従来手法のように編集前後の画像の組では無く, 編集後の画像のみを利用して学習を行う方法（distort-and-recover scheme）を提案.</li><li>action は様々な色調整の操作, reward は教師画像に特徴量がどれだけ近づいたかによって計算.</li><li>MIT-Adobe FiveK dataset を用いた評価実験やユーザースタディーでは先行研究と同等か優位な結果を達成した.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.04450" target="blank">[論文] Distort-and-Recover: Color Enhancement using Deep Reinforcement Learning</a></li><li><a href="https://sites.google.com/view/distort-and-recover/" target="blank">[Project Page] Distort-and-Recover: Color Enhancement using Deep Reinforcement Learning</a></li></ul></div></div><div class="slide_index">[#270]</div><div class="timestamp">2018.5.29 16:50:55</div></div></section><section id="ID_W2F_A_Weakly-Supervised_to_Fully-Supervised_Framework_for_Object_Detection"><div class="paper-abstract"><div class="title">W2F: A Weakly-Supervised to Fully-Supervised Framework for Object Detection</div><div class="info"><div class="authors">Yongqiang Zhang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>弱教師ありの物体認識の学習を使用して, 教師あり物体認識を学習を行う研究. 弱教師ありの物体認識は物体中の最も特徴的な領域や, 複数の領域を抽出してしまう傾向があるが, それらの結果から教師データとして最もらしい Pseudo ground-truth を生成する方法を提案.  PASCAL VOC 2007 と 2012 を用いた評価実験では先行研究よりも優位な結果となった.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-W2F-A-Weakly-Supervised-to-Fully-Supervised-Framework-for-Object-Detection.png" alt="fukuhara-W2F-A-Weakly-Supervised-to-Fully-Supervised-Framework-for-Object-Detection.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>WSDNN [Bilen+ 16] の結果を OICR [Tang+ 17] を用いて改善したものを弱教師ありの物体認識の結果として使用. </li><li>上の結果に対して Pseudo ground-truth excavation (PGE) というアルゴリズムを適用することで, 物体全体を囲う Bounding Box を生成する.</li><li>更に, region proposal network [Ren+ 15] を用いて上の結果を改善したものを　Pseudo ground-truth とする.</li><li>Pseudo ground-truth を用いて, Fast RCNN [Girshick 15] や faster RCNN [Ren+ 15] などの教師あり物体認識の手法の学習を行う.</li><li>PASCAL VOC 2007, 2012 を用いて行った評価実験では先行研究 [Tang+ 17] [Krishna+ 16] と比較して mAP に置いて 5% 程度優位な結果となった.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://ivul.kaust.edu.sa/Documents/Publications/2018/W2F%20A%20Weakly-Supervised%20to%20Fully-Supervised%20Framework.pdf" target="blank">[論文] W2F: A Weakly-Supervised to Fully-Supervised Framework for Object Detection</a></li></ul></div></div><div class="slide_index">[#271]</div><div class="timestamp">2018.6.1 23:39:55</div></div></section><section id="ID_Learning_Descriptor_Networks_for_3D_Shape_Synthesis_and_Analysis"><div class="paper-abstract"><div class="title">Learning Descriptor Networks for 3D Shape Synthesis and Analysis</div><div class="info"><div class="authors">Jianwen Xie, Zilong Zheng</div><div class="conference">CVPR 2018</div><div class="paper_id">1093</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>3次元ボリュームデータの形状特徴をモデリングできる深層畳み込みエネルギーベースなdescriptorネットワークを提案した．</li><li>提案の3D DescriptorNetがvoxelized形状の3D形状特徴を抽出できる．具体的には，voxelized形状のprobability density functionを定義した．また，3次元形状を特徴にマッピングできるボトムアップなボリューメトリックConvNetで特徴の統計またはエネルギー関数を定義した．</li><li>提案手法の貢献としては①ボリュームベースな3次元形状特徴をモデリングできる3D DescriptorNetを提案．②提案手法の学習プロセスをモードseeking,shiftingと解釈した．③形状検索に用いられるconditional 3D DescriptorNetを提案した．④3D形状生成モデルの新たな評価メトリクスを提案した．⑤3D GANを代替できる3D cooperative training schemeを提案した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning-descriptor-for-3D-volumetric.png" alt="Learning-descriptor-for-3D-volumetric"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来あまり提案されていないエネルギーベースな3次元形状descriptorを提案した．</li><li>提案の3D DescriptorNetを3次元形状生成，3次元形状検索，3次元形状スーパー解像度，3次元物体認識などタスクにおいて実験を行った．それぞれstate-of-the-artな性能を得られた．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>コードで実際のネットワーク構造を確認したい．</p></li><li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xie_Learning_Descriptor_Networks_CVPR_2018_paper.pdf">論文</a></p></li></ul></div></div><div class="slide_index">[#272]</div><div class="timestamp">2018.6.7 13:16:29</div></div></section><section id="ID_PointGrid_A_Deep_Network_for_3D_Shape_Understanding"><div class="paper-abstract"><div class="title">PointGrid: A Deep Network for 3D Shape Understanding</div><div class="info"><div class="authors">Truc Le, Ye Duan</div><div class="conference">CVPR 2018</div><div class="paper_id">1246</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>3D CNNに用いられる新たな3次元データの表示方法(volumetric grid及びpoints表示をコンバインした表示方法)及び3DCNNネットワークPointGridを提案した．提案の3次元データ表示方法は畳み込みができるregular構造でありながら，ポイントクラウドのローカル幾何情報を抽出できる．</li><li>提案PointGridの処理ポロセスは：①ポイントクラウドを-1,1の区間のユニットボクスに正規化する②cellでユニットボックスを分割し，cellごとのポイント数をKまたは0にダウンサンプリング（増強の場合もある），cell内のKポイントのx,y,zを3チャンネルの特徴として取り扱う．③前述した処理後の表示を3D encoderまたは3D U-Netにより物体識別、パーツセマンティックセグメンテーションに適用する．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/PointGrid.png" alt="PointGrid"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来の3次元表示方法の①occupacy gridやdistance fieldなどはレギュラー構造であるが，3次元形状の近似方法の特徴により低レベルの3次元局所情報しか表示できない，高レベルの特徴を表示するには高解像度が必要だが，CNNに用いたら処理・メモリ―コストが極めて高くなる．②PointNetがポイントクラウドを直接CNN処理を行えるが，max poolingだけでグローバル特徴の抽出を行っているので，局所的な情報抽出が弱い．以上の問題点から， CNN処理を行えるグリッドとポイント表示をコンバインした構造を提案し，occupacy gridより低解像度で豊かな情報を表示でき， PointNetより局所的情報の抽出が強いPointGridを提案した．</li><li>低解像度で有効的に3次元情報を表示できる．例：16，16，16解像度で良い性能を得られる（従来は64，64，64のボリューメトリックグリッド）</li><li>Modelnet-40, shape-netで物体識別及びパーツセグメンテーションの2つのタスクで従来の手法と相当レベルの精度を得られた（ボリューメトリックグリッド方法で最もメモリー消耗が少ない）．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>PointNetの考え方を従来のボリューメトリック方法の解像度削減に利用し，16，16，16解像度でも良い性能を得られるのが魅力的</li><li>提案のPointGridが構造的簡潔でほかのネットワークにも前処理の一部として用いられそう</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Le_PointGrid_A_Deep_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#273]</div><div class="timestamp">2018.6.7 13:10:24</div></div></section><section id="ID_Hybrid_Camera_Pose_Estimation"><div class="paper-abstract"><div class="title">Hybrid Camera Pose Estimation</div><div class="info"><div class="authors">Federico Camposeco, Andrea Cohen, Marc Pollefeys, Torsten Sattler</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>キャリブレーション済みのピンホールカメラにおいてカメラ姿勢推定問題を解く。例としてStructure-from-Motion (SfM)の2D-3Dマッチングを2D-2Dマッチングのように行う問題である。従来は構造ありの2D-3Dマッチングを解く絶対的なカメラ姿勢推定（absolute pose approaches）か、構造なしのテスクチャベースで2D-2Dマッチング（relative pose approaches）を行なっていたが、両者のいいとこ取りをする。本稿では新規にRANSACベースの手法を提案することで繰り返し最適化を行い、同問題の解決に取り組んだ。提案手法は、2D-3D/2D-2Dマッチングを同時にRANSACの要領で繰り返し最適化することができる（図を参照）。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180607HybridCameraPoseEstimation.png" alt="180607HybridCameraPoseEstimation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Structure-based/Structure-lessなマッチング（それぞれ2D-3D/2D-2Dに対応）を同時に解決する手法であるHybrid-RANSACを提案して、SfMの問題に対して適用した。両者のマッチングを単一の枠組みで実装しただけでなく、両者のいいとこ取りができる手法として完成させた。CVPRオーラルとして採択された。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>SfMのことはそこまで詳しくないのだが文章から「凄さ」が伝わってくる論文だった。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Camposeco_Hybrid_Camera_Pose_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#274]</div><div class="timestamp">2018.6.7 09:00:34</div></div></section><section id="ID_MegDet_A_Large_Mini-Batch_Object_Detector"><div class="paper-abstract"><div class="title">MegDet:A Large Mini-Batch Object Detector</div><div class="info"><div class="authors">Chao Peng, et al.</div><div class="paper_id">1711.07240</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>16~256のような大きなバッチサイズでも学習することができる、物体検出手法MegDetの提案。ミニバッチ数を上げられることから、GPUを効率的に使用することができ、学習速度を向上。複数のGPUからうまくバッチ正規化を行う、Cross-GPU Batch Normalizationを提案。これにより、33時間の学習を4時間に短縮、かつ高精度にうまいこと学習できる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180606MegDet.jpg" alt="20180606MegDet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>2018年現在の著名な物体検出アルゴリズム(Faster R-CNNやMask R-CNNなど)は、全体のフレームワークやロスの設計に力を入れている。本研究では、手薄と思われるバッチサイズに着目し，新しいアプローチで精度向上を図っている。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>GPUの性能(メモリ数)の向上に伴って、この研究は生きてくる可能性がある。学習速度を上げながらCOCO2017一位はすごい。</p><ul><li><a href="https://arxiv.org/pdf/1711.07240.pdf">論文</a></li></ul></div></div><div class="slide_index">[#275]</div></div></section><section id="ID_Rotation_Averaging_and_Strong_Duality"><div class="paper-abstract"><div class="title">Rotation Averaging and Strong Duality</div><div class="info"><div class="authors">Anders Eriksson, Carl Olsson, Fredrik Kahl, Tat-Jun Chin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>本稿では非凸問題の一種であるRotation Averagingに対してLagrangian Dualityを用いる。3次元再構成問題において、その画像群が「どこで、どのカメラ角度で、いつ撮影されたか？」に依存して再構成されるモデルが局所最適解に陥るという問題がRotation Averagingである（<a href="http://www.cs.cornell.edu/~bindel//blurbs/rotavg.html">Rotation averaging</a>）。図のようにカメラの移動軌跡やそのカメラアングルが変化した状態だと3次元再構成の局所解は大きく異なる（3次元再構成が表面のみ捉えていることに依存する）。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180607RotationAveraging.png" alt="180607RotationAveraging"><img src="slides/figs/180607RotationAveraging2.png" alt="180607RotationAveraging2"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Structure-from-Motion (SfM)の重要タスクであるRotation Averagingの問題解決についてLagrangian Dualityを用いた全体最適化（局所最適解をできる限りの場面で脱することができた）を行ったことがもっとも大きな新規性である。シンプル/スケーラブルなアルゴリズムであり、大規模空間に対するSfMにも応用可能である。結果は下の図の通りであり、局所最適解を脱してより詳細な形状復元を行うことに成功した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>ディープラーニングを使っていない側の問題！SfMの未解決問題？であるRotation Averagingを高いレベルで改善している。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Eriksson_Rotation_Averaging_and_CVPR_2018_paper.pdf">論文</a></li><li><a href="http://www.cs.cornell.edu/~bindel//blurbs/rotavg.html">Rotation Averaging</a></li></ul></div></div><div class="slide_index">[#276]</div><div class="timestamp">2018.6.7 08:11:11</div></div></section><section id="ID_An_Unsupervised_Learning_Model_for_Deformable_Medical_Image_Registration"><div class="paper-abstract"><div class="title">An Unsupervised Learning Model for Deformable Medical Image Registration</div><div class="info"><div class="authors">Guha Balakrishnan, Amy Zhao, Mert R. Sabuncu, John Guttag, Adrian V. Dalca</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>脳の平均3D形状である脳アトラスの各ボクセルが患者の脳3次元データのどの位置に対応するか、という画像位置合わせ(image registration)をUnetを用いて正解データ無しの教師無し学習で行う手法を提案。
既存手法は最適化ベースだったが、学習ベースの画像位置合わせを初めて提案。トレーニング、検証で使用されているのは脳のMRIデータだが、
他のデータに対する画像位置合わせにも適用することが可能。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/An_Unsupervised_Learning_Model_for_Deformable_Medical_Image_Registration.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>U-netを用いた学習ベースの3次元画像における画像位置合わせ手法を提案。</li><li>比較は最適化ベースの手法である<a href="https://www.sciencedirect.com/science/article/pii/S1361841507000606">SyN</a>と行った。
SyNと同等の精度を達成し、一方で実行時間はCPU上では約160倍、GPU上では更にその156倍の速度で実行可能。</li><li>教師無し学習のため出力された脳アトラスの全体的な形状は異なっているが、各器官の位置はかなり高い精度で推定できていることが驚き。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>選択分野の勝利？手法に新規性は無く、検証で比較した手法も2008年のものとかなり古いが、それでも同等の精度で実行時間が速くなれば、それはCV分野としてはOKと判断されたのか？</li><li><a href="https://arxiv.org/abs/1802.02604">論文</a></li><li><a href="https://github.com/voxelmorph/voxelmorph">GitHub</a></li></ul></div></div><div class="slide_index">[#277]</div></div></section><section id="ID_Recurrent_Scene_Parsing_with_Perspective_Understanding_in_the_Loop"><div class="paper-abstract"><div class="title">Recurrent Scene Parsing with Perspective Understanding in the Loop</div><div class="info"><div class="authors">Shu Kong, Charless Fowlkes</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Hiroaki Aizawa</div><div class="item1"><div class="text"><h1>概要</h1><p>固定解像度で処理する画像認識システムでは、遠近感を持つシーンの画像において物体が任意のスケールを持つことが問題となる。(距離によって物体のスケールが変わる。カメラから遠いほど物体は小さく、近いほど大きい。)これ解決するために、物体のスケール(Depthに反比例)によってPoolingサイズを可変にするdepth-aware pooling moduleを提案。遠くの物体の細部は保持され、近くの物体は大きな受容野を持つことができる。
Depth画像は与えられるか直接RGB画像から推定され、Depth情報と意味的予測を利用するRecurrent Refinement Moduleにより、Semantic Segmentationを反復的に精錬する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Recurrent_Scene_Parsing_with_Perspective_Understanding_in_the_Loop.PNG" alt="Recurrent_Scene_Parsing_with_Perspective_Understanding_in_the_Loop.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>受容野のサイズを変化させるためにDepth情報を利用しこれを自然にCNNに組み込んだこと(geometricな情報を利用する先行研究はあり)。またこのDepth予測をSemantic Segmentationと互いに補い合う用にRecurrent Refinement Moduleを組み込んだこと。NYU-depth-v2の単眼深度推定においてstate-of-the-artな性能とSemantic Segmentationの性能改善を確認。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Recurrent refinement moduleのLoopにより物体の事前情報を捉えることができるが、Loopによる精度変化が小さい。Curriculum Learningと組み合わせるとおもしろそう。ResNetから得られる特徴はすでにスケールを考慮した特徴が抽出できているようにも思え、depth-aware pooling moduleが活かされているかというと疑問。</p><ul><li><a href="https://arxiv.org/abs/1705.07238">論文</a></li><li><a href="https://www.ics.uci.edu/~skong2/recurrentDepthSeg">Project Page</a></li><li><a href="https://github.com/aimerykong/Recurrent-Scene-Parsing-with-Perspective-Understanding-in-the-loop">GitHub</a></li></ul></div></div><div class="slide_index">[#278]</div><div class="timestamp">2018.6.6 19:36:41</div></div></section><section id="ID_Mobile_Video_Object_Detection_with_Temporally-Aware_Feature_Maps"><div class="paper-abstract"><div class="title">Mobile Video Object Detection with Temporally-Aware Feature Maps</div><div class="info"><div class="authors">Mason Liu and Menglong Zhu</div><div class="conference">CVPR2018</div><div class="paper_id">698</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>モバイルや組み込み機器上で低消費電力かつリアルタイムに動作する物体検出のオンラインモデル．Single-Shotベースの物体検出モデルとLSTMを組み合わせたモデルである．また，通常のLSTMよりも計算コストを大幅に削減できるBottleneck-LSTMを提案する．Bottleneck-LSTMは，NチャンネルのBottleneck特徴マップ（Bt）を計算してすべてのゲートの入力をBtに置き換える．これによるゲート内の計算が減る．LSTM自体をDeepな構成にしても標準LSTMより効率的な計算が可能である．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180606_Bottleneck-LSTM1.jpg" alt="20180606_Bottleneck-LSTM1.jpg"><img src="slides/figs/20180606_Bottleneck-LSTM2.jpg" alt="20180606_Bottleneck-LSTM2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来のVideo object detectionはフレームごとの検出に依存しているため，時間的情報を利用することができなかったが，本研究では検出器の速度を犠牲にせず時間的な情報を組み込んだ．ImageNet VID データセットでmobilenet-SSDよりも高精度（54.4mAP）に検出可能でありながら，モバイルCPU（Qualcomm Snapdragon 835, Xperia XZ Premiumなどに搭載）で15FPSの速さで検出できる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Googleでのインターン成果とのこと．リアルタイム検出は時系列情報があれば精度がよくなるが，それを入れることで速度の低下が起きてしまうのでこの2点のトレードオフになっている？</p><ul><li><a href="https://arxiv.org/abs/1711.06368">arXiv</a></li></ul></div></div><div class="slide_index">[#279]</div><div class="timestamp">2018.6.6 12:06:05</div></div></section><section id="ID_Dense_Decoder_Shortcut_Connections_for_Single-Pass_Semantic_Segmentation"><div class="paper-abstract"><div class="title">Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation</div><div class="info"><div class="authors">Piotr Bilinski, Victor Prisacariu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ResNeXtを用いたEncoder-Decoder（エンコーダ-デコーダ）構造、かつシングルパスのセマンティックセグメンテーション手法を提案する。エンコーダとデコーダは折り返したような構造になっており、エンコーダの特徴は図のように対称となる/同じサイズのデコーダ位置に統合される（enc1-dec1が対応）。今回は特にデコーダ側に改善があり、(1)コンテキスト情報を抽出、(2)セマンティック情報を生成、(3)異なる解像度の出力を適宜統合という新規性がある。これを実現するため、DenseNetを参考にしたDense Decoder Shortcut Connectionsを提案し、デコーダにおいてコンテキスト特徴を全て後段に渡すようにした。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180606DenseDecoderShortcut.png" alt="180606DenseDecoderShortcut"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>デコーダにおいてDenseNetを参考にしたDense Decoder Shortcut Connectionsを提案、コンテキスト情報を後段に渡して精度を向上させた。ResNeXtの構造適用と合わせて各データセットにてState-of-the-artな精度を達成。NYUD datasetにて48.1（mean IoU）、CamVid datasetにて70.9（mean IoU）となった。PascalVOC2012においても81.2であった（SoTAはPSPNetの82.6）。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>セマンティックセグメンテーションの覇権争いが激化。ここら辺まで精度が向上すると確率的にSoTAになったりならなかったりする（回す回数が多いと一回くらい精度が高いモデルが学習される）？逆に、学習しやすい（誰が、どんなパラメータで回しても同じくらいの精度が出る）アーキテクチャというのが提案されてもよいかも。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Bilinski_Dense_Decoder_Shortcut_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://github.com/facebookresearch/ResNeXt">ResNeXt(facebookresearch)</a></li><li><a href="https://github.com/liuzhuang13/DenseNet">DenseNet</a></li><li><a href="https://github.com/hszhao/PSPNet">PSPNet</a></li></ul></div></div><div class="slide_index">[#280]</div><div class="timestamp">2018.6.6 09:38:43</div></div></section><section id="ID_Recognize_Actions_by_Disentangling_Components_of_Dynamics"><div class="paper-abstract"><div class="title">Recognize Actions by Disentangling Components of Dynamics</div><div class="info"><div class="authors">Yue Zhao, Yuanjun Xiong, Dahua Lin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>人物行動認識のための表現に対して、モーションとアピアランスの共起表現（Disentangling Components of Dynamics）を提案する。従来の人物行動認識に限らず動画認識ではRGBを入力とするアピアランス、オプティカルフローを画像に投影したフロー画像が用いられていたが、本論文ではそれらの共起表現を新たに提案した。フロー画像とは異なり、特に「アピアランスの変化」をカラー付きで表現できる。さらに、3Dプーリングを提案し、上記３つのチャンネルからの特徴を蓄積する手法についても考案した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180606DisentanglingAction.png" alt="180606DisentanglingAction"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>人物行動認識の文脈において、新規の特徴表現方法であるDisentangling Components of Dynamicsを提案した。同手法はフローとは異なり、RGB値の変化を効果的に捉える方法である。さらに、3Dプーリングも提案し、RGB/Flowも合わせた3チャンネルの特徴を適切にプーリングすることができる。フルモデルを用い、さらにKineticsにて事前学習を行った実験では、95.9%@UCF101を達成、従来の行動認識の大部分よりも高い精度を実現。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Kinetics Datasetの事前学習特徴が（やはり）強い。ImageNetでは91.8%だったものがImageNet+Kineticsで95.9%。転じて、やはりアルゴリズムなどよりもデータを用意するのがもっとも効果的。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Recognize_Actions_by_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://zhaoyue-zephyrus.github.io/">著者</a></li></ul></div></div><div class="slide_index">[#281]</div><div class="timestamp">2018.6.6 09:16:56</div></div></section><section id="ID_Single-Shot_Refinement_Neural_Network_for_Object_Detection"><div class="paper-abstract"><div class="title">Single-Shot Refinement Neural Network for Object Detection</div><div class="info"><div class="authors">Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, and Stan Z. Li</div><div class="conference">CVPR2018</div><div class="paper_id">545</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>SSDをベースにした2つのモジュールから構成されるSingle-shotベースの物体検出アルゴリズム「RefineDet」を提案．Anchor Refine Module (ARM) とObject Detection Module (ODM) と呼ばれるモジュールと，2つを繋いで特徴マップを転送するTransfer Connection Block (TCB) からなる．ARMは物体が存在しない領域を示すNegative Anchor(※)の削減や，Anchorの粗い調整を行う．ODMはTCBを通じて特徴マップを受け取って座標の回帰およびクラス推定を行う．</p><p>※物体候補領域を示すBounding-boxをAnchorと呼ぶ．SSDでDefault boxと呼ばれているものと同じ．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180606_RefineDet1.jpg" alt="20180606_RefineDet1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>SSDで細かい物体をより精度よく検出するために，一度畳み込んだ特徴マップをDeconvしたりUp samplignしたりする手法がいくつかあるが，この手法はTCBで特徴マップを転送するときに1つ前 (=出力側) の特徴マップをDeconvして足している．Single-shotでありながら2つの役割分割されたモジュールがうまく連携している．推論速度は入力320x320で24.8ms (40.3FPS)，512x512で41.5ms (24.1FPS) @TITAN Xと非常に高速である．精度もDSSDより高性能 (VOC2007: 83.8mAP, MSCOCO: 41.8AP)である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Single-Shotベースの物体検出は前層の特徴マップを持ってくる系が流行り？精度も良い．</p><ul><li><a href="https://arxiv.org/abs/1711.06897">arXiv</a></li><li><a href="https://github.com/sfzhang15/RefineDet">GitHub</a></li></ul></div></div><div class="slide_index">[#282]</div><div class="timestamp">2018.6.6 01:36:45</div></div></section><section id="ID_Neural_Kinematic_Networks_for_Unsupervised_Motion_Retargetting"><div class="paper-abstract"><div class="title">Neural Kinematic Networks for Unsupervised Motion Retargetting</div><div class="info"><div class="authors">Ruben Villegas, Jimei Yang, Duygu Ceylan, Honglak Lee</div><div class="conference">CVPR 2018 Oral</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>異なるキャラクタに対するモーションのリターゲティングをRNN、Cycle consisteny lossを用いることで教師なしで学習する手法を提案。RNNのencoder-decoderを用いて入力された関節位置、局所座標の原点の4次元モーションから、
各関節のクォータニオンと局所座標の4次元モーションを出力しそれをForwad Kinematicsによってターゲットキャラクターに転写する。
これを教師なしで行うためにCycle consistency loss、GAN lossを導入する。
これによって同じモーションを持った異なるキャラクタのデータが無い場合にも、モーションのリターゲティングを行うことが可能となる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Neural_Kinematic_Networks_for_Unsupervised_Motion_Retargetting.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>RNNのencoder-decoder、Cycle consistency lossを用いることで同じモーションを持った異なるキャラクタのデータが無い場合にも、モーションのリターゲティングが可能な手法を提案。</li><li>モーションのリターゲティングはオンラインで実行可能。</li><li> <a href="https://www.mixamo.com/#/">Mixamo animation data</a>を用いて、トレーニングは同じモーションを持たない７体のキャラクタの計1646のモーションを使用し、テストには６体のキャラクタを使用した。</li><li>RNN、RNNからrecurrent connectionを削除したMLP、入力モーションを単純にコピーした結果、ablation testを行い推定された関節位置のMSEを比較した結果、提案手法が最も高い精度を達成した。</li><li>特に入力モーションを単純にコピーした場合にはターゲットキャラクタの足が空中に浮いてしまったが、提案手法ではこれを防ぐことに成功している。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>クォータニオンの出力で止めているのは、クォータニオンがスケルトンに不変であることと、ボーンの回転角を制限するロス関数twist lossを取るためだと考えられる。</li><li>異なるキャラクタで同じモーションのGTがあるようなので、教師あり学習との比較を見てみたかった。一方でことモーションに関しては数値的には悪くても見た目では良し悪しがつかないということもあるので、これを考慮したのかもしれない。</li><li>Most of this work was done during Ruben’ internship at Adobe.</li><li><a href="https://arxiv.org/abs/1804.05653">論文</a></li></ul></div></div><div class="slide_index">[#283]</div></div></section><section id="ID_Cross-Domain_Weakly-Supervised_Object_Detection_through_Progressive_Domain_Adaptation"><div class="paper-abstract"><div class="title">Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation</div><div class="info"><div class="authors">Naoto Inoue, Ryosuke  Furuta, Toshihiko Yamasaki, Kiyoharu Aizawa</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>インスタンスレベルのアノテーションを持つソースドメイン(S)とイメージレベルのアノテーションを持つターゲットドメイン(T)を用いてdomain adaptationを行い、Tに対する物体検出を行う手法を提案。Sを用いて物体検出器のプリトレーニングを行い、
Cycle GANによってSをTに変換した画像を用いて物体検出器のfine-tuningを行う。
続いてSとそのイメージレベルのアノテーションを用いて半教師学習を行いSに対する物体検出を行う。
半教師学習を行う際にインスタンスレベルのアノテーションが施されたデータセットが必要なため、
クリップアート、水彩画、漫画のデータセットの構築も行っている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Cross-Domain_Weakly-Supervised_Object_Detection_through_Progressive_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Cycle GANによる検出器のfine-tuning、半教師学習による物体検出というステップをへてイメージレベルのアノテーションを持つ実画像ではないドメイン(クリップアートなど)に対する物体検出手法を提案。</li><li>Clipart1k, Watercolor2k, Comic2kという、それぞれクリップアート1000枚、水彩画2000枚、漫画2000枚の画像に対してインスタンスレベルのアノテーションを施したデータセットを構築。</li><li>自ら構築した三種のデータセットにおいて教師なし学習、半教師学習、SSD300、YOLOv2と比較した結果、最も高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>検証しているラベル数が最大でも20と少ないことが気になった。これはターゲットドメインの構築が難しかったからであり、データさえあればラベルを増やすことができるのだろうか？</li><li><a href="https://arxiv.org/abs/1803.11365">論文</a></li><li><a href="https://naoto0804.github.io/cross_domain_detection/">Project page</a></li><li><a href="https://github.com/naoto0804/cross-domain-detection">GitHub       </a></li></ul></div></div><div class="slide_index">[#284]</div></div></section><section id="ID_Real-Time_Monocular_Depth_Estimation_Using_Synthetic_Data_With_Domain_Adaptation_via_Image_Style_Transfer"><div class="paper-abstract"><div class="title">Real-Time Monocular Depth Estimation Using Synthetic Data With Domain Adaptation via Image Style Transfer</div><div class="info"><div class="authors">Amir Atapour-Abarghouei, Toby P. Breckon</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>合成画像とそのデプス画像、そして実世界画像を用いてunsupervised domain adaptationを行うことで、実世界画像に対するデプス画像を生成する手法を提案。
実世界画像に対するデプスのアノテーションは困難であり、かつ枚数も多くない。
一方合成画像に対するデプスのアノテーションは完璧だが、
実世界画像に対する推定を行うときにドメインシフトが起きてしまう。
提案手法ではUnetによって合成画像からデプスを推定し、Cycle GANによって実世界画像を合成画像に変換することでデプスを推定する手法を提案。
GPUを用いることで44FPSで実行することが可能。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Real-Time_Monocular_Depth_Estimation_Using_Synthetic_Data_With_Domain_Adaptation_via_Image_Style_Transfer.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ラベルなし実世界画像とラベルあり合成画像に対してCycle GANによるスタイルトランスファーによりdomain adaptaionを行うことで、実世界画像のデプスを推定する手法を提案。</li><li>合成画像、KITTIデータセットでトレーニングを行い、KITTIデータセットの推定精度をstate-of-the-artと比較した結果、最も高い精度を達成。</li><li>Cycle GANによるスタイルトランスファーでは急激な照明変化や影を物体として認識してしまうといったリミテーションが存在する。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>Cycle GANによってdomain adaptationを行う割合ベーシックな手法だが、その推定精度がstate-of-the-artに優っている。</li><li><a href="http://breckon.eu/toby/publications/papers/abarghouei18monocular.pdf">論文</a></li><li><a href="http://dro.dur.ac.uk/24333/">Project page</a></li><li><a href="https://vimeo.com/260393753">Vimeo</a></li></ul></div></div><div class="slide_index">[#285]</div></div></section><section id="ID_Unsupervised_Domain_Adaptation_with_Similarity_Learning"><div class="paper-abstract"><div class="title">Unsupervised Domain Adaptation with Similarity Learning</div><div class="info"><div class="authors">Pedro Pinheiro</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ソースドメイン(S)の各カテゴリの重心ベクトルと、S・ターゲットドメイン(T)から得られたadversarial featuresの行列積を用いることでdomain adaptation(DA)を行う手法を提案。
従来のDAではSとTのそれぞれから得られる特徴量をGANによってdomai-confusionを行い、
Sで学習したラベル識別器をTに適用するという手法だった。提案手法ではadversarial-confusionに加えて、
Sの各カテゴリにおける重心ベクトルとgeneratorから得られる特徴量の類似度を高くするように学習しDAを行う手法を提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Unsupervised_Domain_Adaptation_with_Similarity_Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>domain-confusionに加えてラベルごとの重心ベクトルとgeneratorから得られる特徴量の類似度を高くするように学習しDAを行う手法を提案。</li><li>MNIST・USPS・MISNT-M、Officde-31, VisDAデータセットで検証。11のdomain adaptationにおいて、9つの設定においてstate-of-the-artよりも高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>この論文に限らずDAを提案する論文ではdomain-confusionを可視化しており、数値評価だけではなく、ドメインの分布の可視化画像を載せることも重要だと思われる。</li><li><a href="https://arxiv.org/abs/1711.08995">論文</a></li></ul></div></div><div class="slide_index">[#286]</div></div></section><section id="ID_Image-Image_Domain_Adaptation_with_Preserved_Self-Similarity_and_Domain_Dissimilarity_for_Person_Re-identification"><div class="paper-abstract"><div class="title">Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification</div><div class="info"><div class="authors">Weijian Deng, Univ. of Chinese Academy; Liang Zheng, Qixiang Ye, Guoliang Kang, Yi Yang, Jianbin Jiao</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>人物認証(person re-ID)の精度が落ちないようにソースドメインの人物画像をターゲットドメインの画像に変換するSimilarity Preserving GAN(SPGAN)を提案。ドメイン間の変換をCycleGANで行う。
またそれぞれのperson re-IDのデータセットには基本的に同じ人物は写っていないということを利用して、
ソースドメインとターゲットドメインで異なるデータセットを使用し、
ターゲットドメインへと変換された画像はIDが保たれ、かつターゲットドメインのどの人物のIDとも一致しないように学習を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Image-Image_Domain_Adaptation_with_Preserved_Self-Similarity_and_Domain_Dissimilarity_for_Person_Re-identification.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>person re-IDデータセットの特徴を生かしドメイン変換された画像はターゲットドメインの人物画像とは一致せず、かつ元々のIDを生かすように学習を行い、ドメイン間で人物画像の変換を行うSPGANを提案。</li><li>Market-1501、Duke-MTMC-reIDデータセットで検証を行い、一方のデータセットの人物画像をもう一方のドメイン画像に変換した際に正しくre-IDができるのかを検証した。</li><li>ベースラインであるCycleGANや教師なし学習のstate-of-the-artと比較して最も高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>person re-IDのタスクの中でもソースドメインの人物がターゲットドメインに存在する場合にも発見する、というタスクを解いている。</li><li>ソースドメインとターゲットドメインに含まれるIDが全く違う、ということを逆手にとった手法。</li><li><a href="https://arxiv.org/abs/1711.07027">論文</a></li></ul></div></div><div class="slide_index">[#287]</div></div></section><section id="ID_Boosting_Domain_Adaptation_by_Discovering_Latent_Domains"><div class="paper-abstract"><div class="title">Boosting Domain Adaptation by Discovering Latent Domains</div><div class="info"><div class="authors">Massimiliano Mancini , Lorenzo Porzi, Samuel Rota Bulò, Barbara Caputo, Elisa Ricci</div><div class="conference">CVPR 2018 Spotlight</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>domain adaptaion(DA)に対して、ソースデータは潜在的に複数のドメインで構成されていると仮定し、ソースサンプルがどのドメインに所属しているかを精度よく識別するためにMulti-domain DA layer(mDA-layer)を導入することで、
ターゲットのラベルの識別精度を向上させる手法を提案。
実験ではmulti-soure domain adaptationを行うことでその有効性を検証している。
ソースデータないのドメインを識別するCNNの特徴量を用いることで、ターゲットドメインのラベル識別の精度が向上している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Boosting_Domain_Adaptation_by_Discovering_Latent_Domains.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>mDA layerによってマルチソースドメイン内のドメインを識別する学習を行うことで、ターゲットドメインのラベル識別に有効な特徴量を獲得。</li><li>MNIST・MISNT-m・USPS、Office-31、Office-Caltech、PACSデータセットで提案手法の有効性を検証。state-of-the-artのmulti-source domain adaptation(DA)よりも高い精度を達成。</li><li>ソースサンプルにドメインのラベルが全くない場合とラベルがない場合でも、精度は1%ほどしか変わらない。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1805.01386">論文</a></li></ul></div></div><div class="slide_index">[#288]</div></div></section><section id="ID_Large_Scale_Fine-Grained_Categorization_and_the_Effectiveness_of_Domain-Specific_Transfer_Learning"><div class="paper-abstract"><div class="title">Large Scale Fine-Grained Categorization and the Effectiveness of Domain-Specific Transfer Learning</div><div class="info"><div class="authors">Yin Cui Yang Song, Chen Sun, Andrew Howard, Serge Belongie</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>鳥の種族などより細かいラベルを推定するdomain-specific fine-grained visual categorization(FGVC) taskにおいて、効果的なトレーニングデータセットの構築方法を提案。
事前実験からターゲットドメインの画像の見た目に近い画像を含むソースドメインでトレーニングするほど、
識別精度が高くなるということを発見している。
ターゲットドメインに含まれる画像の見た目に近い画像を多く持つソースドメインのクラスをいくつか選択することで
トレーニングデータセットを構築する。画像の見た目はEarth Mover’s Distanceで測定され、
7つのfine-grainedデータセットにおいて提案手法が効果的であることを示した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Large_Scale_Fine-Grained_Categorization_and_the_Effectiveness_of_Domain-Specific_Transfer_Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>FGVCを行う際のトレーニングスキームとして、ImageNetのような大規模データセットやクラスごとのデータ数が偏っているiNatを学習するのではなく、
より効果的なトレーニングデータセットを構築する手法を提案。</li><li>fine-grainedデータセットCUB200、Stanford Dogs、Flower-102、Stanford Cars、Aircraft、Food101、NABirdsで検証した結果、5つのデータセットにおいて提案手法によって構築されたトレーニングデータセットで
学習した場合に最も高い精度を達成。</li><li>classificationで使用したネットワークはResNet、Inception、Squeeze-and-Excitationであり識別ネットワーク自体には依存しないことも検証している。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>手法自体は単純ながら、事前実験に基づく論文展開や既存手法に対して投げかけた疑問を回収できたところが評価されたと思われる。</li><li><a href="https://vision.cornell.edu/se3/wp-content/uploads/2018/03/FGVC_CVPR_2018.pdf">論文</a></li></ul></div></div><div class="slide_index">[#289]</div></div></section><section id="ID_Residual_Parameter_Transfer_for_Deep_Domain_Adaptation"><div class="paper-abstract"><div class="title">Residual Parameter Transfer for Deep Domain Adaptation</div><div class="info"><div class="authors">Artem Rozantsev, Mathieu Salzmann, Pascal Fua</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ソースドメインを学習したネットワークのパラメタを残差ブロックで変換することでターゲットドメインへdomain adaptaionを行う手法を提案。
既存手法ではドメインに普遍な特徴量を学習していたためにネットワークのパラメタが多すぎてしまう。
提案手法は学習時には残差ブロックとソースドメインを学習するネットワークのファインチューニングを行い、
ソースドメインに対するラベルの識別と2つのドメインに対してadversarial domain adaptationを行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Residual_Parameter_Transfer_for_Deep_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ドメインに普遍な特徴量を学習するのではなく、ソースドメインを学習したネットワークの重みをソースドメイン用に変換することでパラメタ数を抑えかつ精度の高い
domain adaptationを実現。</li><li>state-of-the-artと比べて、SVHN・MNIST、UAV-200データセット、Officeデータセットにおいてもっとも高い精度を達成。</li><li>ソースドメインを学習するネットワークがResNetのような深いネットワークの場合にも有効であることを主張。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.07714">論文</a></li></ul></div></div><div class="slide_index">[#290]</div></div></section><section id="ID_Importance_Weighted_Adversarial_Nets_for_Partial_Domain_Adaptation"><div class="paper-abstract"><div class="title">Importance Weighted Adversarial Nets for Partial Domain Adaptation</div><div class="info"><div class="authors">Jing Zhang, Zewei Ding, Wang Ding, Wanqing Li, Philip Ogunbona</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ターゲットドメインがソースドメインが所持するクラスの一部しか持たずかつラベルがない場合であるpartial domain adaptationをadversarial netベースで行う手法を提案。
adversarila netの手前いにドメインを識別するclassifierを用意し、
このclassifierが精度良く判別可能なソースサンプルはターゲットドメインには含まれていないクラスに所属している可能性が高いので重みを小さくし、
逆にconfidenceが低いソースサンプルはターゲットにも存在するクラスに所属している可能性が高いので重みを大きくする。
この重みとソースサンプルを掛け合わせたものとターゲットサンプルをadversarial netで学習させる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Importance_Weighted_Adversarial_Nets_for_Partial_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>4つのドメインを持つOffice+Caltech-10において、ソースは各ドメインで10のラベル、ターゲットは各ドメインで5つのラベルを使用。同様の設定でOffice-31データセット、Caltech256→Office10データセットで実験を行った。</li><li>partial domain adaptationのstate-of-the-artである<a href="https://arxiv.org/abs/1707.07901">SAN</a>と比較して8つの実験のうち4つの設定でより高い精度を達成。</li><li> <a href="https://arxiv.org/abs/1707.07901">SAN</a>ではソースのクラスの数だけclassifierを必要とするが、提案手法で必要なclassifierは2つのみ。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.09210">論文</a></li></ul></div></div><div class="slide_index">[#291]</div></div></section><section id="ID_Domain_Generalization_with_Adversarial_Feature_Learning"><div class="paper-abstract"><div class="title">Domain Generalization with Adversarial Feature Learning</div><div class="info"><div class="authors">Haoliang Li, Sinno Jilain Pan, Shiqi Wang, Alex Kot</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>Adversarial Autoencoder(AAE)に対してMaximum Mean Discrepancy(MMD)を導入することでトレーニングデータを過学習することなくdomain generalizationを行う手法を提案。
domain generalizationとは、複数ドメインのラベル付きデータセットを学習し、
テスト時にはデータセットに含まれていないドメインのデータセットにおける識別や生成タスクを行うことを指す。
複数のソースドメインで不変な特徴量を取得する<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Ghifary_Domain_Generalization_for_ICCV_2015_paper.pdf">multi-task learning</a>に対して、提案手法ではMMDベースでドメイン間の差分をとることと、
AAEによって特徴量空間に対して事前分布が押し込むことでソースドメインに対する過学習が防ぐ。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Domain_Generalization_with_Adversarial_Feature_Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>AAEに対してMMDを組み込むことで、ソースドメインを過学習することなくdomain generalizationを行う。</li><li>domain generalizationのstate-of-the-artと識別タスクにおいて比較。</li><li>MNISTを15度刻みで回転させた場合の認識精度、VLCSデータセットにおける物体認識、IXMASにおける行動認識においてstate-of-the-artよりも高い精度を達成。</li><li>AAEにおける事前分布の違いによる精度も議論しており、ラプラシアン分布が最も精度が良かったと主張。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://www.researchgate.net/publication/324691022_Domain_Generalization_with_Adversarial_Feature_Learning">論文</a></li></ul></div></div><div class="slide_index">[#292]</div></div></section><section id="ID_Adversarial_Feature_Augmentation_for_Unsupervised_Domain_Adaptation"><div class="paper-abstract"><div class="title">Adversarial Feature Augmentation for Unsupervised Domain Adaptation</div><div class="info"><div class="authors">Riccardo Volpi, Pietro Morerio, Silvio Savarese, Vittorio Murino</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>特徴量空間におけるデータオーギュメンテーションとソースドメインとターゲットドメインに不変な特徴量を取得することでunsupervised data adaptationを行う手法を提案。
右図にあるようにstep1で、ソースドメインとノイズをデコードして生成されたベクトルをGANにかけ、
特徴量空間においてソースドメインに対するオーギュメンテーションを行う。
続いてstep2において、ソースドメインとターゲットドメインを同一のエンコーダーに入力することでドメインに不変な特徴量を取得する。
ベースラインである<a href="https://arxiv.org/abs/1702.05464">Adversarial discriminative domain adaptation</a>ではドメインごとにエンコーダーを使用していたが、提案手法ではエンコーダーは一つ。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Adversarial_Feature_Augmentation_for_Unsupervised_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>GANを用いてソースドメインの特徴量空間でデータオーギュメンテーションを行い、かつソースドメインとターゲットドメインに不変な特徴量を推定することで、unsupervised data adaptationを行った。</li><li>ベースラインである<a href="https://arxiv.org/abs/1702.05464">Adversarial discriminative domain adaptation</a>に対して上記の2つの拡張の有効性を議論している。</li><li>state-of-the-artと比較して、数字の識別、物体の識別において既存手法と同等かそれ以上の精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>Limitationにも書かれているようにsourceとtargetのラベが同じになる保証はなく、最終的な精度はsourceのエンコーダーがどれほどうまく学習できているかに強く依存する。</li><li><a href="https://arxiv.org/abs/1711.08561">論文</a></li><li> <a href="https://github.com/ricvolpi/adversarial-feature-augmentation">GitHub</a></li></ul></div></div><div class="slide_index">[#293]</div></div></section><section id="ID_Dynamic_Video_Segmentation_Network"><div class="paper-abstract"><div class="title">Dynamic Video Segmentation Network</div><div class="info"><div class="authors">Yu-Syuan Xu, Tsu-Jui Fu, Hsuan-Kung Yang, Chun-Yi Lee</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>動画像セグメンテーションの問題に対してネットワーク選択（Decision Network）を行い適応的にCNNモデルを処理するDynamic Video Segmentation Network (DVSNet)を提案する。同手法では性質の異なるふたつのネットワーク（深くて精度が高いが低速/浅くて精度は低いが高速）を組み合わせて交通シーンにおけるシーン解析にて高速な処理を実現する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180605DynamicVideoSegmentationNetwork.png" alt="180605DynamicVideoSegmentationNetwork"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>DVSNetは低速なもので70.1%/20fps、高速なものだと65.2%/34.4fps（いずれもCityScapes datasetにて処理した結果）を達成する。両者を、トレードオフを考慮してあらゆる場面に適応することができるという意味で新規性がある。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>こういう通し方があったのか、と勉強になる。実利用を想定し、トレードオフを考慮、それを解決することも重要な問題である。</p><ul><li><a href="https://arxiv.org/abs/1804.00931">論文</a></li><li><a href="https://challengerocket.com/files/photo_projects/he/hell/hellochick/-sadasd_4f5749/31bc7a20.pdf">論文（査読ver.）</a></li></ul></div></div><div class="slide_index">[#294]</div><div class="timestamp">2018.6.5 21:14:03</div></div></section><section id="ID_Deep_Cross-media_Knowledge_Transfer"><div class="paper-abstract"><div class="title">Deep Cross-media Knowledge Transfer</div><div class="info"><div class="authors">Xin Huang, et al.</div><div class="paper_id">1803.03777</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>画像とテキストなどの異なるメディアタイプ間で検索する、クロスメディア検索手法のcross-media knowledge transfer(DCKT)の提案。大規模なクロスメディアデータセットの知識を、小規模なデータセットのモデルに転移学習する。メディアレベルと相関性レベルでのドメインの違いを最小化するために、2レベルでドメイン変換することで精度向上。また、ドメインの違いを徐々に減らすようにトレーニングサンプルを選択することで、モデルがより頑健になる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180605_DCKT.jpg" alt="20180605_DCKT.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>マルチメディア分野における検索。既存の手法では、ラベル付きデータを学習する方法が多いが、大規模なデータの収集とラベル付けは手間取るため問題とされる。そこで、既存のデータを転移して解決する。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.03777.pdf">論文</a></li></ul></div></div><div class="slide_index">[#295]</div></div></section><section id="ID_Dynamic_Graph_Generation_Network_Generating_Relational_Knowledge_from_Diagrams"><div class="paper-abstract"><div class="title">Dynamic Graph Generation Network: Generating Relational Knowledge from Diagrams</div><div class="info"><div class="authors">Daesik Kim, et al.</div><div class="paper_id">1711.09528</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>視覚情報とテキストの情報が抽象的に統合された図であるダイアグラムを解析するためのunified diagram parsing network(UDPnet)の提案。入力は様々なイラストやテキスト、レイアウトを持つ図のみ。物体検出器によって、図内のグラフ構造を推論し、新手法であるdynamic graph generation network(DGGN)によってグラフを生成。生成されたグラフからテキストで関係性を出力する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180605_DGGN.jpg" alt="20180605_DGGN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>ダイアグラムのような図には、豊富な知識が含まれているが、固有の特性やレイアウトの問題から、コンピュータに自動的に理解させる方法はあまり提案されていない。本手法では、物体検出器やRNNを統合し、ダイアグラムから知識をテキストとして生成する。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>自然画像でなく，人間による作為的なグラフ理解において優れている。人間の意図や、人間にとって自然な解釈を学習できているのではないか。</p><ul><li><a href="https://arxiv.org/pdf/1711.09528.pdf">論文</a></li></ul></div></div><div class="slide_index">[#296]</div></div></section><section id="ID_Instance_Embedding_Transfer_to_Unsupervised_Video_Object_Segmentation"><div class="paper-abstract"><div class="title">Instance Embedding Transfer to Unsupervised Video Object Segmentation</div><div class="info"><div class="authors">Siyang Li, Bryan Seybold, Alexey Vorobyov, Alireza Fathi, Qin Huang, C.-C. Jay Kuo</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>物体インスタンス特有の特徴（同じ物体領域に属しているか？）を捉えることでビデオに対する教師なしの物体セグメンテーションを実施する。ここでは静止画で捉えた特徴を、ビデオに表れる物体候補/オプティカルフローと組み合わせて物体のインスタンスセグメンテーションを実施。本論文ではさらに、ビデオに対するfine-tuningなしに高精度なセグメンテーション手法を構築したと主張している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180605InstanceEmbeddingTransfer.png" alt="180605InstanceEmbeddingTransfer"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>静止画の学習パラメータを動画に適用していく、その際に物体候補/オプティカルフローと統合していくことで動画的な表現を教師なしで獲得していく。DAVIS datasetを用いた評価で78.5%、FBMS datasetにて71.9%（いずれもmean Intersection-over-Union (mIoU)の評価にて）を達成し、それぞれのデータセットでState-of-the-art。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>"Without finetuning"というのもアピールになるということを勉強した（ただしそれでstate-of-the-artである必要がある？）。</p><ul><li><a href="https://arxiv.org/abs/1801.00908">論文</a></li><li><a href="https://github.com/philferriere/tfvos">GitHub(Semi-Supervised Object Segmentation)</a></li></ul></div></div><div class="slide_index">[#297]</div><div class="timestamp">2018.6.5 08:58:32</div></div></section><section id="ID_Depth-Aware_Stereo_Video_Retargeting"><div class="paper-abstract"><div class="title">Depth-Aware Stereo Video Retargeting</div><div class="info"><div class="authors">Bing Li, Chia-Wen Lin, Boxin Shi, Tiejun Huang, Wen Gao, C.-C. Jay Kuo</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ステレオビデオ（Stereo Video）に対するリターゲティング（Retargeting）を扱う。ステレオ（かつビデオ）に対するリターゲティングは従来のリターゲティングと比較すると、動画中の顕著性が高い物体の把握やダイナミクスを含むためまだ新しくチャレンジングな課題である。ここに対して、Depth-aware Fidelity Constraint（距離画像から推定される信頼性のようなもの）を適用することで物体の顕著性を把握しつつ3次元空間を再構成することができる（リターゲティングと3次元再構成の同時推定問題）。最適化にはTotalCost関数を適用して物体の顕著性を把握しつつ形状、時間情報、距離画像のディストーションを推定。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180605DepthRetargeting.png" alt="180605DepthRetargeting"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ステレオビデオの入力から、顕著性の把握、形状推定、時間情報、距離画像のディストーションを同時推定し、従来法であるCVWよりも綺麗なリターゲティング画像を生成することに成功した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>VR/AR、3D映画などに使える！より自然に見せることで映像酔いを軽減することができる？</p><ul><li><a href="http://alumni.media.mit.edu/~shiboxin/files/Li_CVPR18.pdf">論文</a></li><li><a href="http://alumni.media.mit.edu/~shiboxin/">著者</a></li><li><a href="https://news.mynavi.jp/article/computer_vision-43/">リターゲティング（マイナビ記事）</a></li><li><a href="https://ieeexplore.ieee.org/document/7055924/">CVW（従来手法）</a></li></ul></div></div><div class="slide_index">[#298]</div><div class="timestamp">2018.6.5 08:23:28</div></div></section><section id="ID_Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data"><div class="paper-abstract"><div class="title">Frustum PointNets for 3D Object Detection from RGB-D Data</div><div class="info"><div class="authors">Charles R. Qi, et al.</div><div class="paper_id">1711.08488</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>屋内および屋外シーンにおける3D物体検出手法のfrustum PointNetsの提案。まず、RGBデータからCNNで2Dの物体候補領域を推定する。次に、点群の深度情報を用いて、各物体領域の視錐台(viewing frustum)を推定する。最後に、frustum PointNetsによって3Dバウンディングボックスを推定。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180604_PointNets.jpg" alt="20180604_PointNets.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>従来の手法では、画像や3Dボクセルに処理を加えて、3Dデータの自然なパターンや不変性を曖昧にしている。本手法では、RGB-Dスキャンによって生の点群データを直接操作する。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>2Dと3Dで別々のネットワークを使うことで、小さな物体やオクルージョン、まばらな点群についても正確に推定することができる。リアルタイムも実現。</p><ul><li><a href="https://arxiv.org/pdf/1711.08488.pdf">論文</a></li></ul></div></div><div class="slide_index">[#299]</div></div></section><section id="ID_Dynamic_Zoom-in_Network_for_Fast_Object_Detection_in_Large_Images"><div class="paper-abstract"><div class="title">PhaseNet for Video Frame Interpolation</div><div class="info"><div class="authors">Mingfei Gao, et al.</div><div class="paper_id">1711.05187</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>高解像度画像に出現する様々なサイズの物体を、精度の維持と処理コストの低減を実現しながら検出するフレームワークの提案。最初はダウンサンプリングされた粗い画像から、次に高解像度の細かい画像から検出する。強化学習を用いた2つのネットワークで構成。R-net：低解像度の画像を入力し、その検出結果を用いて高解像度領域を解析する。これにより、どの順番にズームインすべき判断できる。Q-net：ズームの履歴を使用し、拡大領域を順次選択。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180604Dynamic_Zoom-in.jpg" alt="180604Dynamic_Zoom-in.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>しっかり検出する範囲を絞ることで処理量を低減、効率化を図ることができる。基本的な検出の構造はいじっていない。処理する画素数を約70％、処理時間を50％以上短縮し、なおかつ高い検出性能を維持できる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>YOLOやSSDなどの物体検出手法の精度向上にも使える。</p><ul><li><a href="https://arxiv.org/pdf/1711.05187.pdf">論文</a></li></ul></div></div><div class="slide_index">[#300]</div></div></section><section id="ID_Efficient_Video_Object_Segmentation_via_Network_Modulation"><div class="paper-abstract"><div class="title">Efficient Video Object Segmentation via Network Modulation</div><div class="info"><div class="authors">Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang, Aggelos K. Katsaggelos</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>セグメンテーションを実行する際に任意のアノテーション済み物体を事前情報（Spatial Prior）として高精度化を図るための技術を提供する。本論文では、最初の一フレームに対してセグメンテーションを行うだけで、動画中の物体に対してセグメンテーションを行うモデルを提案する。アノテーションから抽出した事前情報はニューラルネットの中間層にて情報を挿入して抽象化を行う。図は提案のフレームワークを示しており、VisualModulator（初期フレームのアノテーションから視覚的なガイドを行う）、SegmentationNet（VisualModulator/SpatialModulatorの補助を受けつつ、RGB画像の入力からセグメンテーションを実行）、SpatialModulator（空間的にどこらへんに対象物体があるかをサポート）の３つのコンポーネントから構成される。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180604NetworkModulation.png" alt="180604NetworkModulation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>最初のフレームのアノテーションのみから動画セグメンテーションを実行するという問題を提供した、さらに視覚的な特徴量/位置的な事前知識をセグメンテーションのネットワークに導入し、動画セグメンテーションを高精度化した点が評価された。動画セグメンテーションタスクであるDAVIS2016にて74.0、YoutubeOjbsにて69.0（処理速度は0.14second/image）であった。State-of-the-artには劣る（それぞれ79.8, 74.1）が、処理速度では優っている（提案 0.14 vs. 従来 10.0）。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>メタ学習の枠組みを使用している。</p><ul><li><a href="https://arxiv.org/abs/1802.01218">論文</a></li><li><a href="https://github.com/linjieyangsc/video_seg">GitHub</a></li></ul></div></div><div class="slide_index">[#301]</div><div class="timestamp">2018.6.4 20:56:17</div></div></section><section id="ID_Real-world_Anomaly_Detection_in_Surveillance_Videos"><div class="paper-abstract"><div class="title">Real-world Anomaly Detection in Surveillance Videos</div><div class="info"><div class="authors">Waqas Sultani, Chen Chen, Mubarak Shah</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>監視カメラの文脈において異常検出を実行する研究である。ここで、異常検出においてビデオに対して時間のアノテーションを付与するのは非常にコストのかかる作業であるが、ここに対して弱教師付き学習の一種であるMultiple Instance Learning (MIL)を適用して正常/異常ラベルが付いたビデオから異常検出を行うモデルDeep Anomaly Ranking Modelを提案する。さらに、13種類の異常シーン（e.g. road accident, robbery）を収集したデータセットを提供することで同問題の解決を実践した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180604AnomalyDetection.png" alt="180604AnomalyDetection"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>弱教師付き学習であるMILをベースとして異常検出を行なった、おそらく初めての例であり、その精度は従来法による精度を上回りState-of-the-artとなった（AUCにて75.41を達成）。また、1900の動画に対して13種類の異常を収集したデータセットを構築し、公開した。同データセットは合計で128時間にも及ぶ。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>異常の動画データセットを公開したことが評価できるポイント。現在ではYouTube検索とダウンロードである程度のデータセットは構築できそう？（ここらへんを効率化する研究自体があってもよい）</p><ul><li><a href="https://arxiv.org/abs/1801.04264">論文</a></li><li><a href="http://crcv.ucf.edu/cchen/">Project</a></li><li><a href="http://crcv.ucf.edu/projects/real-world/">DB</a></li></ul></div></div><div class="slide_index">[#302]</div><div class="timestamp">2018.6.4 20:22:54</div></div></section><section id="ID_Normalized_Cut_Loss_for_Weakly-supervised_CNN_Segmentation"><div class="paper-abstract"><div class="title">Normalized Cut Loss for Weakly-supervised CNN Segmentation</div><div class="info"><div class="authors">M. Tang, A. Djelouah, F. Perazzi, Y. Boykov, C. Schroers</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>Weakly-supervisedなセマンティックセグメンテーション手法があって，その方針はインタラクティブに部分的に正解（シードとか）を与えるというものである．そこで，よく用いられるロス関数（クロスエントロピー等）で評価しようとすると，教示の塗りミスが致命的になったりする．そもそも設計的にエラーが考慮されていないからである．</p><p>本論文では，非Deepな手法で行われていた評価指標に基づく新たなロス関数Normalized Cut Lossを提案．</p><p>従来法と違うところは，提案するロス関数におけるクロスエントロピーの部分は，ラベルが既知のシードの部分での評価だけやっているという点．<a href="http://slideplayer.com/slide/6125890/18/images/29/Example+Normalized+Cut.jpg">Normalized Cut</a>はゆるく全ピクセルに対する一貫性の評価を行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Normalized_Cut_Loss_for_Weakly-supervised_CNN_Segmentation.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Fully-supervisedな手法と同レベルの性能を実現できた．</p><p>従来法の知見を活かした橋渡し的手法．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Disney Researchのインターンでやった模様．</p><ul><li><a href="https://arxiv.org/abs/1804.01346">arXiv</a></li></ul></div></div><div class="slide_index">[#303]</div><div class="timestamp">2018.6.4 12:33:29</div></div></section><section id="ID_Burst_Denoising_with_Kernel_Prediction_Networks"><div class="paper-abstract"><div class="title">Burst Denoising with Kernel Prediction Networks</div><div class="info"><div class="authors">B. Mildenhall, J.T. Barron, J. Chen, D. Sharlet, R. Ng, R. Carroll</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>携帯含む最近のカメラは連写機能が付いているので，手ブレのあるようなハンドヘルドカメラの連写で撮ったノイズ入り画像をデノイズしようという話．<strong>連続撮影における手ブレに頑健なデノイズCNN</strong>を提案する．</p><p>写実的ノイズ定式化に基づく，インターネットから拾ってきた加工済み画像からカメラで撮ったような写実的画像を生成する合成データ生成手法で学習データを作成．学習中に空間的に変化するカーネルを使い，位置調整とデノイズを実現．
不慮の局所解落ち回避のための，焼きなましロス関数をガイドとした最適化．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Burst_Denoising_with_Kernel_Prediction_Networks.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>流行に乗った手法（合成データによる学習，適応的パラメータ調整）を使って実現．問題設定も地に足がついている感じがする．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Google Researchのインターンでやった模様．</p><ul><li><a href="https://arxiv.org/abs/1712.02327">arXiv</a></li><li><a href="http://people.eecs.berkeley.edu/~bmild/kpn/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#304]</div><div class="timestamp">2018.6.4 10:40:17</div></div></section><section id="ID_MaskLab_Instance_Segmentation_by_Refining_Object_Detection_with_Semantic_and_Direction_Features"><div class="paper-abstract"><div class="title">MaskLab: Instance Segmentation by Refining Object Detection with Semantic and Direction Features</div><div class="info"><div class="authors">Liang-Chieh Chen, Alexander Hermans, George Papandreou, Florian Schroff, Peng Wang and Hartwig Adam</div><div class="conference">CVPR2018</div><div class="paper_id">525</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>物体のBounding-box detection, Semantic segmentationとDirection predictionを同時に行うモデル「MaskLab」を提案する．Faster R-CNN・ResNet-101をベースに，Bounding-box内の前景と背景をわけることでSegmentationを行う．Mask R-CNNと違い，Segmentationを行うときは単純に前景背景分割をするだけでなくクラス分類も行い，また，各ピクセルのDirectionを予測して同じクラスの重なっている物体のInstance segmentationも可能である．また，検出されたBox内でさらに切り出しを行い，小さな物体の検出をしやすくする仕組みも入れている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180604_MaskLab1.jpg" alt="20180604_MaskLab1.jpg"><img src="slides/figs/20180604_MaskLab2.jpg" alt="20180604_MaskLab2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Object detectionとSemantic segmentationを同時にEnd-to-endで解くモデルの提案．それだけでなく，Semantic segmentationではDirectionを考慮して高精度な認識が可能である．MSCOCOで性能評価を行い，FCIS+++（mAP，Seg：33.6），Mask R-CNN（Seg：35.7，Det：38.2）よりも高い性能（学習時にScale augmentationを行いSeg：38.1，Det：43.0）を達成した．Res-NeXtを用いたMask R-CNN（Seg：37.1，Det：39.8）よりも高性能である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>最近，Detection + Segmentationがいくつか出てきているので今後に注目．検出速度に関する記述は見当たらなかったが，Faster R-CNNベースなのでそれ相応の速度だと思われる．ワンショット系の検出器に適応してこの精度を保ちつつ高速な検出ができればウケそう？</p><ul><li><a href="https://arxiv.org/abs/1712.04837">arXiv</a></li></ul></div></div><div class="slide_index">[#305]</div><div class="timestamp">2018.6.4 10:00:39</div></div></section><section id="ID_Making_Convolutional_Networks_Recurrent_for_Visual_Sequence_Learning"><div class="paper-abstract"><div class="title">Making Convolutional Networks Recurrent for Visual Sequence Learning</div><div class="info"><div class="authors">Xiaodong Yang, Pavlo Molchanov, Jan Kautz</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>RNNの改良であり、畳み込み層や全結合層の役割を前処理として構造に入れ込むPreRNNを提案した。従来のRNNとPreRNNの違いは図に示すとおりである（従来型TraditionalなRNNは構造内にfc/conv+avepoolを要するが、PreRNNではそれらを内包している）。このPreRNNを用いて、より有効だと思われるタスクーSequential Face Alighnment, Dynamic Hand Gesture Recognition, Action Recognitionにて適用した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180604PreRNN.png" alt="180604PreRNN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来型のRNNを改善して、fc-layer/conv+avepool-layerをその構造の中に取り込んだPreRNNを提案し、複数タスク（顔アライメント推定、ジェスチャ認識、人物行動認識）にて従来法よりも高い精度を達成した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>画像キャプションなどにも効果あり？どのように説明文が改善されるのか試してみたい。</p><ul><li><a href="http://xiaodongyang.org/publications/papers/prernn-cvpr18.pdf">論文</a></li><li><a href="http://xiaodongyang.org/publications/papers/prernn-supp-cvpr18.pdf">SupplementaryMaterial</a></li></ul></div></div><div class="slide_index">[#306]</div><div class="timestamp">2018.6.4 09:29:31</div></div></section><section id="ID_Inferring_Shared_Attention_in_Social_Scene_Videos"><div class="paper-abstract"><div class="title">Inferring Shared Attention in Social Scene Videos</div><div class="info"><div class="authors">Lifeng Fan, Yixin Chen, Ping Wei, Wenguan Wang, Song-Chun Zhu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>複数人いる人物が同時に同領域に注意を向けることをCo-attention/Shared-attentionといい、本論文では三人称視点の入力からこの推定に取り組む。ここに対してConvLSTM（Convolutional Long-Short Term Memory）を用いたモデルを適用、さらにはVideoCoAttと呼ばれるTV番組をメインとしたビデオからデータ収集を行なった。モデルは視線推定（YOLOv2による顔検出も含む）、領域推定（Region Proposal Map）、空間推定（Convolution）と時系列最適化（LSTM）から構成される。データは380ビデオ/492,000フレームから構成される。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180604SharedAttention.png" alt="180604SharedAttention"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>新しい問題である、三人称視点からの共注視を設定し、データとモデルを公開したことが採択された理由である。また、実験により従来法を抑えて、提案法が71.4%の精度かつ誤差がもっとも小さい手法であることを明らかにした。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>共注視、面白い！（が、ビデオを見てみると曖昧な部分もありもうすこしアノテーションなどに改善の余地がある？）</p><ul><li><a href="http://www.stat.ucla.edu/~lifengfan/SharedAttention_CVPR18/shared_attention_camera_ready.pdf">論文</a></li><li><a href="https://www.youtube.com/watch?v=4uA5buFgi38">YouTube</a></li><li><a href="http://www.stat.ucla.edu/~lifengfan/shared_attention">Project</a></li></ul></div></div><div class="slide_index">[#307]</div><div class="timestamp">2018.6.4 09:07:48</div></div></section><section id="ID_Aperture_Supervision_for_Monocular_Depth_Estimation"><div class="paper-abstract"><div class="title">Aperture Supervision for Monocular Depth Estimation</div><div class="info"><div class="authors">Pratul P. Srinivasan, Rahul Garg, Neal Wadhwa, Ren Ng, Jonathan T. Barron</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Aperture Supervision（カメラのフォーカスによる教示）により単眼画像からデプスマップを推定する研究である。これを推定するために、Focus/Defocusを処理して、領域ごとの反応を確認することでデプスの教示に相当する。CNNベースの距離画像推定では、確率的距離マップ、Shallow Depth-of-field（各距離における重み付けされたマップ）を適用する。図は本論文における単眼カメラによる距離画像推定のパイプラインである。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180604MonoDepth.png" alt="180604MonoDepth"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>RGB-Depthを変換する、いわゆるダイレクトな距離画像推定では計算コストも高く、かつ解像度も低かったが、本論文ではフォーカスに関係する教示によりこの問題を解決し、単眼による距離画像推定を実現した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>距離画像を直接的には使わなくても、LightFieldなどの情報から距離画像を推定することができるので、他の関連手法とは異なるアプローチを与えている。</p><ul><li><a href="https://arxiv.org/abs/1711.07933">論文</a></li><li><a href="https://github.com/google/aperture_supervision">GitHub</a></li></ul></div></div><div class="slide_index">[#308]</div><div class="timestamp">2018.6.4 08:48:55</div></div></section><section id="ID_Deep_End-to-End_Time-of-Flight_Imaging"><div class="paper-abstract"><div class="title">Deep End-to-End Time-of-Flight Imaging</div><div class="info"><div class="authors">Shuochen Su, Felix Heide, Gordon Wetzstein, Wolfgang Heidrich</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>End-to-EndでセンサデータからToFセンサの出力を行うToFNet (Time-of-Flight Network)を提案する。従来のシステムであh、センサーデータの入力からデノイジング、Phase Unwrapping (PU)やMultipath Correction (MP)を行っていたが、ToFNetでは一括処理が可能となるだけでなく、ノイズがない鮮明な画像を出力可能、リアルタイムで動作可能である。ToFNetはPatchGANという枠組みにより最適化が行われる。PatchGANはEncoder-Decoderの構造をした生成器と非常にシンプルな構造の識別器により構成される。誤差はL1+DepthGradient+Adversarialと、その重み付き和により計算される。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180604E2EToF.png" alt="180604E2EToF"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来のカスケード型処理（デノイジング、PU、MP）ではノイズが蓄積してしまいがちだが、提案のToFNetは一括での処理を行い、(1)ノイズを鮮明に除去できるのみならず(2)リアルタイムでの処理が可能である。主にこの２点が採択された理由であると考える。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Depth推定、すでに数値や見た目による判断が曖昧になりつつある？屋内だけでなく、多様なドメインでの適応が待たれる。</p><ul><li><a href="http://vccimaging.org/Publications/Su2018EndToEndTOF/Su2018EndToEndTOF.pdf">論文</a></li><li><a href="http://vccimaging.org/Publications/Su2018EndToEndTOF/">Project</a></li></ul></div></div><div class="slide_index">[#309]</div><div class="timestamp">2018.6.4 08:26:21</div></div></section><section id="ID_Dont_Just_Assume_Look_and_Answer_Overcoming_Priors_for_Visual_Question_Answering"><div class="paper-abstract"><div class="title">Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering</div><div class="info"><div class="authors">Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darrel and Dawn Song</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>VQAの学習は学習データの答えの分布に依存してしまう。そこで、答えの分布が異なる学習データを用いて学習した場合でもGrounded Visual Question Answering(GVQA)を提案した。
GVQAでは質問に答える上で、(1)必要な情報を認識する（例：物体の色を聞かれている場合対象となる物体を認識する)(2)必要な答えを推測する(例：物体の色を聞かれている場合色を答える)の2つが重要であると仮定する。
そこで、画像から質問に答えるために必要な情報を抽出する部分と答えを推定する部分の2つに分けたモデルを構築した。
その際、質問から質問のタイプ(yes/noで答えられるか)を推定することで、質問の答えを異なるネットワークによって出力させる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Dont_Just_Assume_Look_and_Answer_Overcoming_Priors_for_Visual_Question_Answering.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>質問の答えの分布を学習データとテストデータで異なる分布にしたVQA-CPデータセットを提案した。同データセットを用いて従来手法及びGVQAの精度を調べたところ、従来のデータセットと比べた際の従来手法の精度低下及びGVQAの方が高い精度を記録したことを示した。
また、GVQAによって答えの根拠を説明することが可能となった。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.00377">論文</a></li></ul></div></div><div class="slide_index">[#310]</div><div class="timestamp">2018.6.4 02:20:01</div></div></section><section id="ID_Fooling_Vision_and_Language_Models_Despite_Localization_and_Attention_Mechanism"><div class="paper-abstract"><div class="title">Fooling Vision and Language Models Despite Localization and Attention Mechanism</div><div class="info"><div class="authors">Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darrel and Dawn Song</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Adversarial attackが、VisionとLanguageの融合問題のようにより複雑な問題に対しても有効であるかを調査した。対象とするタスクは、画像キャプショニング及びVQAとして画像のAdversarial exampleによる出力の変化を調べた。
また、これらの手法におけるlocalizationがAdversarial Attackに影響されるかを確認した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Fooling_Vision_and_Language_Models_Despite_Localization_and_Attention_Mechanism.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Dense Captionについては、97％の確率で騙すことに成功した。同じ画像の同じ領域に対しても目標とするキャプションが異なると異なるキャプションを出力させることが可能なことを確認した。
VQAについてもごく一部を除いて騙すことができることを確認した。
Attention Mapを確認すると、Adversarial exampleを入力した場合異なる領域に注目していることが明らかになった。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1709.08693">論文</a></li></ul></div></div><div class="slide_index">[#311]</div><div class="timestamp">2018.6.4 00:26:41</div></div></section><section id="ID_Visual_Question_Reasoning_on_General_Dependency_Tree"><div class="paper-abstract"><div class="title">Visual Question Reasoning on General Dependency Tree</div><div class="info"><div class="authors">Qingxing Cao, Xiaodan Liang, Bailing Li, Guanbin Li, Liang Lin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>VQAの答えだけでなく判断根拠も出力する手法を提案。質問をtree構造に分解し、各nodeに関する情報(例：plane)が画像中のどこに存在するかを示すattention mapを求める。
既に得られているattentionマップ及びhidden stateを更新していくことで、質問の答えとたどり着いていく。
最終的な質問の答えはhidden stateを用いて求める。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Visual_Question_Reasoning_on_General_Dependency_Tree.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>質問への回答の精度は従来手法と比べて大きく向上されているわけではない。従来の判断根拠を求める研究はルールを人間が設計するもしくはground truthが必要であるのに対してこれらを必要とせずに回答根拠を得ることに成功。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.00105">論文</a></li></ul></div></div><div class="slide_index">[#312]</div><div class="timestamp">2018.5.29 19:59:57</div></div></section><section id="ID_Blind_Predicting_Similar_Quality_Map_for_Image_Quality_Assessment"><div class="paper-abstract"><div class="title">Blind Predicting Similar Quality Map for Image Quality Assessment</div><div class="info"><div class="authors">Da Pan, Ping Shi, Ming Hou, Zefeng Ying, Sizhe Fu and Yuan Zhang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像の品質を評価するためのBlind Predicting Similar Quality Map for IQA(BPSQM)を提案した。CNNを用いた画像の品質評価手法は数多く提案されているが、その大半はブラックボックスとなっている。
本研究は、ピクセル単位の画像の損失度合いを示すquality mapを始めに推定することで、画像圧縮などに伴いどのように画像の品質が低下してるかの可視化を可能とした。
また、qualityマップから画像の損失度合いを表すスコアの算出を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Blind_Predicting_Similar_Quality_Map_for_Image_Quality_Assessment.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来のquality mapを求める手法は、損失前の画像(reference)が必要なものが大半であり、reference不要なCNNベースの手法はパッチ単位で推定するのみであった。それに対して本研究は、referenceなしでピクセル単位のquality mapを推定することを可能とした。
損失度合いの推定に関しても、referenceなしの手法と比べて精度の向上を実現した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1805.08493">論文</a></li></ul></div></div><div class="slide_index">[#313]</div><div class="timestamp">2018.6.3 21:52:18</div></div></section><section id="ID_AMNet_Memorability_Estimation_with_Attention"><div class="paper-abstract"><div class="title">AMNet: Memorability Estimation with Attention</div><div class="info"><div class="authors">Jiri Fajtl, Vasileios Argyriou, Dorothy Monekosso, Paolo Remagnino</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像中の記憶に残りやすい領域（Memorability）を可視化するネットワークであるAMNet（Attention and Memorability Network?）の提案。ResNet50による特徴表現、LSTMにより実装されたAttention構造の仕組みによりMemorabilityスコアを算出する。アノテーションは従来研究であるLaMem（下記リンク参照）に使用したデータセットであるSUN Memorability（同じく下記参照）を用いて学習を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180603AMNet.png" alt="180603AMNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来法よりも精度が良かった（より人間の記憶の構造に近かった？）ことを示した。これはアテンション構造を用いていることが、より人間の記憶の仕組みにおいて再現性が良かったことを示しているといえる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>記憶の仕組みも人間の直感が必要な高次機能の再現である。このように高次なラベリングが今後は増えてくると思うし、人間のタスクをカバーする意味でも重要になるか？</p><ul><li><a href="https://arxiv.org/abs/1804.03115">論文</a></li><li><a href="https://github.com/ok1zjf/AMNet">GitHub</a></li><li><a href="http://memorability.csail.mit.edu/download.html">LaMem</a></li><li><a href="http://web.mit.edu/phillipi/Public/WhatMakesAnImageMemorable/">SUN Memorability</a></li></ul></div></div><div class="slide_index">[#314]</div><div class="timestamp">2018.6.3 23:48:57</div></div></section><section id="ID_Lose_The_Views_Limited_Angle_CT_Reconstruction_via_Implicit_Sinogram_Completion"><div class="paper-abstract"><div class="title">Lose The Views: Limited Angle CT Reconstruction via Implicit Sinogram Completion</div><div class="info"><div class="authors">Rushil Anirudh, Hyojin Kim, Jayaraman J. Thiagarajan, K. Aditya Mohan, Kyle Champley, Timo Bremer</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>手荷物検査や医療用として用いられるComputed Tomography (CT)画像の復元を、限られた角度のSinogramの入力から行う技術（CTNet）を提案する。CTNetは1D/2D畳み込みで構成され、SinogramからFull-viewのCT画像を復元することができる。図はCTNetの学習とテストを示したものである。学習時にはGAN-likeな手法により構成され、入力から1DCNNにより特徴量を生成、GeneratorがCT画像を復元、DiscriminatorがReal/Fakeを判断することでGeneratorを鍛える。テスト時にはさらにFBP (Filtered Back Projection)/WLS (Weighted Least Squares)なども用いて最終的な結果を得る。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180527CTNet.png" alt="180527CTNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>角度が限定されたx線画像から、360度のCT画像を生成するというチャレンジングな試みを行ったことが評価された。同課題に対してGAN-likeな手法を提案し、手法的な新規性も打ち出せたことが採択された基準であると考える。PSNRやセグメンテーションベースの方法で評価を行い、従来法よりも優れた手法であることを示した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>CT画像を復元できてしまうのがすごい！</p><ul><li><a href="https://arxiv.org/abs/1711.10388">論文</a></li></ul></div></div><div class="slide_index">[#315]</div><div class="timestamp">2018.5.28 00:07:00</div></div></section><section id="ID_Learning_to_Extract_a_Video_Sequence_from_a_Single_Motion-Blurred_Image"><div class="paper-abstract"><div class="title">Learning to Extract a Video Sequence from a Single Motion-Blurred Image</div><div class="info"><div class="authors">Meiguang Jin, Givi Meishvili, Paolo Favaro</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>１枚のブラー画像から時系列フレームを推定して動画像を生成するアプローチを提案。モーションブラーは通常、カメラなどセンサによる露光により発生するが、その分解は非常に困難な問題として扱われていた。本論文では平均化を除去してフレームを時系列方向に並べ、次にDeconvolutionを復元して同問題に取り組む（この問題は通常、Blind Deconvolutionと言われる）。提案法では、深層学習の手法としてこの両者を実現する構造を構築。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180527SingleMotionBlurredImage.png" alt="180527SingleMotionBlurredImage"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Blind Deconvolutionの課題を取り扱っているが、さらにここでは単一のブラー画像から動画像を生成するアルゴリズムや深層学習アーキテクチャを提案した。特に、ブラー画像から時系列画像を順次復元するための誤差関数を提案したことが最も大きな新規性である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>もともとあった問題に少し味付けして、新しい問題を作り出すセンスが欲しい。。</p><ul><li><a href="https://arxiv.org/pdf/1804.04065.pdf">論文</a></li></ul></div></div><div class="slide_index">[#316]</div><div class="timestamp">2018.5.27 23:44:14</div></div></section><section id="ID_Learning_to_Detect_Features_in_Texture_Images"><div class="paper-abstract"><div class="title">Learning to Detect Features in Texture Images</div><div class="info"><div class="authors">Linguang Zhang, Szymon Rusinkiewicz</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>テクスチャに対して有効かつスケーラブル、さらに学習可能な局所特徴量を提案する。さらに提案手法は既存のランキングロスやFully-Convolutional Networks (FCN; 全層畳み込みネットワーク)と統合可能である。著者らは、新規の学習誤差関数であるPeakednessという指標を畳み込みマップに対して導入した。画像はテスト画像に対して提案手法を施した結果であり、Repeatableな特徴量（画像の中に再帰的に登場するテクスチャ特徴）が検出されている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180527LearningToDetectFeatures.png" alt="180527LearningToDetectFeatures"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>（i）FCN構造によりフルサイズの再帰的なテクスチャパターンを評価することに成功した、（ii）Peakednessという指標を導入し、これを最大化することでテクスチャを評価するための畳み込みマップを洗練化することに成功、という点がもっとも重要な新規性である。実験ではcarpet/asphalt/wood/tile/granite/concrete/coarseといったテクスチャパターンに対して有効であることを示した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>複雑かつ特徴が比較的取りづらいテクスチャの解析は今後さらに重要性を増すと考えられる（道路面のひび割れ調査など）。ここに教師なし学習（Self-Supervision含む）が導入されていくことになると思う。</p><ul><li><a href="http://gfx.cs.princeton.edu/pubs/Zhang_2018_LTD/keypoint-cvpr.pdf">論文</a></li><li><a href="http://gfx.cs.princeton.edu/pubs/Zhang_2018_LTD/index.php">Project</a></li></ul></div></div><div class="slide_index">[#317]</div><div class="timestamp">2018.5.27 23:19:17</div></div></section><section id="ID_Smart_Sparse_Contours_to_Represent_and_Edit_Images"><div class="paper-abstract"><div class="title">Smart, Sparse Contours to Represent and Edit Images</div><div class="info"><div class="authors">T.Dekel, C.Gan, D.Krishnan, C.Liu and W.T.Freeman</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1712.08232</div></div><div class="slide_editor">Kota Yoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>元画像の輪郭情報から画像を再構成する手法を提案.GANをベースとして，入力情報が与えられない領域のテクスチャと細部を合成する.実験では，顔認証システムや人間を対象にして元画像と再構成された画像と区別されないという結果となった．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Smart_Sparse_Contours_to_Represent_and_Edit_Images.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Pix2pixなどの既存の手法よりも大幅に向上している．</li><li>2つのネットワークで構成されており，1つ目のネットワークでは，画像全体の構造，色を再構成，2つ目のネットワークでは画像のテクスチャと細部の表現をしている．</li><li>直感的な操作が可能で，顔のパーツを移動させたり，追加させることもできる．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>入力情報がない輪郭と輪郭の間の画像部分の再構成にも力を入れてる</p></li><li><p><a href="https://arxiv.org/abs/1712.08232">Paper</a></p></li></ul></div></div><div class="slide_index">[#318]</div><div class="timestamp">2018.6.3 19:36:19</div></div></section><section id="ID_R-FCN-3000_at_30fps_Decoupling_Detection_and_Classification"><div class="paper-abstract"><div class="title">R-FCN-3000 at 30fps: Decoupling Detection and Classification</div><div class="info"><div class="authors">Bharat Singh, Hengduo Li, Abhishek Sharma and Larry S. Davis</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Kodai Nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>オブジェクト性検出と分類を分離した物体検出器であるR-FCN-3000を提案した．RoIのための検出スコアを得るために，オブジェクト性検出と分類スコアをかける．
R-FCNで提案されたposition-sensitive filterはfine-grained classificationには必要ないというのが基本アイディア．
また本論文では，R-FCN-3000はオブジェクト数が増えると性能が向上することが示されている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/R-FCN-3000_1.PNG" alt="R-FCN-3000_1.PNG"><img src="slides/figs/R-FCN-3000_2.PNG" alt="R-FCN-3000_2.PNG"><img src="slides/figs/R-FCN-3000_3.PNG" alt="R-FCN-3000_3.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ImageNet detection datasetで一秒あたり30枚の画像を処理したところ，mAPが34.9%であった（YOLO9000は18%）．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.01802.pdf">論文URL</a></li></ul></div></div><div class="slide_index">[#319]</div><div class="timestamp">2018.6.2 21:48:26</div></div></section><section id="ID_Learning_to_See_in_the_Dark"><div class="paper-abstract"><div class="title">Learning to See in the Dark</div><div class="info"><div class="authors">Chen Chen, Qifeng Chen, Jia Xu and Vladlen Koltun</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kodai Nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>暗い環境において，同じシーンを短時間露光で撮影した暗い画像と長時間露光で撮影した明るい画像のrawデータを集めたデータセットを提案した．このデータセットは，5094個の暗い画像のrawデータと424個の明るい画像のrawデータが1対多で対応付けられている．
インドアとアウトドアの両方で撮影を行った．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning-to-see-in-the-dark-1.PNG" alt="Learning-to-see-in-the-dark-1.PNG"><img src="slides/figs/Learning-to-see-in-the-dark-2.PNG" alt="Learning-to-see-in-the-dark-2.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>このデータセットを用いてFCNをトレーニングし，テストしたところ図に示すような結果が得られた．このネットワークはrawデータを直接扱うため，図に示すように，従来の画像処理パイプラインの多くの代わりになる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1805.01934.pdf">論文URL</a></li><li><a href="https://github.com/cchen156/Learning-to-See-in-the-Dark">github</a></li></ul></div></div><div class="slide_index">[#320]</div><div class="timestamp">2018.6.2 18:44:28</div></div></section><section id="ID_AVA_A_Video_Dataset_of_Spatio-temporally_Localized_Atomic_Visual_Actions"><div class="paper-abstract"><div class="title">AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions</div><div class="info"><div class="authors">C. Gu et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">大規模な新規動画データセットを構築．
従来の動画データセットが複合的な行動ラベルを扱うのに対して，
このデータセットではStand, Sit, WatchのようなAtomicな行動ラベル (80 classes) を扱う．
このようなラベルが1秒間隔で動画中のすべての人にアノテーションされており，
しかもBounding Boxまで付いているというのがこのデータセットの強み．
80種類ものAtomicな行動ラベルが大規模にしかも密に付いているデータセットは初．
加えて，Two-stream I3D & Faster R-CNNというような手法を提案．
従来のSpatio-temporal Action Localization用のデータセットではSOTAを達成したものの，
このデータセットは15.6% mAPと問題の難しさも主張している．
</div></div><div class="item2"><img src="slides/figs/AVA_A_Video_Dataset_of_Spatio-temporally_Localized_Atomic_Visual_Actions.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>Bounding Boxまでアノテーションされている初の大規模動画データセットを構築</li><li>動画中の一部ではなく密にAtomicな行動のラベルがアノテーションされている</li><li>Spatio-temporal Localizationをするためのベンチマークとなる新規手法も提案</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://arxiv.org/abs/1705.08421">論文 (arXiv)</a></li><li><a href="https://research.google.com/ava/">データセット</a></li><li><a href="http://activity-net.org/challenges/2018/tasks/guest_ava.html">ActivityNet Challenge 2018 Task B</a></li></ul></div></div><div class="slide_index">[#321]</div><div class="timestamp">2018.6.1 14:53:37</div></div></section><section id="ID_SGAN_An_Alternative_Training_of_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">SGAN: An Alternative Training of Generative Adversarial Networks</div><div class="info"><div class="authors">Tatjana Chavdarova, Idiap and EPFL; Francois Fleuret, Idiap Research Institute</div><div class="conference">CVPR2018</div><div class="paper_id">1712.02330</div></div><div class="slide_editor">KenichiroWani</div><div class="item1"><div class="text"><h1>概要</h1><p>General Advesarial Networks(GAN)は現在，コンピュータビジョン分野で広く使われている手法である．しかしながら，複雑な学習をするには時間がかかり，人の手が必要となる．そこでSGANというトレーニングプロセスを検討する．SGANではいくつかの敵対的でローカルなネットワークの組み合わせを独立させて学習させることでグローバルな一対のネットワークの組み合わせを学習することができる．SGANの学習はローカルディスクリミネータとジェネレータによってグローバルディスクリミネータとジェネレータが学習される．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SGAN_An_Alternative_Training_of_Generative_Adversarial_Networks.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>adversarial pairs (G1,D1),...,(GN,DN)を学習し， G0はD1,...,DNによって学習， D0はG1,...,GNによって学習させることでグローバルな一対のネットワークを学習する。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.02330">arxiv</a></li></ul></div></div><div class="slide_index">[#322]</div><div class="timestamp">2018.5.30 21:15:54</div></div></section><section id="ID_Learning_from_Noisy_Web_Data_with_Category-level_Supervision"><div class="paper-abstract"><div class="title">Learning from Noisy Web Data with Category-level Supervision</div><div class="info"><div class="authors">L.Niu, Q.Tang, A.Veeraraghavan, and A.Sabharwal</div><div class="conference">CVPR 2018</div><div class="paper_id">996</div></div><div class="slide_editor">Kazuma Asano</div><div class="item1"><div class="text"><h1>概要</h1><p>日に日に増えるウェブデータから学習することはポピュラーになりつつあるが，従来の学習とウェブデータを使用した学習では，ラベルが時々間違っているなどの大きなギャップが存在する．これを解決する従来手法では，さらに情報を追加してウェブデータから学習する傾向があったが，この論文では，より活発なカテゴリレベルの監視をすることによりラベルノイズを減らすWSCI(Webly Supervised
learning with Category-level Information)を提案．
分類を行うネットワークをVAE（Variational AutoEncoder）の隠れ層に接続し，分類ネットワークとVAEがカテゴリレベルのHybrid Semantic Informationを共有する．
提案手法の有効性はAwA2, CUB, SUNの3つデータセットで評価している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/0180922_WCSI.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>いずれのデータセットにおいても，提案手法は従来手法に比べ2～5%ほど精度が向上しており，AwA2のデータセットにおいては90%を超える評価を出している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>人間がわざわざデータを集めるのではなく，機械が自動的にデータセットを作ることで今までの負担を大きく減らすことができると考えられる．</p><ul><li><a href="https://arxiv.org/abs/1803.03857">arxiv</a></li></ul></div></div><div class="slide_index">[#323]</div><div class="timestamp">2018.6.22 20:15:55</div></div></section><section id="ID_Geometry-Aware_Learning_of_Maps_for_Camera_Localization"><div class="paper-abstract"><div class="title">Geometry-Aware Learning of Maps for Camera Localization</div><div class="info"><div class="authors">S.Brahmbhatt, J.Gu, K.Kim, J.Hays and J.Kautz</div><div class="conference">CVPR 2018</div><div class="paper_id">340</div></div><div class="slide_editor">Kazuma Asano</div><div class="item1"><div class="text"><h1>概要</h1><p>DNNを使用したデータ駆動型による学習を可能するカメラ位置推定手法, MapNetを提案．MapNetではイメージ間の絶対的な位置姿勢と相対的な位置姿勢のロスを最小限に抑えることができる．
さらに画像だけでなく，Visual odometry(VO)やGPSなどのユビキタスセンサ，Inertial Measurement Unit(IMU)などをカメラ位置推定に加え，
ラベルなしのビデオを利用した，自己教師あり学習によるMapNet+の提案もした．
Pose Graph Optimization(PGO)によって入力データをrefineしてAccurancyの改善を行う．
データセットには小規模な位置推定のトレーニングに7-Senes，大規模なものにはOxford RobotCarsを用いている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180340_mapnet_1.png" alt="Item3Image"><img src="slides/figs/180340_MapNet_2.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>MapNet+は自己教師あり学習とマルチセンサによってパフォーマンスを向上させており，特に大規模な位置推定ではStereo VOやPoseNetなどの従来手法と比較し精度が劇的に向上している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.03342">arXiv</a></li><li><a href="http://research.nvidia.com/publication/2018-06_Geometry-Aware-Learning-of">Project Page</a></li><li><a href="https://www.youtube.com/watch?v=X6mF_IbOb4A">Youtube</a></li><li><a href="https://github.com/NVlabs/geomapnet">GitHub</a></li></ul></div></div><div class="slide_index">[#324]</div><div class="timestamp">2018.6.11 14:01:28</div></div></section><section id="ID_Conditional_Generative_Adversarial_Network_for_Structured_Domain_Adaptation"><div class="paper-abstract"><div class="title">Conditional Generative Adversarial Network for Structured Domain Adaptation</div><div class="info"><div class="authors">W.Hong, Z.Wang, M.Yang and J.Yuan</div><div class="conference">CVPR2018</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>コンピュータによって学習用のアノテーションを生成し，実画像のような合成画像として用いることが流行．しかし，ドメインの不一致という問題が起きる．それを解決するために，GANをFCNフレームワークに統合することでSemanticSegmentationのためのドメイン適用のための手法を提案．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Conditional_Generative_Adversarial_Network_for_Structured_Domain_Adaptation.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>合成画像の特徴を実画像のように変換する条件付きジェネ−レータとディスクリメーターを学習</li><li>ジェネレータは合成画像を実画像のようにディスクリメーターを騙すように学習させることでFCNのパラメータを更新．</li><li>本手法である実際のラベルを用いずに実験を行い，CityscapesデータセットのIoU平均が12〜20上回りSoTA．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>FCN＋GANでSemanticSegmentation</li><li><a href="https://weixianghong.github.io/publications/papers/CVPR_18.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#325]</div><div class="timestamp">2018.5.28 15:36:39</div></div></section><section id="ID_Learning_to_Sketch_with_Shortcut_Cycle_Consistency"><div class="paper-abstract"><div class="title">Learning to Sketch with Shortcut Cycle Consistency</div><div class="info"><div class="authors">Jifei Song, Kaiyue Pang, Yi-Zhe Song, Tao Xiang, Timothy M. Hospedales</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像からスケッチのストロークを取得する手法の提案。人間が画像からスケッチをすると、同じ画像に対しても様々なバリエーションが生じてしまう。
そこで、教師有学習と教師無学習を組み合わせることによって画像からスケッチの取得を実現する。
教師有学習は、画像からスケッチもしくはスケッチから画像という変換を学習する。
教師無学習は、オートエンコーダのように画像もしくはスケッチを符号化し、元に戻すという処理を学習する。
その際、CycleGANのようにドメイン変換を繰り返すのではなく、符号化したものをそのまま復号化する(Shortcut Cycle)。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Sketch_with_Shortcut_Cycle_Consistency.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Pix2pixやCycleGANなどの手法と比較を行い、いずれの手法と比較してもスケッチとして抽象化されつつもセマンティックな特徴を捉えていることを確認した。また、数値評価としてスケッチの認識及び検索タスクを行って評価した。
どちらのタスクにおいても、従来手法と比較して高い精度でスケッチへの変換ができていることを示した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1805.00247">論文</a></li></ul></div></div><div class="slide_index">[#326]</div><div class="timestamp">2018.5.29 14:10:18</div></div></section><section id="ID_Show_Me_a_Story_Towards_Coherent_Neural_Story_Illustration"><div class="paper-abstract"><div class="title">Show Me a Story: Towards Coherent Neural Story Illustration</div><div class="info"><div class="authors">Hareesh Ravi, Lezi Wang, Carlos M. Muniz, Leonid Sigal, Dimitris N. Metaxas, Mubbasir Kapadia</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>複数の文で構成されたテキストの内容を表す画像シークエンスを検索する手法を提案。文章から抽出される特徴と画像から抽出された特徴を対応付けることにより、各文に対して1枚の画像を選択する。
その際、文章特徴はGRUによって前後の文章との関係を含めて抽出する。
また、heやitなどの代名詞が何を指しているかを明らかにするために、テキスト全体としての一貫性を測るcoherence vectorを導入した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Show_Me_a_Story_Towards_Coherent_Neural_Story_Illustration.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ベースラインとなる手法では、文単位で画像の検索を行っているために画像シークエンスとしての一貫性が損なわれてしまう。そこで、GRU及びcoherence vectorによって前後の文で登場した単語などを考慮することが可能となり、テキスト全体を表す画像シークエンスの検索が可能となった。
ユーザースタディにより、ベースライン、coherence vector無し、coherence vector有りの比較を行い、coherence vector有りが最も好まれる結果を得た。
また、画像シークエンスがテキストに合っているかは主観的な評価であるため、saliencyベースの新たな評価指標を提案した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://www.cs.ubc.ca/~lsigal/Publications/cvpr2018ravi.pdf">論文URL</a></li><li><a href="https://arxiv.org/abs/1511.06361">ベースライン</a></li></ul></div></div><div class="slide_index">[#327]</div><div class="timestamp">2018.5.29 12:03:27</div></div></section><section id="ID_SO-Net_Self_Organizing_Network_for_Point_Cloud_Analysis"><div class="paper-abstract"><div class="title">SO-Net: Self-Organizing Network for Point Cloud Analysis</div><div class="info"><div class="authors">Jiaxin Li et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>順序構造に対して不変な３次元 Point Cloud のための deep learning アーキテクチャー SO-Net を提案. Self-Organizing Map (SOM) を作ることで点群の空間分布をモデル化し, SOMのノードを用いて階層的な特徴量の抽出を行う. Point Cloud のクラス分類やセグメンテーションなどのタスクを用いた評価実験では, 先行研究と同等以上の結果をより短い学習時間で達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-SO-Net-Self-Organizing-Network-for-Point-Cloud-Analysis.png" alt="fukuhara-fukuhara-SO-Net-Self-Organizing-Network-for-Point-Cloud-Analysis.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>SOM を用いることで Point Cloud を複数の Point Cloud の部分集合に分割し, 各部分集合ごとの特徴量を抽出した後, 全体の特徴量を階層的に抽出する.</li><li>初期ノードの位置を固定し, 学習を batch 単位で行うことで, SOM の学習が順序構造に対して不変となるようにしている.</li><li>様々なタスクの事前学習として用いるための Point Cloud の autoencoder を提案. </li><li>ネットワークの構造が単純かつ並列計算可能なため, 先行研究よりも短時間で学習をすることが可能.</li><li>point cloud reconstruction, classification, object part segmentation, shape retrieval などの複数のタスクを用いて評価実験を行った.</li><li>評価実験の結果では Point-Net++ や Kd-Net などとの先行研究と同等以上の結果を半分以下の学習時間で達成した.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.04249" target="blank">[論文] SO-Net: Self-Organizing Network for Point Cloud Analysis</a></li><li><a href="https://github.com/lijx10/SO-Net" target="blank">[Code] GitHub</a></li></ul></div></div><div class="slide_index">[#328]</div><div class="timestamp">2018.5.17 12:19:55</div></div></section><section id="ID_Large-scale_Point_Cloud_Semantic_Segmentation_with_Superpoint_Graphs"><div class="paper-abstract"><div class="title">Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs</div><div class="info"><div class="authors">Yoshihiro Fukuhara et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>大規模(数百万規模)な point clouds データに対して効率的に Semantic Segmentation を行う研究. まず, point clouds 全体を形状が単純で, 意味的に同じ点が属する部分集合(superpoint）に分類し, superpoint が作るグラフ（SPG）に graph convorution を適用することで segmentation を行う. Semantic3D と S3DIS dataset を用いた評価実験では先行研究よりも良い結果を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Large-scale-Point-Cloud-Semantic-Segmentation-with-Superpoint-Graphs.png" alt="fukuhara-Large-scale-Point-Cloud-Semantic-Segmentation-with-Superpoint-Graphs.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>superpoint の構成は先行研究(Guinard+17)で提案された, Global Energy を用いて行う.</li><li>各 superpoint の特徴量を PointNet を用いて抽出する. (大規模なデータを扱うため, 各 superpoint 内でダウンサンプリングを行っている.) </li><li>抽出された各 superpoint の特徴量に対して Gated Recurrent Unit (GRU) を用いた graph convorution を適用することで, 各 superpoint のクラス分類を行う.</li><li>Semantic3D と S3DIS dataset を用いた評価実験では, ShapeNet などの先行研究と比較して複数の評価尺度で最も優位な結果を達成した.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.09869" target="blank">[論文] Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs</a></li><li><a href="https://github.com/loicland/superpoint_graph" target="blank">[Code] GitHub</a></li></ul></div></div><div class="slide_index">[#329]</div><div class="timestamp">2018.5.28 00:47:55</div></div></section><section id="ID_FoldingNet_Point_Cloud_Auto_encoder_via_Deep_Grid_Deformation"><div class="paper-abstract"><div class="title">FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation</div><div class="info"><div class="authors">Yaoqing Yang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>3次元点群処理のための autoencoder を提案. Folding という新しい decoding 演算を導入することで, 2次元グリッド上の点から3次元点群の表面上への射影を教師なしで学習した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-FoldingNet-Point-Cloud-Auto-encoder-via-Deep-Grid-Deformation.png" alt="fukuhara-FoldingNet-Point-Cloud-Auto-encoder-via-Deep-Grid-Deformation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>新しい end-to-end な3次元点群処理のための deep autoencoder を提案した.</li><li>提案手法のdecoderのパラメータ数は既存手法の7%であるが, これで2次元グリッドと任意の3次元点群表面への写像が構成できることを理論的に証明した.</li><li>MN40 や MN10 dataset を用いた classification タスクの評価実験では, 最先端の教師あり手法（Achlioptas+17）などと同等の精度を達成した.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.07262" target="blank">[論文] FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation</a></li><li><a href="https://www.youtube.com/watch?v=csC6SodV6vk" target="blank">[動画] YouTube</a></li></ul></div></div><div class="slide_index">[#330]</div><div class="timestamp">2018.5.22 12:19:55</div></div></section><section id="ID_FFNet_Video_Fast-Forwarding_via_Reinforcement_Learning"><div class="paper-abstract"><div class="title">FFNet: Video Fast-Forwarding via Reinforcement Learning</div><div class="info"><div class="authors">Shuyue Lan et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Video Fast-forwarding のタスクを MDP(Markov Decision Process) として定式化し, 強化学習を用いて解く方法を提案. 評価実験では精度と効率の両方に置いて先行研究よりも優れた結果を示した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-FFNet-Video-Fast-Forwarding-via-Reinforcement-Learning.png" alt="fukuhara-FFNet-Video-Fast-Forwarding-via-Reinforcement-Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Video Fast-forwarding を MDP (Markov Decision Process) として定式化した.</li><li>現在の Frame の特徴量を状態, スキップする Frame 数を行動として, Q-learningで強化学習を行う. </li><li>報酬はスキップした Frame の中に重要なものがどの程度含まれていたかに基づいて計算される.</li><li>Tour20 や TVSum dataset を用いた先行研究との比較実験では, 主観評価と定量的評価の両方に置いて最も良い結果となった.(6-20%程度、重要なframeを含んでいる割合が増加)</li><li>先行研究と比較して80%近く処理するフレーム数を削減し, 効率化することに成功した.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://vcg.engr.ucr.edu/publications/Shuyue_CVPR.pdf" target="blank">[論文] FFNet: Video Fast-Forwarding via Reinforcement Learning</a></li></ul></div></div><div class="slide_index">[#331]</div><div class="timestamp">2018.5.17 17:25:55</div></div></section><section id="ID_Egocentric_Activity_Recognition_on_a_Budget"><div class="paper-abstract"><div class="title">Egocentric Activity Recognition on a Budget</div><div class="info"><div class="authors">Rafael Possas et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>ウェアラブルデバイスのような使用可能な電力が限られる状況において, 電力消費と精度を強化学習を用いてバランスするフレームワークを提案. 複数のセンサー情報を用いた行動認識のタスクにおいて, 高精度・高電力消費な predictor と低精度・低電力消費な predictor を強化学習の結果に基づいて適宜切り替えることで少ない消費電力で先行研究と同等の精度を達成した. また, 一人称視点動画行動認識のための新しいデータセットを作成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Egocentric-Activity-Recognition-on-a-Budget.png" alt="fukuhara-Egocentric-Activity-Recognition-on-a-Budget.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ウェアラブルカメラの情報を用いた高精度・高コストな predictor とモーションセンサーの情報を用いた低精度・低コストな predictor のどちらを使用して推定を行うべきかを A3C の agent が判断する.</li><li>どちらのセンサーの情報を用いても正しい推定結果となるような状況では低精度・低コストな predictor を使用した場合に大きな報酬が得られるように agent の学習を行う.</li><li>提案手法では報酬についてのパラメータ１つを調整する事で精度と消費電力の簡単なトレードオフが可能.</li><li>一人称視点動画行動認識のための新しいデータセット（DataEgo）を作成.</li><li>Multimodal egocentric dataset を用いた評価実験では従来手法(Song+16)とほぼ同等の精度を少ない消費電力で達成.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://web.it.usyd.edu.au/~framos/Publications_files/egocentric-activity-recognition%20%282%29.pdf" target="blank">[論文] Egocentric Activity Recognition on a Budget</a></li></ul></div></div><div class="slide_index">[#332]</div><div class="timestamp">2018.5.19 13:40:55</div></div></section><section id="ID_A2-RL_Aesthetics_Aware_Reinforcement_Learning_for_Image_Cropping"><div class="paper-abstract"><div class="title">A2-RL: Aesthetics Aware Reinforcement Learning for Image Cropping</div><div class="info"><div class="authors">Debang Li et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>強化学習 (A3C) を用いて Image cropping を行う手法を提案. 従来の sliding winodow に基づく手法のように膨大な数の cropping 候補を評価する必要がないため, 先行研究よりも短時間で結果の計算が可能. また, 評価実験では精度についても先行研究よりも優位な結果を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-A2-RL-Aesthetics-Aware-Reinforcement-Learning-for-Image-Cropping.png" alt="fukuhara-A2-RL-Aesthetics-Aware-Reinforcement-Learning-for-Image-Cropping.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Image cropping を sequential decision-making process として定式化した. (14種類の cropping を action として, Markov 過程としてモデル化.)</li><li>上記の問題を A3C を用いた強化学習を用いて解いた. </li><li>報酬については学習済みの View Finding Network (Chen＋2017）を使用.</li><li>各ステップで候補となる cropping の種類の数が少ないため, 先行研究と比較して非常に短い計算時間で結果を出力することが可能となった.</li><li>Flickr Cropping Dataset, CUHK Image Cropping Dataset, Human Cropping Dataset を用いて行った評価実験ではいずれも先行研究よりも優位な結果を達成した.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1709.04595.pdf" target="blank">[論文] A2-RL: Aesthetics Aware Reinforcement Learning for Image Cropping</a></li><li><a href="https://github.com/wuhuikai/TF-A2RL" target="blank">[Code] GitHub</a></li></ul></div></div><div class="slide_index">[#333]</div><div class="timestamp">2018.5.22 18:27:55</div></div></section><section id="ID_Good_View_Hunting_Learning_Photo_Composition_from_Dense_View_Pairs"><div class="paper-abstract"><div class="title">Good View Hunting: Learning Photo Composition from Dense View Pairs</div><div class="info"><div class="authors">Zijun Wei, Jianming Zhang, Xiaohui Shen, Zhe Lin, Radomir Mech, Minh Hoai, Dimitris Samaras</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像の構図の良し悪しを評価するComparative Photo Compositionデータセットを構築。10800枚の画像から24の構図の画像を作成し、クラウドソーシングによって2つの構図のどちらがいいかをアノテーションした。
また、入力画像をどのようにクロッピングすると良い構図になるかを提示するシステムを構築した。
その際、IOUを評価尺度にすると構図的に評価が低いものも高いスコアになるため、画像を評価するネットワークから得られるスコアを指標とした。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Good_View_Hunting_Learning_Photo_Composition_from_Dense_View_Pairs.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来のデータセットでは画像に対してスコアがついていたのに対して、構図の異なる2枚の画像どちらがいいかを100万ペアアノテーションを行った。構図推薦システムは、ユーザースタディの結果従来手法よりも良いと感じる人が多いことを確認した。
また、計算速度も従来手法と比べはるかに向上した(75FPS+)．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://www.zijunwei.org/VPN_CVPR2018.html">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#334]</div><div class="timestamp">2018.5.28 00:50:47</div></div></section><section id="ID_DVQA_Understanding_Data_Visualization_via_Question_Answering"><div class="paper-abstract"><div class="title">DVQA: Understanding Data Visualization via Question Answering</div><div class="info"><div class="authors">Kushal Kafle, Brian Price, Scott Cohen, Christopher Kanan</div><div class="conference">CVPR 2018</div><div class="paper_id">694</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>新規なバーグラフに対して質問回答タスクDVQA及びデータセットの提案．</li><li>バーグラフが情報の一つとしてより豊かな統計的な情報を表現できる．提案手法がバーグラフを対象としたDVQAを提案し，バーグラフの自動的情報抽出と理解を可能にした．</li><li>大規模なバーグラフQAデータセットDVQAを提案した．DVQAが3Mのグラフ‐質問ペアから構成され，バーグラフに対し３種類の質問(構造理解，データ検索，reasoning)を設定した．また，全部の質問がopen-endedである．</li><li>DVQAタスクにおいて，2種類のネットワーク構造を提案した．①MOM:グラフの局所領域を抽出し文章を生成ことにより回答できる問題を対応するネットワークboundingbox OCR及びグラフの局所領域を抽出せずに回答する一般的な問題を対応するClassifierの二つのサブネットから構成される．どのネットにより回答するかを2クラス分類問題として取り扱っている②SANDY:従来手法SANにダイナミックエンコーディングモデルを用いて，質問文中のchart-specific単語をエンコーディングし，それをベースに直接chart-specificな回答文を生成できる．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DVQA.png" alt="DVQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>実用性が高い新規なバーグラフに対し質問回答タスクを提案．</li><li>提案データセットDVQAに対し5種類の従来のVQA手法と提案のMOM,SANDYの比較実験を行った．一般的問題・chart-specific問題の両方に対し提案のSANDYモデルが最も良い精度を達成した．</li><li>提案のデータセットDVQAがバーグラフの理解と質問文・回答文によりバーグラフ自動生成に用いられる．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>VQAタスクのVを画像からバーグラフに変更し実用性が高い提案である．</li><li>類似した考えで従来の”V”か“Q”か“A”を同じ処理で別の似た概念に変更する研究をするも面白そう</li><li><a href="https://arxiv.org/abs/1801.08163">論文</a></li></ul></div></div><div class="slide_index">[#335]</div><div class="timestamp">2018.5.25 17:28:12</div></div></section><section id="ID_RotationNet_Joint_Object_Categorization_and_Pose_Estimation_Using_Multiviews_from_Unsupervised_Viewpoints"><div class="paper-abstract"><div class="title">RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints</div><div class="info"><div class="authors">Asako Kanezaki, Yasuyuki Matsushita, Yoshifumi Nishida</div><div class="conference">CVPR 2018</div><div class="paper_id">628</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>物体のマルチ視点の画像からジョイントで3D姿勢推定及び物体認識を行う手法RotationNetの提案．</li><li>3D MFPにより作成されたマルチ視点画像データセットMIROを提案した．(12classes, 10 instances/class,160viewpoints)</li><li>物体を観測する視点及び物体のカテゴリをジョイントで推定した方がより良い精度を達成できると指摘し，更にトレーニングする際に物体を観測する視点をlatent variablesとして取り扱い，視点unalignedな学習データセットからunsupervisedで物体の姿勢推定を学習する．</li><li>また，視点-specificな特徴をクラス内だけではなく，異なるクラス間の姿勢アライメントを行う．</li><li>RotationNetのネットワーク構造はマルチ視点の画像から画像ごとにそ全部の視点の確率(その画像がその視点であるか)及び物体カテゴリを予測し，全部の画像から予測した結果から正解ラベルのクラスの確率＊視点の確率の統合を最大化するように学習する．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/RotationNet.png" alt="RotationNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>物体認識においてはSHREC’17のnormalデータに対し優勝した．また，ModelNet-10,ModelNet-40に対し従来のマルチ視点・ポイントクラウド・ボクセルベースな様々な手法より良い精度を達成．</li><li>物体姿勢推定において，無監督な方法で従来の監督方法レベルな結果が得られた．</li><li>実環境で，良い姿勢な画像をと撮影できるとは限らない．RotationNetで物体の姿勢及び認識を行う際，画像枚数（＞＝１）で動作でき，観測が更新したら予測結果を更新する．そのため，RotationNetはAR応用などの実環境の応用に適応する．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>クラス間のViewpoint-specificな特徴を学習することが面白い．可視化手法を加えて学習済みモデルに対しどういうようにアライメントしているのかを知りたい．また，問題定義を詳細的に考える必要がありそう</li><li>疑問点としては予測したそれぞれの視点の結果の統合は平均をとる？</li><li><a href="https://arxiv.org/abs/1603.06208">論文</a></li><li><a href="https://github.com/kanezaki/rotationnet">コード</a></li></ul></div></div><div class="slide_index">[#336]</div><div class="timestamp">2018.5.25 17:21:58</div></div></section><section id="ID_Visual_to_Sound_Generating_Natural_Sound_for_Videos_in_the_Wild"><div class="paper-abstract"><div class="title">Visual to Sound: Generating Natural Sound for Videos in the Wild</div><div class="info"><div class="authors">Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, Tamara Berg</div><div class="conference">CVPR 2018</div><div class="paper_id">435</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>ビデオからリアルな音声を生成する(waveformな)手法及びビデオ―音声データセットを提案した．</li><li>人がビジョンとサウンド間の関連性をある程度把握できる．そこで，in-the-wildビデオから音声(waveform型)を自動生成するタスクを提案し，また，このタスクのためのデータセットVEGASを提案した．VEGASはAudioSetデータセットをAMTよりクリーンし，10カテゴリのビデオ及び対応した音声28109ペアから構成される．データセットのビデオの総時間が55時間となる．</li><li>提案タスクに対応したフレームワークはビデオエンコーダー及び音声ジェネレータから構成される．音声ジェネレータは階層的RNNを用いた．ビデオエンコーダーに対し:①frame-to-frame②sequence-to-sequence③flow-basedの３種類の設計を用いた．3種類モデルの生成結果に対し定量評価及びヒューマンテストを用いて評価し，flow-based構造が最も良い性能とヒューマン評価を達成した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/VisualToSound_InTheWild.png" alt="VisualToSound_InTheWild"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来のビデオから音声を生成する手法はビデオに対し拘束条件を加えている．提案手法は初めてのin-the-wildビデオから音声を生成する手法．</li><li>ビデオから音声を自動生成する手法の応用場面が広い．(VRシステムでの没入感の増強，音声編集作業の自動化，視覚障害の人に視覚体験を聴覚体験として提供)</li><li>ヒューマンテスト (ビデオがリアルかフェクか)に対し，ビデオエンコーダーをflow-basedな構造を用いた場合，平均73.36%の生成音声がリアル音声と評価された．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・視覚情報の抽出機に更にコンテンツと物体relationなどを重視したネットワークを用いたら更なる良い結果が得られそう・逆設定として，音声情報からビデオの予測も面白そう</p><ul><li><a href="https://arxiv.org/abs/1712.01393">論文</a></li><li><a href="https://github.com/arXivTimes/arXivTimes/issues/562">コード</a></li></ul></div></div><div class="slide_index">[#337]</div><div class="timestamp">2018.5.25 17:15:56</div></div></section><section id="ID_Functional_Map_of_the_World"><div class="paper-abstract"><div class="title">Functional Map of the World</div><div class="info"><div class="authors">Gordon Christie, Neil Fendley, James Wilson, Ryan Mukherjee</div><div class="conference">CVPR 2018</div><div class="paper_id">795</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>建物や土地などの機能的目的を予測するタスクに用いられる大規模な衛星画像データセットfMoWの提案(bounding box, 時系列，カテゴリ，メタ情報などのアノテーションがあり)</li><li>データセットの具体的な統計情報は①200以上の国の１,047,691 枚画像②63カテゴリ③一枚の画像1つ以上のバウンディングボクス定義④時系列画像が大量に含む．</li><li>このデータセットに対応した新たなタスクを設定した：連続な時系列画像によりバウンディングボクス内の物体を認識する．提案データセットfMoWを用いて5つのネットワーク構造:LSTM-M,CNN-I,CNN-IM,LSTM-I,LSTM-IM(I:画像M:メタ特徴)に対し比較実験を行た．平均F1スコアにおいてLSTM-IMが最も高い精度を示したので，時系列情報及びメタ情報をジョイントでreasoningするアプローチの有効性を証明した</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/FunctionalMapOfTheWorld.png" alt="FunctionalMapOfTheWorld"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>公開されている最も大規模な衛星画像データセット．</li><li>異なる国・撮影時間・撮影年代などで撮影された画像から構成され，提案データセットを統計比較などにも用いられる．</li><li>従来の衛星画像データセットは主にbrief momentsの情報だけをキャプチャーし，メタ情報(ロケーション，時間，太陽角度など)がアノテーションされていない．提案データセットはメタ情報をアノテーションし，様々な応用を可能にした．(例：パーキングエリアの時系列駐車量の統計・影と時間情報によりオブジェクトの高さ推定など)</li><li>検出と識別タスクの間に位置付ける新たな問題設定“時系列画像のバウンディングボックス内の物体識別”をして，更に実験を通してメタ情報と時系列情報をジョイントで処理することの重要性を示した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>地理情報に関する分析の研究に用いられるデータセット</p></li><li><p>国のバリエーションが豊かなデータセットなので，国ごと上空シーン特徴の比較などにも用いられる</p></li><li><p><a href="https://arxiv.org/abs/1711.07846">論文</a></p></li><li><p><a href="https://github.com/fMoW">コード</a></p></li><li><p><a href="https://github.com/fMoW/dataset">データセット</a></p></li><li><p><a href="https://www.iarpa.gov/index.php/working-with-iarpa/prize-challenges/1015-functional-map-of-the-world-fmow">fMoW Challenge</a></p></li></ul></div></div><div class="slide_index">[#338]</div><div class="timestamp">2018.5.25 17:05:45</div></div></section><section id="ID_Deep_Cocktail_Networks_Multi-source_Unsupervised_Domain_Adaptation_with_Category_Shift"><div class="paper-abstract"><div class="title">Deep Cocktail Networks: Multi-source Unsupervised Domain Adaptation with Category Shift</div><div class="info"><div class="authors">Ruijia Xu, Ziliang Chen, Wangmeng Zuo, Junjie Yan, Liang Lin</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ソースドメインのラベル付きデータセットが複数ある場合のunsupervised domain adaptation(UDA)であるmultiple domain adaptation(MDA)によってターゲットドメインのクラシフィケーションを行う
Deep Cocktail Network(DCTN)を提案。MDAではUDAで問題視されるドメインシフトに加えて、
ソースドメインのデータセット間で全てのカテゴリが共有されていないカテゴリシフトが存在する。
DCTNでは、k番目のソースドメインのデータセットとターゲットドメインのデータセットを入力として
discriminatorによってperplexity scoreを算出することでどのソースドメインのデータセットの分布に近いかを算出し、
これを全てのソースドメインのデータセットに対して行い、perplexity scoreを重み付けるすることで最終的な識別結果を出力する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Cocktail_Networks_Multi-source_Unsupervised_Domain_Adaptation_with_Category_Shift.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>discriminatorによってターゲットドメインがソースドメインのデータセットのうちどのデータの分布に近いかを計算することで、MDAに取り組むDCTNを提案。</li><li>3つのベンチマークにおいてUDAのstate-of-the-artと比較し他結果、提案手法が最も高い精度を達成。</li><li>カテゴリシフトを解決できているかどうかを確認するために、ターゲットドメイン内でカテゴリの重複あり/なしにおける識別結果を比較したところ、
state-of-the-artと同等以上の精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>discriminatorが算出したperplexity scoreによって重み付けをするというシンプルな手法だが、UDAに取り組むstate-of-the-artよりも高い精度を達成している。</li><li><a href="https://arxiv.org/abs/1803.00830">論文</a></li></ul></div></div><div class="slide_index">[#339]</div></div></section><section id="ID_Unsupervised_Correlation_Analysis"><div class="paper-abstract"><div class="title">Unsupervised Correlation Analysis</div><div class="info"><div class="authors">Yedid Hoshen, Lior Wolf</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>2つのドメインを結合する手法であるCanonical Correlation Analysis(CCA、正準相関分析)を教師なし学習に対して行うUnsupervised Correlation Analysis(UCA)を提案。
既存のCCAは教師あり学習かつ2つのドメインが何らかの対応関係を持っていることを前提としていたが、
UCAは教師なし学習かつ2つのドメインに対応関係がない場合を想定している。
教師あり学習とは異なり、トレーニング時に2つのドメインにおける相関係数を計算することができないため、入力する2つのドメインと、
ネットワークによって射影された潜在変数空間の3つのドメイン間の射影、逆射影がうまくいくように様々なロスをとることで学習を行う。
ロスに対するablationも行なっている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Unsupervised_Correlation_Analysis.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>教師なしかつ2つのドメインに対応関係がない状況におけるCCAの拡張であるUCAを提案。</li><li>評価尺度として潜在変数空間における相関係数、AUCを用いて以下の5つの状況で実験を行なった。1.MNISTの画像とそのミラー画像、2.MNISTの上半分の画像と下半分の画像、3.鳥の画像とそのキャプション、4.花の画像とそのキャプション、5.Flickerの画像とそれに付随する5つの文章。
関節位置のエラーを測定したところ上記のstate-of-the-artの手法と同等、あるいは上回る精度を達成。</li><li>教師なし学習の結果をGANと比較しており、全ての実験においてGANよりも高い精度を達成。</li><li>教師あり学習をUCAで行なった結果も乗せられており、実験3、４、5において通常のCCAよりも高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>現状のネットワークを見ると、それぞれのドメインにおける直交性と、それぞれのドメインの射影先が同じ空間になるように様々なロスをとっているだけなので、
もう少しアップデートすることができるかもしれない。</li><li>CCAの特徴であるL_Orthだけを除いた場合に、どれほどの影響が出るのかが気になった。</li><li><a href="https://arxiv.org/abs/1804.00347">論文</a></li></ul></div></div><div class="slide_index">[#340]</div></div></section><section id="ID_Transferable_Joint_Attribute-Identity_Deep_Learning_for_Unsupervised_Person_Re-Identification"><div class="paper-abstract"><div class="title">Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification</div><div class="info"><div class="authors">Jingya Wang, Xiatian  Zhu, Shaogang Gong, Wei Li </div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルなしデータセットにおいてperson re-identification(re-id)を教師なしで行うために、ラベルありデータセットからdomain adaptationを行うTransferable Joint Attribute-Identity 
Deep Learning(TJ-AIDL)を提案。person re-idとは、街中の監視カメラのような異なる視点、
重複のない領域を撮影された映像内の同一人物を探すことである。
TJ-AIDLにはアイデンティティーを推定するIdentity branch、アトリビュートを推定するAttribute branch、
アトリビュートからアイデンティティーを推定するモジュールであるIdentity Inferred Attirbute(IIA)からなる。
domain adaptationの際には、Attribute branch、IIAの更新のみを行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Transferable_Joint_Attribute-Identity_Deep_Learning_for_Unsupervised_Person_Re-Identification.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>domain adaptationを用いて教師なしでperson re-idを行うために、画像のアトリビュートからアイデンティティーを推定するTJ-AIDLを提案。</li><li>personn re-idのベンチマークである4つのデータセットを使用しており、Rank-1mAPにおいてre-idを教師なしで行うstate-of-the-artよりも高い精度を達成。</li><li>TJ-AIDLにおいてアトリビュート/アイデンティティーのみ学習した際の結果、adaptation有り/無しの結果についても議論しており、提案したTJ-AIDLが最も高い精度となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.09786">論文</a></li></ul></div></div><div class="slide_index">[#341]</div></div></section><section id="ID_Duplex_Generative_Adversarial_Network_for_Unsupervised_Domain_Adaptation"><div class="paper-abstract"><div class="title">Duplex Generative Adversarial Network for Unsupervised Domain Adaptation</div><div class="info"><div class="authors">Lanqing Hu, Meina Kan, Shiguang Shan, Xilin Chen</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>同一カテゴリのdomain間におけるadaptation, transferをラベル識別と2つのdiscriminatorを用いるネットワークDupGANを提案。target domainにはラベルがない状況である教師なし学習を対象としている。
DupGANはencoderでそれぞれのドメインの潜在変数をエンコードし、generatorでデコードを行い、
2つのdiscriminatorでそれぞれのドメインに対してfake/realとラベルの認識を行う。
結果はdomain transferされた数字画像のラベル認識・生成結果、物体認識の精度において比較を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Duplex_Generative_Adversarial_Network_for_Unsupervised_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ラベル認識と2つのdiscriminatorによってdomain adaptaion/transferをおこなうDupGANを提案。</li><li>既存手法である<a href="http://jmlr.org/papers/volume17/15-239/15-239.pdf">DANN</a>、<a href="https://arxiv.org/abs/1702.05464">ADDA</a>はadversarial lossを使用してtarget→source のマッピングを行うが、
これらの手法ではマッピングされたtarget domainの分布が歪んでいないことは保証できない。
一方DupGANではラベルの認識を行わせることでカテゴリ構造を保つことができる。また提案手法では画像の生成も可能である。</li><li>state-of-the-artと比較して、数字画像データセットであるMNIST、USPS、SVHN、SVHN-extraそれぞれのデータセット間におけるdomain transferに対するラベル認識の結果、
最も高い精度を達成。またdomain transferによる画像も生成することが可能。</li><li>31種類のラベル、3つのドメインを持つOffice-31データセットにおける物体認識結果がstate-of-the-artよりも高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>クラシフィケーション生成された画像ではなくはエンコードされた潜在変数に対して行われている。</li><li>画像の生成力はそこまで高くなく、実際Office31に対する画像生成は難しかったと主張している。</li><li><a href="http://vipl.ict.ac.cn/uploadfile/upload/2018041610083083.pdf">論文</a></li></ul></div></div><div class="slide_index">[#342]</div></div></section><section id="ID_Pixels_voxels_and_views_A_study_of_shape_representations_for_single_view_3D_object_shape_prediction"><div class="paper-abstract"><div class="title">Pixels, voxels, and views: A study of shape representations for single view 3D object shape prediction</div><div class="info"><div class="authors">Daeyun Shin, Charless Fowlkes, Derek Hoiem</div><div class="conference">CVPR 2018</div><div class="paper_id">384</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>1枚の画像から3次元形状を推定するタスクにおいて，異なる形状representation及びcoordinate framesを用いた場合，精度がどのように変化するのかの徹底的比較実験に関する研究．</li><li>従来形状推定タスクにおいて異なる設計の比較分析の研究がないので，著者達が異なる設計を比較できるフレームワーク及び具体的な実験を行った．</li><li>比較実験は具体的に，a.RGB画像b.デプス画像からの形状推定タスクにおいて，“①マルチサーフェス画像VS volumetricデータ表示②viewer-centered VS object-centeredな座標”などの設定に対し，定量的及び定性的な比較実験を行った．</li><li>提案の比較用フレームワークはencoder-decoderベースなネットワークを用いて，decoderに変更を加えることで， マルチサーフェス画像及び volumetricデータの2種類を生成できるようにした．また，coordinate frameをスイッチすることにより，viewer/object centeredを変更できる．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/A_Study_Of_Shape_Representations.png" alt="A_Study_Of_Shape_Representations"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>3次元形状推定タスクにおいて，異なる設定の比較実験を行った．</li><li>形状representationの設定において，Multi-surfaceの方がvoxel と比べunseenクラスにおいてより良い性能を達成した． Multi-surfaceの方が高い解像度をエンコーディングできるのが理由な可能性があると指摘した．</li><li>coordinate framesの設定において，意外なことに従来広く採用されているobject-centeredはviewer-centeredと比べunseenクラスにおいて精度が劣っていて， object-centeredの方がカテゴリ認識に対応が強いのが原因となることを指摘した．</li><li>以上の結論を元に，object centeredなsurface-basedな1枚の画像から3次元形状推定の手法3D-R2N2を提案し，PASCAL 3D+データセットにおいてmean IoU0.414を達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>比較をしていない設計(Oct-tree based representationなど)もあるので，そういった構造に対して比較実験を行うのも面白い．</p></li><li><p>3次元あたりの徹底的比較を行って，何らかの結論を出すような研究がまだ少ないので，研究テーマを沢山作れるかも？</p></li><li><p><a href="https://arxiv.org/abs/1804.06032">論文</a></p></li></ul></div></div><div class="slide_index">[#343]</div><div class="timestamp">2018.5.24 18:20:50</div></div></section><section id="ID_PlaneNet_Piece-wise_Planar_Reconstruction_from_a_Single_RGB_Image"><div class="paper-abstract"><div class="title">PlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image</div><div class="info"><div class="authors">Chen Liu, Jimei Yang, Duygu Ceylan, Ersin Yumer, Yasutaka Furukawa</div><div class="conference">CVPR 2018</div><div class="paper_id">336</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>1枚のRGB画像から“piece-wise planar depthmap”を推定するend-to-endなネットワークを提案した．提案手法を用いてRGB画像から平面パラメータ及び平面セグメンテーションマスク及びデプスマップを同時に推定できる．</li><li>画像からpiece-wiseな平面を検出するタスクはARの応用に一つ重要なタスクとなっている．しかし従来，デプス推定とpiece-wiseな平面検出を同時に行う研究がない．著者達が新たにこのタスク及びタスクに対応できるネットワークを定義した．</li><li>提案フレームワークは:①DRNs(Dilated Residual Networks)を用いて入力画像から特徴抽出を行う②平面パラメータ推定・non-planarデプスマップ推定・セグメンテーションマスク推定の3つの推定ネットワークを用いる③推定した3つの結果から“piece-wise planar depthmap”を生成する．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/PlaneNet_PieceWise_PlaneEstimation.png" alt="PlaneNet_PieceWise_PlaneEstimation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>新規な問題定義．実験で提案手法が部屋のレイアウト推定・ARアプリ(テクスチャー編集・バーチャルルーラーなど)に応用できることを指摘した．</li><li>51,000枚ほどの学習データを作成した．(これが大変そう)</li><li>plane segmentationタスクにおいてNYUデータセットでの精度が従来の三つの手法より優れている(比較している手法は2009年，2009年，2012年の手法だけど。。)</li><li>デプスマップ推定タスクにおいてNYUv2データセットにおいて前述した３つの手法より精度良い</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>ARアプリに応用できるところから考えると単純なデプス推定より実用性が高い</p></li><li><p>平面検出も同時に行うので，部屋レイアウト推定に良い精度を達成したのが理解できる．しかし，疑問としては提案手法が平面検出＋デプス推定だけで部屋の幾何構造実際は学習していないので，デプス推定＋平面パーツ検出の従来研究と比べると新規性と技術的の難しさがどこなのかちょっとわからない</p></li><li><p><a href="https://www.cse.wustl.edu/~chenliu/planenet/paper.pdf">論文</a></p></li><li><p><a href="https://github.com/art-programmer/PlaneNet">コード</a></p></li><li><p><a href="http://www.cse.wustl.edu/~chenliu/planenet.html">プロジェクト</a></p></li></ul></div></div><div class="slide_index">[#344]</div><div class="timestamp">2018.5.24 18:13:54</div></div></section><section id="ID_PointNetVLAD_Deep_Point_Cloud_Based_Retrieval_for_Large-Scale_Place_Recognition"><div class="paper-abstract"><div class="title">PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place Recognition</div><div class="info"><div class="authors">Mikaela Angelina Uy, Gim Hee Lee</div><div class="conference">CVPR 2018</div><div class="paper_id">573</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>PointNetとNetVLADを用いたポイントクラウドベースな“場所検索”ネットワークPointNetVLAD及びデータセットの提案．</li><li>従来の自動運転などに用いられる場所検索技術では2次元画像ベースで行われている．しかし，照明条件などに対しロバスト性が低い．ポイントクラウドベースな場所検索が従来良いグローバル特徴抽出機がないため，まだ研究されていない．近年PointNetなどの良いポイントクラウド特徴抽出機が提案され，そこで著者達がPointNetとNetVLADを用いたLiDARで撮ったポイントクラウドをベースとした場所検索手法を提案した．</li><li>提案データセットの収集過程は:①Oxford RobotCar などのdatasetからフルールートを選択する②フルールートから局所を選択する③選択した局所ポイントクラウドをダウンサンプルと正規処理を行う．また，Oxford RobotCar 以外，3種類の他のデータセットからデータを集めた．</li><li>fixedサイズなポイントクラウドからグローバル特徴を抽出できるPointNet，NetVLADと全結合層をコンバインたend-to-endなグローバル特徴抽出機を構築した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/PointNetVLAD.png" alt="PointNetVLAD"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>新規なポイントクラウドベースな場所検索及び場所検索3次元ポイントクラウドデータセットの提案．</li><li>従来の2次元画像ベースな場所検索と比べ，提案したポイントクラウドベースな場所検索が照明条件にロバストである．</li><li>PointNetとNetVLADを用いているので，ポイントクラウドの無順序性及びpermulationを対応できる．</li><li>新規なロス関数Lazy quadrupletを定義した．</li><li>提案データセットにおいて，PointNetとModelNetなどの従来手法と比べ良い検索精度達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>PointNet，PointNet++, Kd-networkなどのポイントクラウドデータを扱えるネットワークでポイントクラウドから情報抽出を利用した研究がこれからまだ増えるのかな？</p></li><li><p>ポイントクラウドデータを直接処理できるネットワークがいくつかあるが，主にPointNet，PointNet++が引用されていそう．ほかの手法があまり使われていない理由が知りたい</p></li><li><p><a href="https://arxiv.org/abs/1804.03492">論文</a></p></li><li><p><a href="https://github.com/mikacuy/pointnetvlad">コード</a></p></li></ul></div></div><div class="slide_index">[#345]</div><div class="timestamp">2018.5.24 18:03:54</div></div></section><section id="ID_Pix3D_Dataset_and_Methods_for_3D_Object_Modeling_from_a_Single_Image"><div class="paper-abstract"><div class="title">Pix3D: Dataset and Methods for 3D Object Modeling from a Single Image</div><div class="info"><div class="authors">Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Tianfan Xue, Joshua Tenenbaum, William Freeman</div><div class="conference">CVPR 2018</div><div class="paper_id">375</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>大規模なピクセルレベルに対応付けられたimage-shape pairsデータセットPix3Dの提案及び画像から同時に三次元形状及び姿勢を推定するネットワークの提案．</li><li>従来のimage-shape pairsデータセットは①合成データセットを用いる②image-shapeの対応が精密ではない③データセット規模が小さいなどの問題点がある．そこで，著者達が大規模なピクセルレベルに対応付けられたデータセットを提案した．Pix3Dは395個の3次元物体モデル(9カテゴリ)，10069ペアの画像―形状ペアから構成される．画像と形状のペアはピクセルレベルの精密的に対応付けられている．</li><li>データセットの収集段階では:①IKEA及び自撮りで大量な画像―形状ペアを集める②AMTにより画像からキーポイントをアノテーションする③Efficient PnP及びLevenberg-Marquardtを用いて粗い・精密なposeを求める．</li><li>更に，提案手法は画像から同時に姿勢及び3次元形状を予測できるネットワークを提案した．提案ネットワークはまず画像から2.5Dスケッチを推定し，推定したスケッチをエンコーディングする．また，デコーディングにより3次元形状を推定し，同時にview estimatorネットワークにより姿勢を推定する．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Pix3D.png" alt="Pix3D"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来のデータセットではCGモデルで合成されている方が多く，提案のデータセットが実物体を用い，更にピクセルレベルな精密度の画像―形状対応付けアノテーションがある．</li><li>画像から同時に形状姿勢を推定するフレームワークの定量化結果は提案したデータセットでは3D-VAE-GAN,MarrNetなどの従来手法より良い精度を達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>現在の学習データアノテーション段階でAmazon Mechanical Turkを用いている．Semantic Keypointの自動的検出を用いたら自動化できることはデータセットの更なる拡大化につなぎられそう</p></li><li><p><a href="https://arxiv.org/abs/1804.04610">論文</a></p></li><li><p><a href="https://github.com/xingyuansun/pix3d">コード</a></p></li></ul></div></div><div class="slide_index">[#346]</div><div class="timestamp">2018.5.24 17:57:53</div></div></section><section id="ID_Learning_to_Look_Around_Intelligently_Exploring_Unseen_Environments_for_Unknown_Tasks"><div class="paper-abstract"><div class="title">Learning to Look Around: Intelligently Exploring Unseen Environments for Unknown Tasks</div><div class="info"><div class="authors">Dinesh Jayaraman, Kristen Grauman</div><div class="conference">CVPR 2018</div><div class="paper_id">152</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>新規な問題設定“シーンや物体を有効的に観測できる視点を学習する”及びこの問題を対応できる “アクティブ観測補完”ネットワークの提案．</li><li>従来のCVタスクは主に与えられた観測(画像・ビデオ・ポイントクラウドなど)から視覚性質(クラス分類・検出など)の分析を行う．しかし，リアルな知能はまず環境から目的を達成するための観測を取得することから始まる．また，異なる観測から得られる情報量も異なる．そこで，著者達が“active observation completion”タスクを提案し，未知なシーンかオブジェクトからシーン及び物体のより多く3次元情報が含めた数が限られた観測視点の推定を目標とする．</li><li>提案手法は強化学習を用いる．RNNベースなネットワークを用いて選択された視点からシーンか物体のパーツ情報を統合する．また，統合されたモデルから推定できるunobserved視点とgt間の誤差をベースにロス関数を設定した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/LearningToLookAround.png" alt="LearningToLookAround"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>学習データを手動でラベリングする必要がないので，大量な学習が行える．</li><li>提案フレームワークを“シーン”の補完及び“物体モデル”の補完の2種類だいぶ異なったタスクに実験を行い，良い精度を達成したので，”提案した“無監督探索的な”フレームワークを遷移学習でほかのタスクに用いられる．</li><li>SUN360(Scene dataset)及び”ModelNet” (Object dataset)を用いて，従来のいくつかベースとなる手法より良い精度を達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>Interactive 環境でのVQAタスク(Embodied Question Answeringなど)は環境から“情報量が豊かな画像”を集めるのが重要の一環なので，提案フレームワークを用いられそう．</p></li><li><p><a href="https://arxiv.org/abs/1709.00507">論文</a></p></li></ul></div></div><div class="slide_index">[#347]</div><div class="timestamp">2018.5.24 17:50:58</div></div></section><section id="ID_PU-Net_Point_Cloud_Upsampling_Network"><div class="paper-abstract"><div class="title">PU-Net: Point Cloud Upsampling Network</div><div class="info"><div class="authors">Lequan Yu, XIANZHI LI, Chi-Wing Fu, Daniel  Cohen-Or, Pheng-Ann Heng</div><div class="conference">CVPR 2018</div><div class="paper_id">355</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>data-drivenなポイントクラウドアップサンプリング手法の提案．スパースなポイントクラウドから，もっとデンスでユニフォームなポイントクラウドを取得できる．</li><li>従来の2D画像super-resolutionタスクと比べ，3D Upsamplingでは処理対象が空間オーダーとレギュラー構造がないポイントクラウドで，物体の本当のサーフェス(ポイントクラウドのリアル物体)に近づき，点の密度も均等であることがタスクの目標となる．こういったことから，提案手法はポイントクラウドからマルチレベルの特徴を抽出し，更にマルチブランチで特徴を拡張することにより，ポイントクラウドの局所及びグローバルな情報を取得できる．</li><li>提案ネットワークPU-Netは入力のポイントクラウド(N points)に対し①ポイントクラウドに対し異なるスケールのパッチを抽出し，②パッチからPointNet++を用いたマルチレベルの特徴抽出を行う．③feature expansion構造により特徴を拡張し，④全結合層を用いて出力のポイントクラウド(N＊ｒ points)を生成する．また，物体のサーフェスまでの距離及びポイントクラウドの過密程度を基準に，ジョイントロスを設計した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/PU_Net.png" alt="PU_Net"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>新たな評価指標：“物体のサーフェスまでの距離偏差”及び“ポイントクラウド分布のユニフォーム性”を評価できる指標を提案し，この2つの指標においてSHREC2015データセットに対し従来研究より優れた精度と指摘した．</li><li>Pointnet++を用いてローカル及びグローバル情報抽出を行うので，ポイントクラウドの幾何的無オーダーを対応できる</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>提案手法を更に発展し物体モデルの補完およびアップサンプリング同時にできることを期待される</p></li><li><p>Pointnet++を基本構造として使っていることがすごそう</p></li><li><p><a href="https://arxiv.org/abs/1801.06761">論文</a></p></li></ul></div></div><div class="slide_index">[#348]</div><div class="timestamp">2018.5.24 17:36:47</div></div></section><section id="ID_Deep_Unsupervised_Saliency_Detection_A_Multiple_Noisy_Labeling_Perspective"><div class="paper-abstract"><div class="title">Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling Perspective</div><div class="info"><div class="authors">J.Zhang, T.Zhang, Y.Daiy, M.Harandi, and R.Hartley</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1803.10910</div></div><div class="slide_editor">KotaYoshdia</div><div class="item1"><div class="text"><h1>概要</h1><p>深層学習を用いた教師あり学習による顕著性の検出方法は教師データに依存する．そこで，“汎化能力を改善しつつ教師データなしで顕著性マップを学習することは可能か？”という問いに対して，弱いものやのノイズのある教師なし顕著性検出手法によって生成される多数のノイズラベルを学習することによって教師なしで顕著性の検出を行った．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Unsupervised_Saliency_Detection_A_Multiple_Noisy_Labeling_Perspective.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来の教師なし顕著性検出に新たな顕著性を推定し，複数のノイズの多い顕著性検出方法から顕著性マップを学習する．</li><li>我々の深層学を用いた顕著性検出モデルは，人間のアノテーションなしでEnd to Endで学習できとても簡潔である．</li></ul></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><ul><li>評価実験をしたところ従来の教師なしの顕著性検出方法を大きく上回り，深層学習を用いた顕著性の精度と同等のものとなった．</li><li><a href="https://arxiv.org/pdf/1803.10910">Paper</a></li></ul></div></div><div class="slide_index">[#349]</div><div class="timestamp">2018.5.23 20:28:11</div></div></section><section id="ID_Cross-View_Image_Synthesis_using_Conditional_GANs"><div class="paper-abstract"><div class="title">Cross-View Image Synthesis using Conditional GANs</div><div class="info"><div class="authors">Krishna Regmi and Ali Borji</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>対応する航空写真とストリートビュー写真間の変換を行うcGANを提案．pix2pixによる変換に比べて，オブジェクトの正しいセマンティックスを捉え維持する変換が可能となっている．提案したcGANモデルは２つあり，X-Fork とX-Seq と呼んでいる．出力が変換画像とセグメンテーションマップであることが特徴．Inception Scoreの比較実験をすると，航空写真からストリートビュー方向の変換ではがX-Forkが優れ，逆方向の変換ではX-Seqの生成結果が優れていることがわかった．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Cross-View_Image_Synthesis_using_Conditional_GANs_fig.png" alt="Image"><br>256x256の解像度で生成可能．gがストリートビューで，aが航空写真に当たる．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>X-Forkは１つのGeneratorと１つのDiscriminatorから成るシンプルな構成のcGAN．出力は変換後の画像とセグメンテーションマップの２つであることが特徴．</li><li>X-Seqは２つのGeneratorと２つのDiscriminatorから成るcGAN．１つ目のGeneratorで変換後の画像を生成．それを元に２つ目のGeneratorでセグメンテーションマップを生成する．<br>セグメンテーションマップのGround-Truthには，学習済みのRefineNetを用いた生成結果を使用している．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>航空写真とストリートビューという劇的に見た目が変わる場合の変換において，どのようなことが問題点となるのか５つ挙げられていたので気になる場合は元論文を参照してください．</li><li>コードやデータは公開予定</li><li><a href="https://arxiv.org/abs/1803.03396">arXiv</a></li></ul></div></div><div class="slide_index">[#350]</div><div class="timestamp">2018.5.23 20:24:52</div></div></section><section id="ID_Multimodal_Explanations_Justifying_Decisions_and_Pointing_to_the_Evidence"><div class="paper-abstract"><div class="title">Multimodal Explanations: Justifying Decisions and Pointing to the Evidence</div><div class="info"><div class="authors">D. H. Park et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">性能がよく，かつ説明可能なモデルの実現のための新規手法の提案．
これまでの説明可能なモデルは視覚的なAttentionのみやテキストの説明のみという単一のmodalだけだったのに対して，
この論文では両者を合わせたmulti-modalな説明を出力可能にした．
それを行う手法の提案と，学習と評価に使うデータセットを構築したのがこの論文のContribution．
データセットはVQAと静止画からのActivity Recognitionのタスクで，
従来あったデータセットに，理由のテキスト説明と視覚的な根拠となった領域のアノテーションを追加して作成．
手法は，まず答えを出力して，それを元に根拠となった理由を出力するという形式のネットワーク構造を採用．
</div></div><div class="item2"><img src="slides/figs/Multimodal_Explanations_Justifying_Decisions_and_Pointing_to_the_Evidence.png" alt="Multimodal_Explanations_Justifying_Decisions_and_Pointing_to_the_Evidence.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>モデルの出力に加えて視覚的，テキストのmulti-modalな根拠説明をする手法を提案</li><li>VQAとActivity Recognitionでそれを評価可能なデータセット（追加アノテーション）を構築</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://arxiv.org/abs/1802.08129">論文 (arXiv)</a></li><li>データセットはまだ公開されていない模様</li></ul></div></div><div class="slide_index">[#351]</div><div class="timestamp">2018.5.8 12:00:18</div></div></section><section id="ID_A_Variational_U-Net_for_Conditional_Appearance_and_Shape_Generation"><div class="paper-abstract"><div class="title">A Variational U-Net for Conditional Appearance and Shape Generation</div><div class="info"><div class="authors">Patrick Esser, Ekaterina Sutter, Björn Ommer</div><div class="conference">CVPR 2018 Spotlight</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>画像を構成する成分はshape(ジオメトリ、ポーズなど)とappearanceであるという考えのもと、VAEによってappearanceを推定し、
U-Netにshapeを学習させることで入力画像のappearanceとshapeの
片方を保ったままもう一方を変更することが可能なVariational U-Netを提案。
通常のVAEではshape、appearanceの分布を分離することが不可能なため、
VAEに画像とshapeを入力することでappearanceの特徴量を抽出し、U-Netによってshape情報を保つように学習を行う。
shapeとして体のポーズや線画が入力される。トレーニングデータには同一物体に対する様々なバリエーションの画像は必要としない。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/A_Variational_U-Net_for_Conditional_Appearance_and_Shape_Generation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>VAEでappearanceを、U-Netでshapeを学習させることで画像に内在する2つの事前分布を別々に学習することができるVarational U-Netを提案。</li><li>コンディションによって画像を編集するpix2pixとポーズをコンディションとして人物画像を編集するPG2と比較を行った。COCO、DeepFashion、Market-1501データセットにおいてSSIMやIS、
関節位置のエラーを測定したところ上記のstate-of-the-artの手法と同等、あるいは上回る精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>VAEとU-Netのいいとこ取りをすることで、2つの変数を扱うことが可能になった。</li><li><a href="https://arxiv.org/abs/1804.04694">論文</a></li><li><a href="https://compvis.github.io/vunet/">Project page</a></li><li><a href="https://github.com/CompVis/vunet">GitHub</a></li></ul></div></div><div class="slide_index">[#352]</div></div></section><section id="ID_Total_Capture_A_3D_Deformation_Model_for_Tracking_Faces_Hands_and_Bodies"><div class="paper-abstract"><div class="title">Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies</div><div class="info"><div class="authors">Hanbyul Joo, Tomas Simon, Yaser Sheikh</div><div class="conference">CVPR 2018 Oral</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>表情、体全体の動き、手のジェスチャといった様々なスケールの動きをマーカー無しでキャプチャするdeformation modelである”Frankenstein”と”Adam"を提案。
3Dキャプチャシステムに置いて、画像の解像度と3Dキャプチャシステムの視野はトレードオフであるため、
体の局所的な動きと全体的な動きを同時に捉えことは難しかった。提案手法では顔、両手、両足、
手の指における3Dキーポイントと3D Point Cloudを用いて表情などの
局所的モーションと体全体のモーションをキャプチャすることができるFrankensteinを構築。
また70人のトラッキングデータを用いてFrankensteinモデルを最適化することで、
髪と服を表現することが可能なAdamモデルを提案。結果は既存手法とのトラッキングの精度によって比較している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Total_Capture_A_3D_Deformation_Model_for_Tracking_Faces_Hands_and_Bodies.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>表情や手のジェスチャといった局所的なモーションと、体全体の動きを同時にトラッキングすることが可能なdefromation modelを提案。620台のVGAカメラと31台のHDカメラが必要とする。</li><li>state-of-the-artである<a href="http://files.is.tue.mpg.de/black/papers/SMPL2015.pdf">SMPL</a>では顔の表情を表現することは不可能だが、提案手法では可能になっている。</li><li>SMPLとトラッキングにおけるGTとのオーバーラップを計算した結果、SMPLが84.79%であるのに対し提案手法は87.74%となり、提案手法の方が高い精度を達成</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1801.01615">論文</a></li><li><a href="http://www.cs.cmu.edu/~hanbyulj/totalcapture/">Project Page</a></li><li><a href="https://www.youtube.com/watch?v=5QzdXQSf-oY">Video</a></li></ul></div></div><div class="slide_index">[#353]</div></div></section><section id="ID_SfSNet_Learning_Shape_Reflectance_and_Illuminance_of_Faces_in_the_Wild"><div class="paper-abstract"><div class="title">SfSNet: Learning Shape, Reflectance and Illuminance of Faces in the Wild</div><div class="info"><div class="authors">Soumyadip Sengupta, Angjoo Kanazawa, Carlos D. Castillo, ; David Jacobs</div><div class="conference">CVPR 2018 Spotlight</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ラベル付き合成顔画像とin-the-wildなラベルなし実顔画像のどちらもトレーニングデータとして使用することで、実顔画像からシェイプ、リフレクタンス、イルミネーションを推定してリコンストラクションをend-to-endに行うSfSNetを提案。
実顔画像に十分なラベルがついているデータセットが存在しない、という問題を解決。Shape from Shading(SfS)のアイディアに基づき、
低周波成分を合成顔画像から、高周波成分を実顔画像から推定する。リコンストラクションされた画像のL1ロスを取ることで、
トレーニングにおける合成顔画像と実画像の橋渡しが行われる。リコンストラクションにはランバーシアンレンダリングモデルを使用する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SfSNet_Learning_Shape_Reflectance_and_Illuminance_of_Faces_in_the_Wild.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ラベル付きの合成顔画像とラベルなしの実世界顔画像でトレーニングすることで、実世界顔画像の法線、アルベド、シェーディングを推定しインバースレンダリングを行うSfSNetを提案。</li><li>インバースレンダリングによってリコンストラクションされた画像のロスを取ることで、合成顔画像と実世界顔画像の橋渡しを実現。</li><li>インバースレンダリングの見た目がstate-of-the-artよりも良い結果となった。</li><li>法線・シェーディングの推定精度が、法線・シェーディング単体をそれぞれ推定するstate-of-the-artよりも良い結果となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>画像をリコンストラクションする際によく使われるU-NetではなくResNetを使った理由についても議論されている。</li><li><a href="https://arxiv.org/abs/1712.01261">論文</a></li><li><a href="https://senguptaumd.github.io/SfSNet/">Project Page</a></li><li><a href="https://github.com/senguptaumd/SfSNet">GitHub</a></li></ul></div></div><div class="slide_index">[#354]</div></div></section><section id="ID_Whos_Better_Whos_Best_Pairwise_Deep_Ranking_for_Skill_Determination"><div class="paper-abstract"><div class="title">Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination</div><div class="info"><div class="authors">Hazel Doughty, Dima Damen and Walterio Mayol-Cuevas</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>2つの動画から、手術や絵を描くなどの技能がどちらが上かを予測する手法の提案。入力動画をTemporal Segment Networks(リンク参照)によりいくつかのセグメントに分割し，技能評価に用いるフレームを3枚選択する。
技能評価の学習は、2つの動画のどちらが技能が上か、2つの動画の技能が同じであるとき同じであると判定できるかの2つの尺度をロスとして行う。
技能を表すスコアは、Two Stream CNN(リンク参照)によって空間と時間それぞれについてスコアを取得する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Whos_Better_Whos_Best_Pairwise_Deep_Ranking_for_Skill_Determination.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>手術、ピザ生地をこねる、絵を描く、箸を使うの4つの技能を撮影したデータセットにより実験を行った。そのうち絵を描く、箸を使うは新たにデータセットを構築した。
全てのタスクで70%以上の精度を達成し、箸を使う以外のタスクではベースラインと比べ精度が向上した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1703.09913">論文</a></li><li><a href="https://arxiv.org/abs/1406.2199">Two Stream CNN</a></li><li><a href="https://arxiv.org/abs/1608.00859">Temporal Segment Networks</a></li></ul></div></div><div class="slide_index">[#355]</div><div class="timestamp">2018.5.22 17:48:35</div></div></section><section id="ID_LiteFlowNet_A_Lightweight_Convolutional_Neural_Network_for_Optical_Flow_Estimation"><div class="paper-abstract"><div class="title">LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation</div><div class="info"><div class="authors">T. Hui et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">FlowNet2よりも，性能が良く，モデルサイズが小さく，高速に動作するOptical Flow推定手法を提案．
FlowNet2（Feature Warping, Correlation）は性能が良いけどモデルサイズが大きい，
SPyNet（ピラミッド構造を採用）はモデルが小さいけど性能はあまり良くない，
ということで，提案手法は両者の良いところを合わせることをしている．
2フレームを入力として，各フレームをCNNに入れてピラミッド構造の特徴表現を得る．
一番解像度の低いところから順にFlow推定を繰り返していって洗練化していく．
各Flow推定では軽量な2つのモデルをカスケードさせたりして2フレーム間の大きな移動にも対応しながら，
軽量かつ高速な推定を実現．
</div></div><div class="item2"><img src="slides/figs/LiteFlowNet_A_Lightweight_Convolutional_Neural_Network_for_Optical_Flow_Estimation.png" alt="LiteFlowNet_A_Lightweight_Convolutional_Neural_Network_for_Optical_Flow_Estimation.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>軽量な2つのネットワークをカスケードさせて使うCascaded flow inferenceの提案</li><li>CNNベースのFlow推定にFlow Regularizationを導入</li><li>高性能，省メモリ，高速な推定を実現</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://arxiv.org/abs/1805.07036">論文 (arXiv)</a></li><li><a href="http://mmlab.ie.cuhk.edu.hk/projects/LiteFlowNet/">プロジェクトページ</a></li><li><a href="https://github.com/twhui/LiteFlowNet">コード (GitHub)</a></li><li>カスケード構造が複雑でなぜこれが良いのか少し納得しにくい</li><li>実験は各コンポーネントのON/OFFで性能比較がわかりやすい</li></ul></div></div><div class="slide_index">[#356]</div><div class="timestamp">2018.5.8 12:00:18</div></div></section><section id="ID_Person_Transfer_GAN_to_Bridge_Domain_Gap_for_Person_Re-Identification"><div class="paper-abstract"><div class="title">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</div><div class="info"><div class="authors">Longhui Wei, Shiliang Zhang, Wen Gao and Qi Tian</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>Person Re-identification (ReID)のパフォーマンスは大きく向上したが，複雑なシーンや照明の変化、視点や姿勢の変化といった問題の調査は未だなされていない．本稿ではこれらの問題に関する調査を行った．このためにMulti-Scene MultiTime person ReID dataset (MSMT17)を構築した．またドメインギャップがデータ間に存在するため，このドメインギャップを埋めるためのPerson Transfer Generative Adversarial Network (PTGAN)を提案した．実験ではPTGANによってドメインギャップを実質的に狭められることを示した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180522_PTGAN1.png" alt="1"><img src="slides/figs/20180522_PTGAN2.png" alt="2"><img src="slides/figs/20180522_PTGAN3.png" alt="3"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ReIDを行う際の現実的な問題について網羅的に調査</li><li>新たなReIDデータセットMSMT17を構築</li><li>データ間のドメインギャップを埋めるPTGANを提案</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1711.08565.pdf">論文</a></li></ul></div></div><div class="slide_index">[#357]</div><div class="timestamp">2018.5.22 17:09:22</div></div></section><section id="ID_Zero-Shot_Sketch-Image_Hashing"><div class="paper-abstract"><div class="title">Zero-Shot Sketch-Image Hashing</div><div class="info"><div class="authors">Yuming Shen, Li Liu, Fumin Shen and Ling Shao</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>大規模スケッチベース画像検索において，既存の手法では学習中にカテゴリの存在しないスケッチクエリがある場合失敗するという問題がある．本稿ではそのような問題を解決するZero-shot Sketch-image Hashing(ZSIH)モデルを提案した．2つのバイナリエンコーダとデータ間の関係を強化する計3つのネットワークで構成される．重要な点として，Zero-shot検索での意味的な表現を再構成する際に生成的ハッシングスキームを定式化する点である．Zero-shotハッシュ処理を行う初のモデルであり，関連する研究と比較しても著しく精度が向上した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180522_ZSIH1.png" alt="1"><img src="slides/figs/20180522_ZSIH2.png" alt="2"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>スケッチイメージハッシングの研究において初のZero-shot</li><li>意味的な表現を再構成する際に生成的ハッシングスキームを定式化</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.02284.pdf">論文</a></li></ul></div></div><div class="slide_index">[#358]</div><div class="timestamp">2018.5.22 16:03:53</div></div></section><section id="ID_Lions_and_Tigers_and_Bears_Capturing_Non-Rigid_3D_Articulated_Shape_from_Images"><div class="paper-abstract"><div class="title">Lions and Tigers and Bears: Capturing Non-Rigid, 3D, Articulated Shape from Images</div><div class="info"><div class="authors">Silvia Zuffi, Angjoo Kanazawa and Michael J. Black</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>3Dスキャンは人間をキャプチャするために設計されており，自然環境での使用や野生動物のスキャンおよびモデリングには不向きという問題がある．この問題を解決する方法として，画像から3Dの形状を取得する方法を提案した．SMALモデルを画像内の動物にフィット，形状が一致するようにモデルの形状を変形(SMALR)，さらに複数の画像においても整合性がとれるよう姿勢を変形させ、詳細な形状を復元する．本手法は，従来の手法に比べ大幅に3D形状を詳細に抽出することを可能にするだけでなく，正確なテクスチャマップを抽出し，絶滅した動物といった新しい種についてもモデル化できることを可能にした．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180501_lion.png" alt="Item3Image"><img src="slides/figs/20180501_lion2.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>3Dスキャンが困難な動物のモデルを構築する方法を提案</li><li>SMALモデルを基として形状を変形させることで，より詳細な3D復元が可能</li><li>上記手法により，一貫したテクスチャマップの抽出が可能</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://files.is.tue.mpg.de/black/papers/zuffiCVPR2018.pdf">論文</a></li></ul></div></div><div class="slide_index">[#359]</div><div class="timestamp">2018.5.22 15:06:58</div></div></section><section id="ID_DOTA_A_Large-scale_Dataset_for_Object_Detection_in_Aerial_Images"><div class="paper-abstract"><div class="title">DOTA: A Large-scale Dataset for Object Detection in Aerial Images</div><div class="info"><div class="authors">Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, Liangpei Zhang</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Tetsuya Narita</div><div class="item1"><div class="text"><h1>概要</h1><p>俯瞰画像から物体検出するためのデータセットを提案．従来のデータセットのものよりも小さい物体が多いデータセットである．各画像は4000×4000ピクセルであり，さまざまな大きさ，向き，形状を示す物体を含む．データセットは15カテゴリに分類されており，188282のインスタンスを含み，それぞれは任意の四角形でラベリングされている．人工衛星での物体検出の基礎構築のために，DOTA上の最先端の物体検出アルゴリズムを評価した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DOTA.png" alt="DOTA.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>俯瞰画像データセット内のインスタンスは小さいものの割合が高く，細かいものも検出可能人工衛星による物体検出に応用が利く可能性を示唆．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.10398">論文</a></li></ul></div></div><div class="slide_index">[#360]</div><div class="timestamp">2018.5.21 18:34:11</div></div></section><section id="ID_Illuminant_Spectra-based_Source_Separation_Using_Flash_Photography"><div class="paper-abstract"><div class="title">Illuminant Spectra-based Source Separation Using Flash Photography</div><div class="info"><div class="authors">Zhuo Hui, Kalyan Sunkavalli, Sunil Hadap, and Aswin C. Sankaranarayanan</div><div class="conference">CVPR2018</div><div class="paper_id">752</div></div><div class="slide_editor">Kouyou OTSU</div><div class="item1"><div class="text"><h1>概要</h1><p>フラッシュを当てた状態の写真とそうでない写真の2種類を利用して，画像を光源の違いに基づく構成画像へと自動的に分離するアルゴリズムの提案．2つの写真の色情報の違いに基づき，光源に対応するスペクトルや陰影との関係を見出す．従来手法と比較して，光の色合いや陰影を忠実に反映した低ノイズでの分離が可能であることを示した(従来手法(Hsu et.al.)でのSNR:10.13dB 提案手法でのSNR 20.43dB)．また，提案手法が画像のライティングの編集，カラー測光ステレオに有用であることを示した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Illuminant_Spectra-based_Source_Separation_Using_Flash_Photography.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>光源分離にカメラのフラッシュを利用（手軽）</li><li>従来手法を上回る性能．</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1704.05564">論文</a></li><li><a href="https://www.youtube.com/watch?v=7iizopNYJT0">動画</a></li></ul></div></div><div class="slide_index">[#361]</div><div class="timestamp">2018.5.21 20:53:52</div></div></section><section id="ID_Multi-Label_Zero-Shot_Learning_with_Structured_Knowledge_Graphs"><div class="paper-abstract"><div class="title">Multi-Label Zero-Shot Learning with Structured Knowledge Graphs</div><div class="info"><div class="authors">Chung-Wei Lee, Wei Fang, Chih-Kuan Yeh, Yu-Chiang Frank Wang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Shusuke Shigenaka</div><div class="item1"><div class="text"><h1>概要</h1><p>この論文は,各々の入力インスタンスに対して,複数の見えないクラスラベルを予測できるmulti-label learning及びmulti-label zero-shot learning(ML-ZSL)の新しい深層学習の提案した研究．
提案手法は複数のラベル間で人間が関心を持つsemantic knowledgeをグラフの中に組み込むことにより,
情報伝播メカニズムを学習し見えているクラスと見えないクラスの間の相互依存関係をモデル化することに適用できる．
本手法はstate-of-the-artと比較して,同等または改善されたパフォーマンスとして達成をすることができる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/171106526.png" alt="171106526"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・見た目だけでなく,経験を通して学んだ知識を使って物体を認識・WordNetから観察された知識グラフをend-to-endの学習フレームワークに組み込み,意味空間に電番されるラベル表現と情報を学習
・NUS-81およびMS-COCOの結果をWSABIE,WARP,Fast0Tag,Logisticsと比べたところ精度について一番高い結果を残した．
・ML-ZSLについてもFast0Tagと比べて高い精度を残している．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.06526">論文</a></li></ul></div></div><div class="slide_index">[#362]</div><div class="timestamp">2018.5.22 14:28:22</div></div></section><section id="ID_Wasserstein_Introspective_Neural_Networks"><div class="paper-abstract"><div class="title">Nonlinear 3D Face Morphable Model</div><div class="info"><div class="authors">Luan Tran, Xiaoming Liu</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>generatorとdiscriminatorを一つのモデルで表現するIntrospective Neural Network(INN)に対してwasserstein distanceを導入することで、INNと同等の生成能力・識別能力を保ちつつclassifierにおけるCNNの数を20分の1にしたWasserstein INN(WINN)を提案。
生成された画像の比較はDCGAN、INN for generative(INNg)、INNgのclassifierにおけるCNNを一つにしたINNg-singleと行った。
またadversarial exampleに対して頑健な識別精度を達成した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Wasserstein_Introspective_Neural_Networks.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>INNにwasserstein distanceを導入することで、生成・識別においてINNと同等以上の性能を持ちながら識別器におけるCNNの数が20分の1であるIWNNを提案。</li><li>テクスチャの生成やCelebA・SVHNを学習することで生成された画像はDCGANと比べてはっきりとしており質が高い。</li><li>CIFAR-10の学習によって生成された画像におけるInception scoreはDCGANの方が良い結果となった。</li><li>CNN、ReosNet、ICNと比較して、adversarial exampleに対する誤識別率が低く、 adversarial examples に惑わされずに識別を行うことが可能。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.08875">論文</a></li><li><a href="https://github.com/kjunelee/WINN">GitHub</a></li><li><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Lazarow_Introspective_Neural_Networks_ICCV_2017_paper.pdf">Introspective Neural Networks for Generative Modeling</a></li></ul></div></div><div class="slide_index">[#363]</div></div></section><section id="ID_Nonlinear_3D_Face_Morphable_Model"><div class="paper-abstract"><div class="title">Nonlinear 3D Face Morphable Model</div><div class="info"><div class="authors">Luan Tran, Xiaoming Liu</div><div class="conference">CVPR 2018 Spotlight</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>3Dスキャンデータを使用せずにin-the-wildな顔画像のみを用いてencoder-decoderによって3D Morphable Model(3DMM)を生成する手法を提案。生成された3DMMを nolinear 3DMMと呼んでいる。
従来のlinear 3DMMは学習のために3Dスキャンデータが必要であり、かつPCAによって次元削減を行うため表現力に乏しいという問題点があった。
提案手法ではencoderによってプロジェクション、シェイプ、テクスチャのパラメタを取得し、decoderによってシェイプ、テクスチャを推定する。
また初期の学習では既存手法によって得られる3DMMのプロジェクションパラメタ、
シェイプパラメタとUV空間から得られるテクスチャを擬似的なGTとすることで弱教師学習を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Nonlinear_3D_Face_Morphable_Model.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>3Dスキャンデータを使用せずに、in-the-wildな顔画像のみを学習させることで、入力画像から3D Morphalbe Modelを生成する。</li><li>linear 3DMMと比較して、3次元形状、テクスチャの精度が高い。また見た目もGTにより近い。</li><li>顔のアラインメントにおいてstate-of-the-artよりも高い精度を達成。</li><li>3次元形状における精度はstate-of-the-artと同等であった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><li>弱教師学習がどれほど影響を持つかが気になった。</li><ul><li><a href="https://arxiv.org/abs/1804.03786">論文</a></li><li><a href="http://cvlab.cse.msu.edu/project-nonlinear-3dmm.html">Project page</a></li></ul></div></div><div class="slide_index">[#364]</div></div></section><section id="ID_UV-GAN_Adversarial_Facial_UV_Map_Completion_for_Pose-invariant_Face_Recognition"><div class="paper-abstract"><div class="title">UV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face Recognition</div><div class="info"><div class="authors">Jiankang Deng, Shiyang Cheng, Niannan Xue, Yuxiang Zhou, Stefanos Zafeiriou</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>in-the-wildな入力顔画像から得られるUVマップの補完をU-Netで行う手法を提案。入力画像に対して3D Morphalbe Modelを適用し不完全なUVマップを取得し、U-Netで補完を行うように学習を行う。
discriminatorにはUVマップ全体と顔領域の判定をさせる。
またUVマップの個人性が失われないように、アイデンティティーに関するロスを取る。
1892人のUVマップをもつWildUVデータセットの構築も行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/UV-GAN_Adversarial_Facial_UV_Map_Completion_for_Pose-invariant_Face_Recognition.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>in-the-wildな顔画像に対してもリアルかつ精度の高いUVマップの補完を達成。入力されるUVマップが50%欠けていても補完可能。</li><li>入力画像からUVマップと3D shapeを取得するため、入力画像を任意の顔向きに編集可能。</li><li>横向き顔画像から生成されたUVマップはPSNR, SSIMにおいて既存手法を上回る精度を達成。</li><li>frontal-profile face verificationにおいてstate-of-the-artを上回る94.05%を達成。</li><li>1892のアイデンティティーのUVマップをもつ大規模UVマップデータセットであるWildUVデータセットを公開（予定）。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.04695">論文</a></li></ul></div></div><div class="slide_index">[#365]</div></div></section><section id="ID_LIME_Live_Intrinsic_Material_Estimation"><div class="paper-abstract"><div class="title">LIME: Live Intrinsic Material Estimation</div><div class="info"><div class="authors">A. Meka, M. Maximov, M. Zollhöfer, A. Chatterjee, H.P. Seidel, C. Richardt and Ch. Theobalt</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>単RGB画像で，リアルタイムに材質反射特性を推定する手法を提案し，デモシステムを作った．</p><p>構造は，主に複数のU-Netからなり，それぞれ前景セグメンテーション，スペキュラー推定，鏡面反射推定を行う．ロス関数も定義．</p><p>さらに，形状情報も使えるのなら，低・高周波光源情報の推定も可能．連続撮影時の光源情報の連続性を考慮した時系列統合の枠組みも提案．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/LIME_Live_Intrinsic_Material_Estimation.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>実用的なシチュエーション（リアルタイム，複雑な光源下，連続撮影）で利用可能であることを示している．</li><li>定性，定量評価を行い，性能の良さを示している．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>デモビデオを作り慣れているように見えるあたり，CG勢と思われる．デモも結構評価されているだろうか．
アプリケーション枠で評価されるように書いているかもしれない．</p><ul><li><a href="https://arxiv.org/abs/1803.05619">arXiv</a></li><li><a href="https://www.youtube.com/watch?v=5ntLiAYsMm4">Youtube</a></li><li><a href="http://gvv.mpi-inf.mpg.de/projects/LIME/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#366]</div><div class="timestamp">2018.5.21 21:08:44</div></div></section><section id="ID_Fast_End-to-End_Trainable_Guided_Filter"><div class="paper-abstract"><div class="title">Fast End-to-End Trainable Guided Filter</div><div class="info"><div class="authors">H. Wu, S. Zheng, J. Zhang, K. Huang</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>低解像度＋高解像ガイダンスマップを与えると，高解像度画像を効率的（省計算時間，省メモリ）に出力できるGuided Filtering Layerなるものを提案．</p><p><a href="http://kaiminghe.com/eccv10/">GuidedFilter</a>は，
空間的に変化する線形変換行列のグループとして表現でき，
CNNに統合可能．つまり，end-to-endで最適化可能な
深層ガイデッドフィルタネットワークを構成できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Fast_End-to-End_Trainable_Guided_Filter_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Fast_Image_Processing_ICCV_2017_paper.pdf">Context Aggregation Network</a>にGuided Filtering Layerを載せたものを、5つの先進的な画像処理タスクで試したところ，<strong>10～100倍高速</strong>であり，<strong>SoTA性能</strong>も出た．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>かなり省コストになっている．DNN導入可能にするように（エレガントに）定式化し，コストダウンしつつ深層学習できるようにする手法がいくつか見られている．</p><ul><li><a href="https://arxiv.org/abs/1803.05619">arXiv</a></li><li><a href="https://github.com/wuhuikai/DeepGuidedFilter">GitHub</a></li></ul></div></div><div class="slide_index">[#367]</div><div class="timestamp">2018.5.21 20:01:20</div></div></section><section id="ID_Guide_Me_Interacting_with_Deep_Networks"><div class="paper-abstract"><div class="title">Guide Me: Interacting with Deep Networks</div><div class="info"><div class="authors">Christian Rupprecht, Iro Laina, Nassir Navab, Gregory D. Hager and Federico Tombari</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>CNNにより学習したタスクの出力結果に対して、人間がヒント(例：画像中に空は見えない)を与えていくことで精度向上を図る研究。CNNモデルをheadとtailの2つのパートに分割し、headから得られた特徴マップをヒントによって修正していくことで精度の向上を実現する。
その際、ネットワークの重みを更新するのではなく修正に用いるパラメータを言語情報から推測することで行う。
ネットワークの予測結果とground truthの差分を取り、正しく予測できていない物体の種類や位置を推定することで学習に用いる文章は自動で生成する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Guide_Me_Interacting_with_Deep_Networks.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>セマンティックセグメンテーションにより実験を実施したところ、クラス間違い、物体の一部が欠けている、物体の一部のみが見えるといったケースにおいて精度が向上することを確認した。ヒントを繰り返し与えていくことはノイズとなってしまうためあまり精度が向上しなかった。
従来のディープラーニングは一度学習をしてしまうと得られる出力が固定されてしまうのに対して、人間が介入することで結果を変えるという新しい応用方法を提案している。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.11544">論文</a></li></ul></div></div><div class="slide_index">[#368]</div><div class="timestamp">2018.5.21 16:15:43</div></div></section><section id="ID_Face_Detector_Adaptation_without_Negative_Transfer_or_Catastrophic_Forgetting"><div class="paper-abstract"><div class="title">Face Detector Adaptation without Negative Transfer or Catastrophic Forgetting</div><div class="info"><div class="authors">Muhammad Abdullah Jamal, Haoxiang Li, Boqing Gong</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔検出におけるターゲットドメインからソースドメインへのadaptationを、negative transferとcatastrophic forgettingの両方を引き起こさずに行う手法を提案。
negative transferとはadaptation後のソースドメインにおける検出精度がadaptation前のソースドメインにおける検出精度に劣ることを指し、
catastorophic forgettingとはadaption後におけるソースドメインの検出精度が著しく下がることを指す。
提案手法では、ソースドメインとターゲットドメインの違いを、ロス関数とDNNの重みの差分で表現し、
この差分がなくなるように学習を行う手法を提案。
またターゲットドメインにface or notのラベルがないという状況も考えて教師あり学習だけでなく教師なし学習、
半教師あり学習の結果についても議論を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Face_Detector_Adaptation_without_Negative_Transfer_or_Catastrophic_Forgetting.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ソースドメインとターゲットドメインの違いを、DNNのロス関数・重みの差分で表現することでadaptationを行った。</li><li>実験は、CascadeCNN+AFLW(25000 faces), Faster-R CNN+WIDER FACE dataset(393,703 faces, highly labeled)の2つのモデルでソースドメインの学習を行い、ターゲットドメインははFDDB(5171 labeled faces)、COFWで行った。</li><li>検出結果はターゲットドメインのみを学習した検出器、ソースドメインからターゲットドメインへfine tuningされた検出器、domain adaptaionを行うstate-of-the-artと比較を行った。提案手法はターゲットドメインにおける検出においてもっとも高い精度を達成。
またソースドメインにおける検出においてもターゲットドメインのみを学習した識別器と同等の精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>adaptationというより、もはやトレーニングデータセットの事後拡張となっており、後でトレーニングデータを追加したくなった時に有用なのではないだろうか。</li><li><a href="http://boqinggong.info/papers/cvpr18-deep-face.pdf">論文</a></li><li><a href="http://boqinggong.info/papers/cvpr18-deep-face-supp.pdf">Supplementary</a></li></ul></div></div><div class="slide_index">[#369]</div></div></section><section id="ID_Extreme_3D_Face_Reconstruction_Looking_Past_Occlusions"><div class="paper-abstract"><div class="title">Extreme 3D Face Reconstruction: Looking Past Occlusions</div><div class="info"><div class="authors">Anh Tuấn Trần, Tal Hassner, Iacopo Masi, Eran Paz, Yuval Nirkin, Gérard Medioni</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要s</h1><p>入力顔画像からバンプマップや視点を推定することで、入力画像からは見えていない側面や、強いオクルージョンがある顔画像からも精度の高い三次元形状を取得する手法を提案。
入力画像から帯域的な情報として三次元の大まかな形と、
局所的な情報としてしわなどのディティールを表現するバンプマップを別々のDNNモデルを使って取得する。
続いてオクルージョンがある場合には、バンプマップが不自然な起伏を持つため深層学習による修正を行う。
最後に顔の対称性を利用して、入力画像からは見えていない側面などをルールベースで復元する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Extreme_3D_Face_Reconstruction_Looking_Past_Occlusions.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>入力画像から3Dモデル全体を一気に復元するのではなく、帯域的な特徴と局所的な情報を分けて取り扱うことで精度の高い三次元復元を可能にした。</li><li>結果の評価は復元された三次元形状による個人認証の精度で行っている。画像にオクルージョンがない場合にはstate-of-the-artよりも高い精度を達成。オクルージョンがある場合でも、オクルージョンがない場合よりと比べて2%ほどしか劣らなかった。(state-of-the-artはそもそもオクルージョンを考慮できない。)</li><li>復元された三次元形状は、既存手法がオクルージョンを考慮することができなかったりシワなどの復元ができていないのに対して、提案手法ではオクルージョンがある場合でもシワなどの詳細な情報を復元できている。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>帯域的な顔形状の復元やバンプマップの修正などを既存手法に頼っているものの、復元された三次元形状は既存手法に比べて圧倒的なクオリティを持つ。
しかし形状自体のGTとの比較がなかったのが残念。</li><li><a href="https://arxiv.org/abs/1712.05083">論文</a></li><li> <a href="https://github.com/anhttran/extreme_3d_faces">GitHub</a></li></ul></div></div><div class="slide_index">[#370]</div></div></section><section id="ID_InverseFaceNet_Deep_Monocular_Inverse_Face_Rendering"><div class="paper-abstract"><div class="title">InverseFaceNet: Deep Monocular Inverse Face Rendering</div><div class="info"><div class="authors">Hyeongwoo Kim, Michael Zollhöfer, Ayush Tewari, Justus Thies, Christian Richardt, Christian Theobalt</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>実世界の3D顔モデルを使用せず合成された3DモデルのみでCNNをトレーニングすることで、実世界の顔画像から顔向き、形、表情、リフレクタンス、イルミネーションの3D復元を行う手法を提案。
CNNをトレーニング際の問題点として、実世界の3D顔モデルに対するアノテーションが足りないという問題があった。
これに対して、実世界の顔画像から推定されるパラメタと合成顔から推定されるパラメタに対してself-supervised bootstrappingを行うことで、
トレーニングに使用する合成顔3Dモデルのパラメタの分布を実世界のパラメタの分布に近づくようにトレーニングデータを逐次的に更新を行うことで、
CNNの学習を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/InverseFaceNet_Deep_Monocular_Inverse_Face_Rendering.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>self-supervised bootstrappingを使用することで、実世界のパラメータを再現するように合成顔のデータセットを再構築することで、データセットがないという問題に取り組んだ。</li><li>既存の学習ベースの手法に比べて、ジオメトリーにおいて最も高い精度を達成。</li><li>最適化ベースの手法に比べると、パーツのディティールやシワの再現の精度が悪い。</li><li>リミテーションとして、データセットにない顔向きや髪によるオクルージョンを考量することができない。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>異なるドメインを使ったトレーニングの方法として、GANを使ってcross domainの分布を近づける方法が提案されているなど、トレーニングデータ不足を解決する方法が提案されてきている。</li><li><a href="https://arxiv.org/abs/1703.10956">論文</a></li><li><a href="https://web.stanford.edu/~zollhoef/papers/arXiv17_Inverse/supple.pdf">Supplementary</a></li></ul></div></div><div class="slide_index">[#371]</div></div></section><section id="ID_Towards_Pose_Invariant_Face_Recognition_in_the_Wild"><div class="paper-abstract"><div class="title">Towards Pose Invariant Face Recognition in the Wild</div><div class="info"><div class="authors">Jian Zhao, Yu Cheng, Yan Xu, Lin Xiong, Jianshu Li, Fang Zhao</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>様々な照明環境、表情をした横向き顔画像を入力として、正面顔画像を生成することで高い個人認証率を達成するGANベースのPose Invariant Model(PIM)というネットワークを提案。
学習で使用できるトレーニングデータが少ないため、効率的かつ過学習を防ぐために以下のようにPIMを構築。<li>顔全体を生成するgeneratorと両目・鼻・口の4つのパーツを生成するgeneratorを用意。</li><li>4つのパーツが検出された画像と取得できない画像(横顔画像など)を異なるドメインの画像とみなして、cross-domain adversarial trainingを行うことで、両目・鼻・口を復元。</li><li>上記のGANを２セット用意し、discriminator同士でlearning to learnを行うことで効率的な学習を行った。</li></p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Towards_Pose_Invariant_Face_Recognition_in_the_Wild.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>2つのGANをもつTP-GANやDR-GANは最適化が困難で合ったが、これに対してlearning-to-learnを導入することでこの問題を解決。</li><li>MultiPIE、CFPデータセットにおいて様々な角度の顔画像に対する個人識別においてほぼ全てのケースにおいてstate-of-the-artよりも優れた精度を達成。(唯一Multi-PIEで顔向きが±30°の場合にTP-GANに劣った。)</li><li>横向き顔画像から生成される正面顔画像において、既存手法ではテクスチャが崩れていたり完全に正面を向いていない場合があったが、提案手法では見た目が良い正面顔画像を生成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>データセットが少ないという根本的な問題に対して、cross-domain adversarial training、learing to learnを行うことで解決しているが、これがデータベースが欠乏している他の問題設定でも解決できるのかを試してみたい。</li><li><a href="https://arxiv.org/abs/1803.00130">論文</a></li></ul></div></div><div class="slide_index">[#372]</div></div></section><section id="ID_Ring_loss_Convex_Feature_Normalization_for_Face_Recognition"><div class="paper-abstract"><div class="title">Ring loss: Convex Feature Normalization for Face Recognition</div><div class="info"><div class="authors">Yutong Zheng, Dipan K. Pal and Marios Savvides</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>DNNによって得られた特徴量を超球面上に配置するように正規化を行うロス関数であるRing lossを提案。特に教師あり識別問題においてはDNNによる特徴量を正規化することでより精度の高いモデルを構築することができる、
というアイディアもとにRing lossを提案。
SoftMaxといった基本的なロス関数と組み合わせることでより高い精度を達成。
実験には様々な識別タスクを行うことができる顔データセットを用いることで、精度の向上を確認した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Ring_loss_Convex_Feature_Normalization_for_Face_Recognition.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>SoftMaxとSphereFaceにRing lossを組み合わせることでLFW, IJB-A Janus, Janus CS3, CFP, MegaFaceデータセットにおけるface verification, identificationにおいて他のロス関数と同等あるいはそれ以上の精度を達成。</li><li>極端に低解像度の画像におけるface matchingにおいてベースラインの手法を凌駕した。</li><li>実験ではResNet64を使用。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.00130">論文</a></li></ul></div></div><div class="slide_index">[#373]</div></div></section><section id="ID_Label_Denoising_Adversarial_Network_LDAN_for_Inverse_Lighting_of_Face_Images"><div class="paper-abstract"><div class="title">Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Face Images</div><div class="info"><div class="authors">Hao Zhou, Jin Sun, Yaser Yacoob, David W. Jacobs</div><div class="conference">CVPR 2018 Spotlight</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>3Dモデルから実画像へのドメイン変換をGANによって行うことで、単一顔画像から照明パラメタを推定するLabel Denoising Adversarial Network(LDAN)を提案。
人の顔画像に対して照明パラメタ(論文で使用されているのは37次元の球面調和関数)がアノテーションされたデータセットがないため、
3Dモデルを使用してFeature Netと呼ばれるネットワークで中間特徴量を取得し、
中間特徴量からLightning Netを用いて照明パラメタの推定を学習。
続いて人の顔画像に対して、既存手法を用いてノイズが乗った照明パラメタを取得し、
人の顔画像に対してもFeature Netを新しく学習し、
3D モデルから得られた中間特徴量と共にGANに入力することでドメインの変換を行うことでノイズが除去された照明パラメタを取得。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Label_Denoising_Adversarial_Network_LDAN_for_Inverse_Lighting_of_Face_Images.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>単一画像からの照明パラメタの推定という問題に対して、初めて学習ベースの手法を提案。</li><li>結果の比較は19の照明環境が用意されているMultiPieデータセットで行い、推定されたパラメータに対する識別を行うことで精度を評価。state-of-the-artに比べて識別精度およびユークリッド距離・Q値におけるAUCで最も高い精度を達成。</li><li>同問題を扱う既存手法が最適化ベースということもあり、既存手法と比べて10万倍のスピードで実行可能。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>GANを使って異なるドメインの特徴量を同じ空間にマップする考え方は既に<a href="https://arxiv.org/abs/1702.05464">Adversarial Discriminative Domain Adaptation</a>によって提案されているが、異なる点としては[Eric et al.]はGANのロスしか使っていないが、この方法では写像がうまく行かず、
A→A', B→Bと学習して欲しいところをやA→B', B→A'といった写像を学習してしまう。
これを解消するために、lightning netで得られたパラメータに対するL2ロスを取ることでこれを解消。</li><li><a href="https://arxiv.org/abs/1802.06713">論文</a></li></ul></div></div><div class="slide_index">[#374]</div></div></section><section id="ID_Disentangling_3D_Pose_in_A_Dendritic_CNN_for_Unconstrained_2D_Face_Alignment"><div class="paper-abstract"><div class="title">Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face Alignment</div><div class="info"><div class="authors">Amit Kumar, Rama Chellappa</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔向きをコンディションとして与え木構造で表された顔のランドマークを学習させることで、顔のランドマーク推定を行うPose Conditioned Dendritic CNN(PCD-CNN)を提案。
顔のコンディションはPoseNetにより出力された値を使用する。
顔のランドマークを木構造として与えることで、ランドマークの位置関係を利用してCNNを学習させた。
また提案ネットワークはPCD-CNNと通常のCNNの二段階になっており、
後段のCNNをファインチューニングすることでランドマークのポイント数が違うデータセットや顔向き推定などの他のタスクにも適用可能。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Disentangling_3D_Pose_in_A_Dendritic_CNN_for_Unconstrained_2D_Face_Alignment.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ネットワークをPCD-CNNとCNNの二段階で構成することで、異なるランドマークのポイント数や顔向き推定といった他のタスクにも適用可能。</li><li>顔向きをコンディションとして与えることで推定精度が向上。また、20FPSで実行が可能。　</li><li>AFLW, AFWデータセットにおいてランドマークの推定精度がstate-of-the-artよりも高い推定精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.06713">論文</a></li></ul></div></div><div class="slide_index">[#375]</div></div></section><section id="ID_Multi-Image_Semantic_Matching_by_Mining_Consistent_Features"><div class="paper-abstract"><div class="title">Multi-Image Semantic Matching by Mining Consistent Features</div><div class="info"><div class="authors">Qianqian Wang, Xiaowei Zhou and Kostas Daniilidis</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Kazuho Kito</div><div class="item1"><div class="text"><h1>概要</h1><p>ノイズを考慮しつつ、数千もの画像セット全てにおいて一致する(信頼できる)特徴を見出すことで、画像間の対応を図るマッチング手法。マッチングはセマンティック性を考慮することができる（目と目、耳先と耳先など）これにより、一貫性がある画像セット内で信頼できる特徴の関係を確立。何千もの画像を処理する場合にスケーラブルな手法。つまりは数に頑健。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Multi-Image_Semantic_Matching_by_Mining_Consistent_Features.PNG" alt="Multi-Image_Semantic_Matching_by_Mining_Consistent_Features.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来手法では、全てのペアで対応する関係を最適化していたが、本手法では、特徴の選択とラベリングに着目し、信頼度の高い特徴のみを用いた疎なセットのみで識別、マッチングする。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>図は中の左が出力結果であり、目は青、耳は黄色、鼻は赤など各特長の意味を理解し、マッチングを成功させている。</p><ul><li><a href="https://arxiv.org/pdf/1711.07641.pdf">論文</a></li></ul></div></div><div class="slide_index">[#376]</div><div class="timestamp">2018.5.21 11:27:27</div></div></section><section id="ID_Learning_Intrinsic_Image_Decomposition_from_Watching_the_World"><div class="paper-abstract"><div class="title">Learning Intrinsic Image Decomposition from Watching the World</div><div class="info"><div class="authors">A. Uthors, B. Uthors and C. Uthors</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Intrinsic Image Decompositionのために，時間経過とともに照明が変化するビデオを使ったCNNの学習方法を提案．正解の Intrinsic Imageが不要な点が強みである．学習が完了したモデルは単一画像に対して適用できるよう汎化しており，いくつかのベンチマークに対して良い結果となった．<br>Contribution：<br>・データセット（BigTime）の公開．室内，室外両方での照明変化のあるビデオと画像シーケンスのデータセット．<br>・このGround Truthを含まないデータを使った手法の提案．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_Intrinsic_Image_Decomposition_from_Watching_the_World_fig.png" alt="Image"><br>学習時：ラベル無しで，視点が固定され照明が変化するビデオを学習に利用する．<br>テスト時：単一画像からintrinsic image decompositionを行う．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>最適化ベースのIntrinsic Decomposition手法と，機械学習手法の間に位置する手法と言える．<br>・U-netに似た構造のCNN．<br>・Lossの工夫：画像ペア全てを考慮するall-pairs weighted least squares lossとシーケンス全体のピクセル全てを考慮するdense, spatio-temporal smoothness loss．最適化ベースのlossをフィードフォワードネットワークのlossとして利用する．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Intrinsic image decompositionとは，入力された1枚の画像をreflectance画像とshading画像の積に分解する問題のこと．<br>intrinsic imagesのGround Truthを大規模に揃えることは困難．</p><ul><li><a href="https://arxiv.org/abs/1804.00582">arXiv</a></li></ul></div></div><div class="slide_index">[#377]</div><div class="timestamp">2018.5.21 11:26:41</div></div></section><section id="ID_Photographic_Text-to-Image_Synthesis_with_a_Hierarchically-nested_Adversarial_Network"><div class="paper-abstract"><div class="title">Photographic Text-to-Image Synthesis with a Hierarchically-nested Adversarial Network</div><div class="info"><div class="authors">Zizhao Zhang, Yuanpu Xie, Lin Yang</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>階層的入れ子構造の識別器を使用し，テキストから高解像画像を生成するGANを提案．end-to-endの学習で高解像画像の統計量を直接モデルリングすることが可能な手法．これは，step-by-stepで高解像画像を生成するStackGANとは異なる点である．複数のスケールの中間層に対して階層的入れ子構造の識別器を使用することで中間サイズレベルでの表現に制約を加え，生成器が真の学習データの分布を獲得しやすくする．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Photographic_Text-to-Image_Synthesis_with_a_Hierarchically-nested_Adversarial_Network_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>新しい構造と，lossの工夫でtext-to-imageのタスクで高解像画像の生成を可能とした．<br>・hierarchical-nested Discriminatorを使用．<br>・lossには，pair lossとlocal adversarial lossを使用する．pair lossでは入力テキストと生成画像が一致しているかを評価．local adversarial lossでは生成画像の細部の質を評価する．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.09178">arXiv</a></li></ul></div></div><div class="slide_index">[#378]</div><div class="timestamp">2018.5.21 11:22:05</div></div></section><section id="ID_Connecting_Pixels_to_Privacy_and_Utility_Automatic_Redaction_of_Private_Information_in_Images"><div class="paper-abstract"><div class="title">Connecting Pixels to Privacy and Utility: Automatic Redaction of Private Information in Images</div><div class="info"><div class="authors">Tribhuvanesh Orekondy, Mario Fritz, Bernt Schiele</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>プライバシー保護のために画像に含まれる個人的な情報を自動的に改変する手法の提案．プライバシーを守りつつ画像の有用性を保つためのトレードオフが問題となる．有用性を保つためには改変する領域サイズが最小限である必要があり，これをセグメンテーションの問題として取り組む．</p><p>Contribution:</p><ul><li>データセットの公開．様々な種類のプライバシーのラベルが，ピクセルレベルとインスタンスレベルで与えられている自然画像の初のデータセット．</li><li>モデルの提案．多様な個人情報を自動的に改変するモデルを提案する．正解のアノテーションに対して83％の正解率を達成した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Connecting_Pixels_to_Privacy_and_Utility_Automatic_Redaction_of_Private_Information_in_Images_fig.png" alt="Image"><br>指紋，日時，人，顔，ナンバープレートを黒く塗りつぶせている．<br>他にも，住所やメールアドレスのようなテキスト情報や顔や車椅子などの視覚情報，あるいはテキストと視覚情報を合わせたものなど，多様な個人情報に対応するデータセットとモデルを提案．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>どのような対象(Textual, Visual, Multimodal)を扱うかで使用するモデルは異なる．<br>Textualな対象では，Sequence Labelingを使用する．<br>VisualとMultimodalな対象では，Fully convolutional instance-aware semantic segmentationを使用する．<br>Nearest Neighborなどのベースライン手法と比較を行なっている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>画像全体を黒く塗ればプライバシーは保護されるが，画像の価値がなくなるので，トレードオフが存在する．<br>データセットを作った貢献がメイン．プライバシー保護のためのアノテーションを行ったことで，それなりの正解率で個人情報の改変を行えるようになった．</p><ul><li><a href="https://arxiv.org/abs/1712.01066">arXiv</a></li></ul></div></div><div class="slide_index">[#379]</div><div class="timestamp">2018.5.21 11:17:12</div></div></section><section id="ID_Disentangling_Structure_and_Aesthetics_for_Style-aware_Image_Completion"><div class="paper-abstract"><div class="title">Disentangling Structure and Aesthetics for Style-aware Image Completion</div><div class="info"><div class="authors">Andrew Gilbert, John Collomosse, Hailin Jin, and Brian Price</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ノンパラメトリックのInapinting手法を提案．<br>視覚的な構造とスタイルをdeep embeddingすることで，パッチの検索と選択の際に視覚的なスタイルを考慮することが可能で，さらに，パッチのコンテンツを補完画像のスタイルに合わせるためのneural stylizationが可能となる．この手法は，patch-basedの手法とgenerativeベースの手法の架け橋的な補完手法である．<br>技術的貢献：<br>・style-aware optimization<br>・adaptive stylization</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Disentangling_Structure_and_Aesthetics_for_Style-aware_Image_Completion_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>以下の手順で画像補完を行う．<br>１．スタイルを考慮して穴に埋める候補を検索する<br>２．補完画像と構造とスタイルが合うパッチをMRFで複数集め，選択する<br>３．選択されたパッチを補完画像のスタイルに変換する</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://personal.ee.surrey.ac.uk/Personal/J.Collomosse/pubs/Gilbert-CVPR-2018.pdf">論文pdf</a></li></ul></div></div><div class="slide_index">[#380]</div><div class="timestamp">2018.5.21 11:09:50</div></div></section><section id="ID_DeblurGAN_Blind_Motion_Deblurring_Using_Conditional_Adversarial_Networks"><div class="paper-abstract"><div class="title">DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks</div><div class="info"><div class="authors">Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, Jiˇri Matas</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>motion deblurringのためのGAN(DeblurGAN)を提案．structural similarity measureとアピアランスでSoTA．ブラーを除去した画像で物体検出の精度を出すことで，ブラー除去モデルの質を評価するという方法を提案．提案手法は，質だけでなく実行速度も優れており，従来手法の５倍の速さがある．モーションブラーのかかった画像を合成するための方法を紹介し，そのデータセットもコード，モデルとともに公開．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DeblurGAN_Blind_Motion_Deblurring_Using_Conditional_Adversarial_Networks_fig.png" alt="Image"><br>ブレを除去してからYOLOで検出すると精度が良くなることを示している．これをDeblurモデルの指標にすることができると主張．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>loss：WGANによるAdversarial lossとPerceptual loss</li><li>構造：畳み込み，instance normalization層，ReLU関数から成るResBlockの繰り返しがメインで，出力するときに入力画像を加算するglobal skip connectionを持つ．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>最近のGAN手法やテクニックを詰め込んで，新しく作ったデータセットを利用したらSoTAがでたという感じ．テクニカルな貢献はあまりなさそう．</p><ul><li><a href="https://github.com/KupynOrest/DeblurGAN">GitHub</a></li><li><a href="https://arxiv.org/abs/1711.07064">arXiv</a></li></ul></div></div><div class="slide_index">[#381]</div><div class="timestamp">2018.5.21 11:05:29</div></div></section><section id="ID_Learning_to_Understand_Image_Blur"><div class="paper-abstract"><div class="title">Learning to Understand Image Blur</div><div class="info"><div class="authors">Shanghang Zhang, Xiaohui Shen, Zhe Lin, Radom ́ır Meˇch, Joa ̃o P. Costeira, Jose ́ M. F. Moura</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ボケ(blur)が望ましいのか否かと，そのボケが写真のクオリティーにどのような影響を与えているのかを，自動的に理解するアルゴリズムは少ない．この論文では，blur mapの推定とこのボケの望ましさの分類を同時に行うフレームワークを提案する．</p><p>貢献：</p><ul><li>ボケを検出することと，画像の質という点でボケを理解することを同時に行うのは，おそらく初めての研究．ABC-FuseNetというネットワークを提案．</li><li>１万枚のデータセット（SmartBlur）の公開．ピクセルごとにボケがかかっているか３段階でラベルづけ．さらに，画像ごとにボケの望ましさ(desirability)をラベルづけ．</li><li>SmartBlurと他の公開データセットで実験を行い．blur mapの推定とボケの望ましさの分類がSoTAを超えた．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Understand_Image_Blur_fig.png" alt="Item3Image"><br>ボケ具合をピクセルごとに３段階で示し，ボケの望ましさも出力する．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>ABC-FuseNetでは，低レベルのボケの推定と高レベルの画像内で重要コンテンツの理解の二つを行う．<br>A: attention map，FCNである．<br>B: blur map，Dilated Convolutionとpyramid pooling, Boundary Refinement用の層を使ってblurの推定を行う．<br>C: content feature map，ResNet-50を使ってコンテンツの特徴を抽出．<br>ボケの推定はBによって行い，ボケの望ましさの分類はA, B, Cから得られた特徴を用いて行う．ネットワーク全体をEnd-to-endで学習することができる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>ボケを軽減するための研究は多いが，ボケが全て邪魔とは言えない．ボケを効果的に利用することで，写真の印象が良くなることもある．いいボケなのか，悪いボケなのかの判断も必要だというモチベーションがある．</p><p>コード，データセットは以下に公開予定</p><ul><li><a href="https://github.com/Lotuslisa/Understand_Image_Blur">GitHub</a></li><li><a href="http://users.eecs.northwestern.edu/~xsh835/assets/cvpr2018_smartblur.pdf">論文</a></li></ul></div></div><div class="slide_index">[#382]</div><div class="timestamp">2018.5.21 10:50:21</div></div></section><section id="ID_Tags2Parts_Discovering_Semantic_Regions_from_Shape_Tags"><div class="paper-abstract"><div class="title">Tags2Parts: Discovering Semantic Regions from Shape Tags</div><div class="info"><div class="authors">Sanjeev Muralikrishnan, Vladimir G. Kim, Siddhartha Chaudhuri</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>指定された形状のタグに強く関係する領域を検出する手法の提案．明示的に領域ごとのラベリングはなく，さらにあらかじめセグメンテーションされていない状況で，形状のタグを与えた時に領域を発見するという問題設定．難しい点は，オブジェクトのタグという弱い教師情報からポイントごとのラベルを細かく出力する必要があること．このために分類とセグメンテーションを同時に行うネットワークを使う．形状ごとのタグからポイントごとの予測を得るためのネットワーク構造（WU-net）を提案したことがメインの貢献．</p><p>学習が完了すれば，タグが不明な形状に対しても手法を適用することができる．また，元々Weakly-supervised用に提案しているが，strongly-supervised用としても利用できる手法となった．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Tags2Parts_Discovering_Semantic_Regions_from_Shape_Tags_fig.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>U-net風のWU-netを提案．U-netから修正した点は，<br>・浅いU型の構造を3回くりかし，skip-connectionで密に繋がっている．深いU型1回の場合との結果の違いを図示している.<br>・セグメンテーションの用の隠れ層にタグ分類用の層を追加．(元々のは，strongly-supervised セグメンテーション用に設計されているので．)</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>３D形状としてはボクセル表現を使用．64×64×64 cubical gridを入力する．</p><ul><li><a href="https://arxiv.org/abs/1708.06673">arXiv</a></li></ul></div></div><div class="slide_index">[#383]</div><div class="timestamp">2018.5.21 10:40:57</div></div></section><section id="ID_Neural_3D_Mesh_Renderer"><div class="paper-abstract"><div class="title">Neural 3D Mesh Renderer</div><div class="info"><div class="authors">Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ニューラルネットワークに組み込むことができる3Dメッシュのレンダラーである Neural Renderer を提案。レンダリングの『逆伝播』と呼ばれる処理をニューラルネットワークに適した形に定義し直した．そしてこのレンダラーを<br>・一枚の画像からの3Dメッシュの再構成（ボクセルベースの再構成との比較あり）<br>・画像から3Dへのスタイル転移と3D版ディープドリーム<br>に応用できることを示した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Neural_3D_Mesh_Renderer_fig.png" alt="Image"><br>2D-to-3Dスタイルトランスファーの例</p></div></div><div class="item3"><div class="text"><h1>方法</h1><p>従来のままでレンダリングの操作が処理の途中にあると逆伝播が行えない状態であるので，レンダリングのための勾配を定義することでニューラルネットワークの中にレンダリング操作を加えても学習を行えるようにした．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://hiroharu-kato.com/projects/neural_renderer.html">プロジェクトサイト</a></li><li><a href="https://github.com/hiroharu-kato/neural_renderer">GitHub</a></li><li>３Dの形式には様々ある（ポイントクラウド，ボクセル，メッシュなど）が，3Dメッシュは効率的で表現能力が高く直感的な形式だそう．</li></ul></div></div><div class="slide_index">[#384]</div><div class="timestamp">2018.5.21 10:28:19</div></div></section><section id="ID_Demo2Vec_Reasoning_Object_Affordances_from_Online_Videos"><div class="paper-abstract"><div class="title">Demo2Vec: Reasoning Object Affordances from Online Videos</div><div class="info"><div class="authors">Kuan Fang, Te-Lin Wu, Daniel Yang, Silvio Savarese and Joseph J. Lim</div><div class="conference">CVPR2018</div><div class="paper_id">1387</div></div><div class="slide_editor">KazuhoKito</div><div class="item1"><div class="text"><h1>概要</h1><p>商品などのデモンストレーションの映像の特徴を通してその商品などのアフォーダンスを推論する研究．映像から埋め込みベクトルを抜き出すことで，ヒートマップと行動のラベルとして特定のもののアフォーダンスを予測するDemo2Vecモデルを提案．また，YouTubeの製品レビュー動画を集め，ラベリングすることでOnline Product Review detaset for Affordande(OPRA)を構築．</p></div></div><div class="item2"><div class="text"><p> </p><img src="slides/figs/Demo2Vec_Reasoning_Object_Affordances_from_Online_Videos.PNG" alt="Demo2Vec_Reasoning_Object_Affordances_from_Online_Videos.PNG"></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>アフォーダンスのヒートマップと行動のラベルの予測に関し，RNNの基準よりよいパフォーマンスを達成</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>YouTubeで公開されている動画では，Demo2Vecを用いてある物体のデモ動画からSawyer robotのEnd Effectorを予測したヒートマップの地点に移動するように制御させている様子を見ることができる．</p><ul><li><a href="http://ai.stanford.edu/~kuanfang/pdf/demo2vec2018cvpr">論文</a></li><li><a href="https://sites.google.com/view/demo2vec/">ProjectPage</a></li><li><a href="https://www.youtube.com/watch?v=UT1QohPIioU">YouTube</a></li></ul></div></div><div class="slide_index">[#385]</div><div class="timestamp">2018.5.20 22:42:02</div></div></section><section id="ID_Probabilistic_Plant_Modeling_via_Multi-View_Image-to-Image_Translation"><div class="paper-abstract"><div class="title">Probabilistic Plant Modeling via Multi-View Image-to-Image Translation</div><div class="info"><div class="authors">Takahiro Isokane, Fumio Okura, Ayaka Ide, Yasuyuki Matsushita, Yasushi Yagi</div><div class="conference">CVPR 2018</div><div class="paper_id">368</div></div><div class="slide_editor">Goshi Sasaki</div><div class="item1"><div class="text"><h1>概要</h1><p>葉に隠れていても３次元の枝構造を多視点画像から推測できるようにした。多視点からの植物画像を入力として枝構造の２次元確率マップをdropoutを取り入れたPix2Pixで推測して、それらから３次元の確率構造を作成した。最後にpartical floｗシュミレーションによって明確な３次元の枝構造を生成した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Probabilistic_Plant_Modeling.png" alt="architecture"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>葉や他の枝によって隠れてしまっていても枝構造を生成できるようにした。ベイジアンPix2Pixを利用することで植物の３次元構造をより正確に表せるようにした。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1804.09404.pdf">論文</a></li></ul></div></div><div class="slide_index">[#386]</div><div class="timestamp">2018.5.20 20:53:44</div></div></section><section id="ID_ROAD_Reality_Oriented_Adaptation_for_Semantic_Segmentation_of_Urban_Scenes"><div class="paper-abstract"><div class="title">ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes</div><div class="info"><div class="authors">Yuhua Chen, Wen Li, Luc Van Gool</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Hiroaki Aizawa</div><div class="item1"><div class="text"><h1>概要</h1><p>synthetic-to-realな変換を行う際に、1)モデルがsyntheticにoverfitするstyleの側面と、2)syntheticとrealの分布の違いの側面から発生する2つの問題があることに著者らは着目している。解決するために、前者はtarget guided distillation、後者はspatial-aware adaptationという手法を提案し、それを組み合わせた Reality Oriented ADaptation Network(ROAD-Net)を考案。GTAV/SYNTHIA - Cityscapesの適合タスクで評価し、sotaのsemantic segmentationモデルの汎化性能を向上したことを確認。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/ROAD_Reality_Oriented_Adaptation_for_Semantic_Segmentation_of_Urban_Scenes.PNG" alt="ROAD_Reality_Oriented_Adaptation_for_Semantic_Segmentation_of_Urban_Scenes.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Semantic SegmentationへのDomain Adaptationの適用が新しい。</li><li>結果もまたNonAdaptなPSPNetからmIoUが約11.6%向上している。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://cvpaperchallenge.github.io/CVPR2018_Survey/#/Learning_to_Adapt_Structured_Output_Space_for_Semantic_Segmentation">Learning to Adapt Structured Output Space for Semantic Segmentation</a>と目的と対象が似通っている。どちらもクラス分類で得られる特徴(ImageNetで学習されたpretrain model)がsegmentationでは有効ではないという主張であり、これをもとにそれぞれmulti-scaleな手法と、distillationによる手法と異なるアプローチをとっているのが興味深い。</li><li>spatial-aware adaptationはPatchGANと似通っており同様の性質を持つ？</li></ul><ul><li><a href="https://arxiv.org/abs/1711.11556">arxiv</a></li></ul></div></div><div class="slide_index">[#387]</div><div class="timestamp">2018.5.20 19:20:29</div></div></section><section id="ID_Gated_Fusion_Network_for_Single_Image_Dehazing"><div class="paper-abstract"><div class="title">Gated Fusion Network for Single Image Dehazing</div><div class="info"><div class="authors">Wenqi Ren Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun Cao, Wei Liu, Ming-Hsuan Yang</div><div class="conference">CVPR2018</div><div class="paper_id">404</div></div><div class="slide_editor">Kazuma Asano</div><div class="item1"><div class="text"><h1>概要</h1><p>霧がかかった画像(hazy input)から更に３つの入力，White balanced input，Contrast enhanced input，Gamma corrected inputを計算して導出し，これらの異なる入力間の外観差に基づきピクセル単位のConfidence Mapを計算する．これらを学習することで鮮明な画像を生成するMulti-scale Gated Fusion Network(GFN)を開発した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180404GFN_result.png" alt="Item3Image"><img src="slides/figs/180404GFN_network.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来手法と比較し，実装や再現が容易であり，また出力結果もPSNR，SSIMともに従来手法より高い評価となっている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.00213">arXiv</a></li><li><a href="https://sites.google.com/site/renwenqi888/research/dehazing/gfn">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#388]</div><div class="timestamp">2018.5.14 12:31:27</div></div></section><section id="ID_AdaDepth_Unsupervised_Content_Congruent_Adaptation_for_Depth_Estimation"><div class="paper-abstract"><div class="title">AdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimation</div><div class="info"><div class="authors">J.Nath, K.Phani, K.Uppala, A.Pahuja and R.V.Babu</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1803.01599</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>教師あり深層学習による手法は単眼カメラ画像における深さ推定に対して良い結果を出している．しかし．grand truthを得るためにはノイズに影響され，コストもかかる．合成データセットを用いた場合の深度推定では固有のドメインにしか対応していなく，自然なシーンに対して対応するのが難しいと言われる．この問題に対応するため，Adversalな学習と対応したターゲットの明確な一貫性をかすこと事によりAdaDepthを提案．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/AdaDepth_Unsupervised_Content_Congruent_Adaptation_for_Depth_Estimation.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>高次元の構造化エンコーダ表現に作用する，教師なしの敵対的適応設定AdaDepthを提案．</li><li>新規の特徴を再構成する正則化フレームワークを使用して適応表現にコンテンツ一貫性を課すことでモード崩壊の問題に取り組んだ．</li><li>最小限の教師データでの自然シーンの深度推定タスクにおいてSoTAを達成．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.01599">Paper</a></li></ul></div></div><div class="slide_index">[#389]</div><div class="timestamp">2018.5.20 15:52:52</div></div></section><section id="ID_End-to-end_learning_of_keypoint_detector_and_descriptor_for_pose_invariant_3D_matching"><div class="paper-abstract"><div class="title">End-to-end learning of keypoint detector and descriptor for pose invariant 3D matching</div><div class="info"><div class="authors">Georgios Georgakis, Srikrishna Karanam,Ziyan Wu,Jan Ernst,Jana Kosecka</div><div class="conference">CVPR 2018</div><div class="paper_id">227</div></div><div class="slide_editor">Goshi Sasaki</div><div class="item1"><div class="text"><h1>概要</h1><p>End-to-Endで3次元空間における特徴点の抽出とマッチングを行う手法を提案した。2つの距離画像を入力とし、VGG-16 を利用したFaster R-CNNを基本構造としている。
２つの距離画像からそれぞれVGG−16を利用して特徴マップを作成し、RPNにより領域候補を推定して、ROIプーリング層、全結合層を経て特徴量ベクトルを作り出す。最終的にcontrastive lossを利用して得られた特徴量間の対応関係を求めた。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/keypoint_detector.png" alt="architecture"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>初めてEnd-to-Endで3次元マッチングを行えるようにした。ノイズ環境下においてキーポイントマッチングで従来手法のHarris3D +FPFHなどよりも10％以上高い精度を出した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1802.07869.pdf">論文</a></li></ul></div></div><div class="slide_index">[#390]</div></div></section><section id="ID_AttnGAN_Fine-Grained_Text_to_Image_Generation_with_Attentional_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks</div><div class="info"><div class="authors">Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>アテンションドリブン，複数ステージでのRefineによって，テキストから詳細な画像を生成するGANを提案．CUBデータセットとCOCOデータセットでinception scoreがstate of the artを超えた．生成画像の特定の位置をワードレベルで条件付けしていることを示した．</p><p>貢献：<br>・Attentional Generative Adversarial NetworkとDeep Attentional Multimodal Similarity Model(DAMSM)の提案．<br>・実験でstate-of-the-art GAN modelsを超えたことを示す．<br>・ワードレベルで自動的に生成画像の一部をアテンションするのは初である．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/AttnGAN_Fine-Grained_Text_to_Image_Generation_with_Attentional_Generative_Adversarial_Networks_fig.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>・Attentional Generative Networkはセンテンスの特徴から始めて段階的に画像を高精細にしていくネットワークで，途中にアテンションレイヤーからのワード特徴を入力して条件付けする．<br>・各解像度に対してそれぞれDiscriminatorがある．<br>・最終的な解像度になったあと，Image Encoderにて局所的な画像特徴量とし，ワード特徴量とDAMSMにて比較することで，生成画像の細部がどれくらい単語に忠実であるか評価する．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・StackGANの著者も共著にいる．<br>・アテンションにより生成箇所を局所に向けることで，COCOのような複雑なシーンでも対応できるようになっている．</p><ul><li><a href="https://arxiv.org/abs/1711.10485">arXiv</a></li></ul></div></div><div class="slide_index">[#391]</div><div class="timestamp">2018.5.19 13:50:16</div></div></section><section id="ID_From_source_to_target_and_back_Symmetric_Bi-Directional_Adaptive_GAN"><div class="paper-abstract"><div class="title">From source to target and back: Symmetric Bi-Directional Adaptive GAN</div><div class="info"><div class="authors">Paolo Russo, Fabio M. Carlucci, Tatiana Tommasi and Barbara Caputo</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>SBADA-GANの提案．（Symmetric Bi-Directional ADAptive Generative Adversarial Network）<br>unsupervised cross domain classificationにフォーカス.<br>ラベルが与えられるSourceのサンプルを利用して，最終的にはTargetの分類問題を解く．SourceのサンプルをTargetのドメインに(Image-to-Imageの)マッピングをし，同時に逆方向も行う．分類器の学習に利用するのは，Sourceサンプル，TargetをSource風にしたもの，SourceをTarget風にしてさらにSource風に戻した３種類を使う．それぞれにラベルもしくは擬似ラベルを付与して学習する．テスト時はTargetサンプルのクラスを予測したいので，Target用の分類器と，TargetサンプルをSource風にしてから入力するSource用の分類器の２つを使用する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/From_source_to_target_and_back_Symmetric_Bi-Directional_Adaptive_GAN_fig.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>セルフラベリングの使用．Source用の分類器に制約を課す</li><li>class consistency lossの導入．Generatorとともに利用することで両方向のドメイン変換がお互いに影響し合うようになる．安定性と質向上の効果．最終的な目標である分類問題を解くことに有効．</li><li>例えばSource側のDiscriminatorは，RealサンプルとしてSource画像を使い，FakeサンプルとしてTarget画像をSource画像風にGeneratorでドメイン変換した画像を使う．</li><li>（問題設定的に）Source側の分類器にはクラスラベルによる学習ができる．</li><li>SourceとTargetの双方向のサンプル生成のための二つadversarial lossと，二つのclassification lossを同時に最小化する．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1705.08824">arXiv</a></li></ul></div></div><div class="slide_index">[#392]</div><div class="timestamp">2018.5.19 14:15:18</div></div></section><section id="ID_Deep_Photo_Enhancer_Unpaired_Learning_for_Image_Enhancement_from_Photographs_with_GANs"><div class="paper-abstract"><div class="title">Deep Photo Enhancer: Unpaired Learning for Image Enhancement from Photographs with GANs</div><div class="info"><div class="authors">Yu-Sheng Chen, Yu-Ching Wang, Man-Hsin Kao, Yung-Yu Chuang</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>学習ベースで画像のエンハンスメントを行う手法の提案．入力として「良い」写真のセットを使う．このセットに含まれる特色を持つように変換することが「エンハンスメント」に繋がると定義する．エンハンスメント問題をimage-to-imageの問題として扱い，提案手法は「良い」写真のセットの中で共通の特色を発見することを狙っている．普通の写真のドメインを「良い」写真のドメインに変換すれば良いとし，（CycleGANのような）２方向GANを以下の３つの工夫とともに利用する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Photo_Enhancer_Unpaired_Learning_for_Image_Enhancement_from_Photographs_with_GANs_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>Contribution</h1><ul><li>global featureを使ったU-netの利用．これがシーンの状況，照明条件，対象のタイプの情報を捉える．</li><li>WGANのためのadaptive weighting schemeを提案．収束を早める．</li><li>individual batch normalization layersの利用．Generatorは入力データの分布により適応するようになる．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>Flickerのレタッチされた写真を利用するなどしている．</li><li>Adobeがプロ写真家一人一人のレタッチ方法を再現するという機能を実装するのも近いかもしれない．</li><li>ハイダイナミックレンジの写真にしたらエンハンスされていると思っている節がある．</li><li><a href="https://www.csie.ntu.edu.tw/~cyy/publications/papers/Chen2018DPE.pdf">論文</a></li></ul></div></div><div class="slide_index">[#393]</div><div class="timestamp">2018.5.19 13:33:54</div></div></section><section id="ID_Imagine_it_for_me_Generative_Adversarial_Approach_for_Zero-Shot_Learning_from_Noisy_Texts"><div class="paper-abstract"><div class="title">Imagine it for me: Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts</div><div class="info"><div class="authors">Yizhe Zhu, Mohamed Elhoseiny, Bingchen Liu, and Ahmed Elgammal</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Wikipediaのようにノイズの多いテキストからzero-shot learningを行うためのGAN用いる方法を提案．GANを使ってテキストが表現するオブジェクトのビジュアル的な特徴を生成する．オブジェクトのクラスごとに特徴を近い位置にembeddingできれば良い．これができれば後は教師あり手法で分類を行えることになる.<br>コントリビューション：</p><ul><li>zero-shot learningにおいてUnseenであるクラスのテキスト記述からvisual featureを生成することで，zero-shot learningを従来の分類問題にしてしまう．generative adversarial approach for ZSL (GAZSL) ．</li><li>ノイズを抑制するためのFC層と埋め込み後のクラス識別性を高めるvisual pivot regularizationの提案．</li><li>zero-shot recognition, generalized zero-shot learning, and zero-shot retrievalという複数のタスクでstate-of-the-art手法を超えた．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Imagine_it_for_me_Generative_Adversarial_Approach_for_Zero-Shot_Learning_from_Noisy_Texts_fig.png" alt="Image"></p><p>左上段がFakeデータを作るストリーム．左下段がRealデータを作るストリーム．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>Unseenクラスについてのノイズを含むテキスト記述を入力とし，このクラスのvisual featureを生成するGANを提案．テキストから生成されるvisual featureをFakeデータとし，真の画像から得られるvisual featureをRealデータとしてGANを学習．</p><ul><li>テキストのembedding後，FC層で次元圧縮をし，ノイズの影響を軽減．</li><li>生成された特徴のクラス間の識別性を保存するために, visual pivot regularizationを利用．Generatorの更新に利用．</li><li>Realデータとして真の画像からvisual feature得る際にはVGGを利用．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.01381">arXiv</a></li></ul></div></div><div class="slide_index">[#394]</div><div class="timestamp">2018.5.19 13:28:25</div></div></section><section id="ID_MoCoGAN_Decomposing_Motion_and_Content_for_Video_Generation"><div class="paper-abstract"><div class="title">MoCoGAN: Decomposing Motion and Content for Video Generation</div><div class="info"><div class="authors">Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, Jan Kautz</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>教師不要でコンテンツとモーションという要素に分解し，ビデオを生成するGANを提案．コンテンツを固定しモーションのみ変化させることや，逆も可能．広範囲の実験を行い，量と質ともにSoTAであることを確認．人の服装とモーションの分離や，顔のアイデンティティーと表情の分離が可能であることを示している．</p><p>Contribution:・ノイズからビデオを生成する，条件なしでのビデオ生成GANの提案．
・従来手法では不可能である，コンテンツとモーションのコントロールが可能なこと
・従来のSoTA手法との比較</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/MoCoGAN_Decomposing_Motion_and_Content_for_Video_Generation_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>GAN．</li><li>ランダムベクトルのシーケンスをビデオフレームのシーケンスにマッピングするGenerator．ランダムベクトルの一部はコンテンツ，もう一部はモーションを指定するもの．</li><li>コンテンツの部分空間はガウス分布でモデル化．モーションの部分空間はRNNでモデル化．</li><li>Generatorは一つのフレーム分をベクトルからフレームにマップする働きだけなので，モーションを決めるのは連続するベクトルを生成するRNN部分となる．</li><li>1枚のフレームを入力とするDiscriminatorと連続した数フレームを入力とするDiscriminatorを使うGAN構造を新たに提案．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>ビデオはコンテンツとモーションに分けられるという前提（prior）からスタート</li><li><a href="https://arxiv.org/abs/1707.04993">arXiv</a></li></ul></div></div><div class="slide_index">[#395]</div><div class="timestamp">2018.5.19 13:08:06</div></div></section><section id="ID_Finding_It_Weakly-Supervised_Reference-Aware_Visual_Grounding_in_Instructional_Videos"><div class="paper-abstract"><div class="title">Finding “It”: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos</div><div class="info"><div class="authors">De-An Huang, Shyamal Buch, Lucio Dery, Animesh Garg, Li Fei-Fei, Juan Carlos Niebles</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>言語的な文脈の中で指示語からそれが何であるかを特定する問題（Visual Grounding; 「それを取ってください」の「それ」を動画中から探索するなど）を扱う論文である。この問題に対してMIL（Multiple Instance Learning）を参考にした弱教師付き学習であるReference-aware MIL（RA-MIL）を用いて解決する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180518VisualGrounding.png" alt="180518VisualGrounding"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>画像に対するVisual Groundingが空間的な関係性を捉えるのに対して、Visual Groundingは時間的な関係性を捉える課題である。YouCookII/RoboWatch datasetにて処理を行った結果、弱教師付き学習であるRA-MILを適用するとVisual Groundingに対して精度向上することを明らかにした。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Language and Visionの課題はすでに動画にまで及んでいる。Visual Groundingのみならず、新規問題設定を試みた論文として精読してもよいかも？それと視覚と言語のサーベイ論文は読んでみたい</p><ul><li><a href="http://ai.stanford.edu/~dahuang/papers/cvpr18-ramil.pdf">論文</a></li><li><a href="http://ai.stanford.edu/~dahuang/">著者</a></li><li><a href="http://aclweb.org/anthology/D15-1021">視覚と言語のサーベイ論文</a></li></ul></div></div><div class="slide_index">[#396]</div><div class="timestamp">2018.5.18 16:30:52</div></div></section><section id="ID_Practical_Block-wise_Neural_Network_Architecture_Generation"><div class="paper-abstract"><div class="title">Practical Block-wise Neural Network Architecture Generation</div><div class="info"><div class="authors">Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, Cheng-Lin Liu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ブロック単位でのアーキテクチャ生成手法であるBlockQNNを提案。Q学習（Q-Learning）を参考にして高精度なニューラルネットを探索的（ここではEpsilon-Greedy Exploration Strategyと呼称）に生成する。基本的には生成したブロックを積み上げることによりアーキテクチャを生成するが、早期棄却の枠組みも設けることで探索を効率化している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180517BlockQNN.png" alt="180517BlockQNN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ブロック単位でニューラルネットのアーキテクチャを探索するBlockQNNを提案した。同枠組みはHand-craftedなアーキテクチャに近い精度を出しており（CIFAR-10のtop-1エラー率で3.54）、探索空間を削減（32GPUを3日間使用するのみ！）、さらに生成した構造はCIFARのみならずImageNetでも同様に高精度を出すことを明らかにした。ネットワーク構造の探索問題においてブロックに着目し、性能を向上させると同時に同様の枠組みを複数のデータセットにて成功させる枠組みを提案したことが、CVPRに採択された基準である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>ここから数年で、practicalなGPU数（8GPUや4GPUなど）、1日以内の探索で解決するようになると予想される（し、してくれないと一般の研究者/企業が参入できない）。</p><ul><li><a href="https://arxiv.org/abs/1708.05552">論文</a></li></ul></div></div><div class="slide_index">[#397]</div><div class="timestamp">2018.5.17 13:12:12</div></div></section><section id="ID_Residual_Dense_Network_for_Image_Super-Resolution"><div class="paper-abstract"><div class="title">Residual Dense Network for Image Super-Resolution</div><div class="info"><div class="authors">Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>低解像画像から高解像画像（SR; super-resolution image）を復元するための研究で、DenseNet（論文中の参考文献7）を参考にしたResidual Dense Networks (RDN)を提案して同課題にとりくんだ。異なる劣化特徴をとらえたモデルであること、連続的メモリ構造（Contiguous Memory Mechanism）やコネクションを効果的にするResidual Dense Blockを提案したこと、Global Feature Fusionにより各階層から総合的な特徴表現、を行い高解像画像を復元した。DenseNetで提案されているDense Blockと比較すると、提案のResidual Dense Blockは入力チャネルからもスキップコネクションが導入されているため、よりSRの問題設定に沿ったモデルになったと言える。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180517ResidualDenseNetwork.png" alt="180517ResidualDenseNetwork"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>高解像画像を復元するための改善として、DenseNetを改良したRDNを提案した。Dense Blockを置き換え、より問題に特化したResidual Dense Blockを適用。実験で使用した全てのデータセット（Set5, Set14, B100, Urban100, Manga109）の全てのスケール（x2, x3, x4）にて従来手法よりも良好なAverage PSNR/SSIMを記録した。結果画像はGitHubのページなどを参照されたい。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>課題の肝をつかんで、従来提案されている効果的な手法を改善できるセンスを磨きたい。</p><ul><li><a href="https://arxiv.org/abs/1802.08797">論文</a></li><li><a href="https://github.com/yulunzhang/RDN">GitHub</a></li></ul></div></div><div class="slide_index">[#398]</div><div class="timestamp">2018.5.17 12:47:07</div></div></section><section id="ID_Three_Dimension_Human_Pose_Estimation_in_the_Wild_by_Adversarial_Learning"><div class="paper-abstract"><div class="title">Three Dimension Human Pose Estimation in the Wild by Adversarial Learning</div><div class="info"><div class="authors">Wei Yang, Wanli Ouyang, Xiaolong Wang, Jimmy Ren, Hongsheng Li, Xiaogang Wang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>現在でもチャレンジングな課題として位置付けられる人物に対する3次元姿勢推定に関する研究で、Adversarial Learning (AL)を用いて学習を実施。問題設定としては「多量の」2次元姿勢アノテーション+「少量の」3次元姿勢アノテーションを使用することで、新規環境にて3次元姿勢推定を実行することである。本論文で提案するALではG（生成器）として、2D/3Dのデータセットからそれぞれ2D/3Dの姿勢を推定、実際のデータセットからアノテーションを参照（リアル）して、生成されたものか、データセットのアノテーションなのかを判断（D; 識別器）させることで学習する。G側の姿勢推定ではHourglassによるConv-Deconvモデルを採用、D側には3つの対象ドメイン（オリジナルDB、関節間の相対的位置、2D姿勢位置と距離情報）を入れ込んだMulti-Source Discriminatorを適用する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180517PoseGAN.png" alt="180517PoseGAN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>GANに端を発する敵対的学習を用いて、3次元姿勢に関するアノテーションが少ない場合でもドメイン依存をすることなく3次元姿勢推定を可能にする技術を提案した。また、もう一つの新規性としてドメインに関する事前知識を識別器に入れ込んでおくmulti-source discriminatorについても提案した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>少量のラベル付きデータが用意できていれば、ドメイン関係なく推定ができるという好例である。データとアノテーションに関連するのはCG/敵対的学習/教師なし/ドメイン適応などで、これらは現在のCVにおいても重要技術。少なくともお金がないとクラウドソーシングでデータが集められないという構図を変えたいと思っている。</p><ul><li><a href="https://arxiv.org/abs/1803.09722">論文</a></li><li><a href="http://www.cs.cmu.edu/~xiaolonw/publication.html">著者</a></li></ul></div></div><div class="slide_index">[#399]</div><div class="timestamp">2018.5.17 12:03:18</div></div></section><section id="ID_Gesture_Recognition_Focus_on_the_Hands"><div class="paper-abstract"><div class="title">Gesture Recognition: Focus on the Hands</div><div class="info"><div class="authors">Pradyumna Narayana, J. Ross Beveridge, Bruce A. Draper</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>手部領域に着目してチャネルを追加することにより、ジェスチャ認識自体の精度を高めていくという取り組み。従来型のマルチチャネル（rgb, depth, flow）のネットワークでは限定的な領域を評価して特徴評価を行なっていたが、提案のFOANetでは注目領域（global, right hand, left hand）に対して分割されたチャネルの特徴を用いて特徴評価を行い識別を実施する。図に示すアーキテクチャがFOANetである。FOANetでは12のチャネルを別々に処理・統合し、統合を行うネットワークを通り抜けて識別を実施する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180517FOANet.png" alt="180517FOANet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>手部領域に着目し、よりよい特徴量として追加できないか検討した、とういアイディア自体が面白い。また、ChaLearn IsoGD datasetの精度を従来の67.71%から82.07まで引き上げたのと、同じようにNVIDIA datasetに対しても83.8%から91.28%に引き上げた。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>あまりメジャーに使用されているDBではないが、重要課題を見つけてアプローチする研究は今後さらに必要になってくる？一番最初に問題を解いた人ではないが、二番目に研究をして実利用まで一気に近づけられる人も重宝される。</p><ul><li><a href="http://www.cs.colostate.edu/~draper/papers/narayana_cvpr18.pdf">論文</a></li></ul></div></div><div class="slide_index">[#400]</div><div class="timestamp">2018.5.17 11:20:46</div></div></section><section id="ID_Direct_Shape_Regression_Networks_for_End-to-End_Face_Alignment"><div class="paper-abstract"><div class="title">Direct Shape Regression Networks for End-to-End Face Alignment</div><div class="info"><div class="authors">X. Miao, X. Zhen, V. Athitsos, X. Liu, C. Deng and H. Huang</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>顔のアライメントにおいて，Direct shape regression networkを提案．いくつかの新しい構造を組み合わせている．(1)二重Conv，
(2)フーリエ特徴プーリング，
(3)線形低ランク学習．
顔画像-顔形状間の高い非線形関係性（初期化への強い依存性，ランドマーク相関導出の失敗）の問題を解決する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Direct_Shape_Regression_Networks_for_End-to-End_Face_Alignment_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>複数の新しい構造の定義</li><li>いくつかのケースでSoTAを超える性能．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://xinxinmiao.github.io/pdfs/1607.pdf">論文</a></li></ul></div></div><div class="slide_index">[#401]</div><div class="timestamp">2018.5.18 11:57:02</div></div></section><section id="ID_Scale-recurrent_Network_for_Deep_Image_Deblurring"><div class="paper-abstract"><div class="title">Scale-recurrent Network for Deep Image Deblurring</div><div class="info"><div class="authors">X. Tao, H. Gao, Y. Wang, X. Shen, J. Wang, J. Jia</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>coarse-to-filneに単画像デブラーリングする，Scale-recurrent Network (SRN-DeblurNet)を提案．</p><p>構造的には，(1)入出力がピラミッド画像，
(2)中間はUnet，
(3)最終層の出力を第1層に注入（Recurrent）し，ピラミッド画像の枚数分実行．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Scale-recurrent_Network_for_Deep_Image_Deblurring_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>シンプルでパラメータ数が少ない．</li><li>SoTAを超える性能．例もすごいきれいになっているように見える．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>見た目明らかにきれいになっていると，やはり評価したくなる．</p><ul><li><a href="https://arxiv.org/abs/1802.01770">arXiv</a></li></ul></div></div><div class="slide_index">[#402]</div><div class="timestamp">2018.5.18 11:02:11</div></div></section><section id="ID_Convolutional_Neural_Networks_with_Alternately_Updated_Clique"><div class="paper-abstract"><div class="title">Convolutional Neural Networks with Alternately Updated Clique </div><div class="info"><div class="authors">Yibo Yang et al., </div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>従来のCNNの構造では基本的に決められた方向へのみのforwardを行うのに対して、すべてのレイヤー間で結合を持つClique blockで構成されるClique Netの提案。CIFAR-10でSoTA、その他ImangeNetやSVHNでも少ないパラメータでSoTAに匹敵する精度を記録。</p></div></div><div class="item2"><img src="slides/figs/Convolutional_Neural_Networks_with_Alternately_Updated_Clique.png" alt="Convolutional_Neural_Networks_with_Alternately_Updated_Clique.png"></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>Clique blockでは以下のような処理が行われる。</p><ul><li>畳み込み層によってすべての層を共通の特徴マップで初期化。</li><li>ある層に対して、他のすべての層から畳み込み結合した値で更新。これを各層に対して順次行い、すべての層で更新したら1つのStageが終了。</li><li>上記を決められたStage数行う。畳み込み結合の重みはStage間で共有する。</li></ul><p>DenseNetの拡張に近い構造のため妥当性があり、実際に精度が出ている点が強い。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.10419">論文</a></li></ul></div></div><div class="slide_index">[#403]</div></div></section><section id="ID_Geometry_Guided_Convolutional_Neural_Networks_for_Self_Supervised_Video_Representation_Learning"><div class="paper-abstract"><div class="title">Geometry Guided Convolutional Neural Networks for Self-Supervised Video Representation Learning </div><div class="info"><div class="authors">Chuang Gan et al., </div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>合成画像のペア間のフローと教師ラベルのない実画像のペア間のデプスを推定することによってシーン認識、行動認識のための表現学習を行う研究。フロー推定を行ったのち、デプス推定にfine-tuningし、さらに目的となるタスクにfine-tuningする。
直感的には、低レベルな特徴が獲得されそうだが、行動認識などの高次な問題設定でも効果を発揮した。</p></div></div><div class="item2"><img src="slides/figs/Geometry_Guided_Convolutional_Neural_Networks_for_Self_Supervised_Video_Representation_Learning.png" alt="Geometry_Guided_Convolutional_Neural_Networks_for_Self_Supervised_Video_Representation_Learning.png"></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>多段にfine-tuningするため、初期の問題設定によって獲得した特徴が失われてしまう可能性があるので、２段目のfine-tuning時にはfine-tuning前の出力結果への蒸留を同時に行う。ImageNetのpretrainingとも行動認識において補間的な関係がある。表現学習自体での使用データが少ないのに関わらず高い精度向上が実験的に示されたことが大きなcontributionだと考えられる。        </p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>特徴のforgetを防ぐ手法は、複数のタスクで学習済みモデルを作成する際に、その順番が重要となるような状況で有用だと思われる。既存手法との比較においては今回は+αのデータを利用している点はフェアではないと感じた。
また、目的のタスクへのfine-tuningの際のフレームペアの選び方などの詳細な設定が記されていなかった。主に精度評価のみで、高次なタスクでうまくいく考察が少なく、疑問もあった。</p><ul><li><a href="http://cseweb.ucsd.edu/~haosu/papers/cvpr18_geometry_predictive_learning.pdf">論文</a></li></ul></div></div><div class="slide_index">[#404]</div></div></section><section id="ID_Learning_to_Compare_Relation_Network_for_Few-Shot_Learning"><div class="paper-abstract"><div class="title">Learning to Compare: Relation Network for Few-Shot Learning</div><div class="info"><div class="authors">F. Sung, Y. Yang, L. Zhang, T. Xiang, P.H.S. Torr, T.M. Hospedales</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>メタ学習を用いたFew-shot learningの新しい枠組み，Relation Networkの提案．一度学習されれば，ネットワークのアップデートの必要なしに新しいクラスの画像分類ができるようになる．</p><p>1エピソードにおける少数の画像の比較によって距離メトリックを学習するメタラーニングを行う．少数の新クラスの代表画像群とクエリ画像の関連性スコアの比較により，追加学習なしに新クラス画像分類が行える．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Compare_Relation_Network_for_Few-Shot_Learning_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>再学習しなくても，データさえ用意しておけば未知のクラスも分類可能な画像分類器ができる．</li><li>Zero-shot learningにも拡張可能．</li><li>シンプルで，高速に動作し，拡張性も高い．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>テスト時も少数のデータを用意しておけば，という考え方はイマドキ感がある．</p><ul><li><a href="https://arxiv.org/abs/1711.06025">arXiv</a></li><li><a href="https://github.com/lzrobots/LearningToCompare_ZSL">GitHub</a></li></ul></div></div><div class="slide_index">[#405]</div><div class="timestamp">2018.5.18 10:30:48</div></div></section><section id="ID_MegaDepth_Learning_Single-View_Depth_Prediction_from_Internet_Photos"><div class="paper-abstract"><div class="title">MegaDepth: Learning Single-View Depth Prediction from Internet Photos</div><div class="info"><div class="authors">Z.Li and N.Snavely</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1804.00607</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>画像における深度予測はCV分野において基本的なタスクである．既存の手法は学習データによる制約が伴う．今回提案する手法では，インターネットの画像をデータセットとするMVSの手法を改良し，既存の3D reconstructionとsemantic ラベルを組みわせて大規模な深度予測モデルであるMegaDepthを提案．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/MegaDepth_Learning_Single-View_Depth_Prediction_from_Internet_Photos.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>セマンティックセグメンテーションを用いた順序による深度関係を自動で拡張</li><li>MegaDepthが強力なモデルであることを示すために膨大なインターネット画像を使い検証</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>深度予測にsemantic ラベルを取り入れることで精度が向上．</li><li>semanticラベルを用いており，複雑背景における物体検出にも応用可能かも！！</li><li><a href="https://arxiv.org/pdf/1803.01599.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#406]</div><div class="timestamp">2018.5.18 02:33:22</div></div></section><section id="ID_Real-Time_Rotation-Invariant_Face_Detection_with_Progressive_Calibration_Networks"><div class="paper-abstract"><div class="title">Real-Time Rotation-Invariant Face Detection with Progressive Calibration Networks</div><div class="info"><div class="authors">FXuepeng Shi, Shiguang Shan, Meina Kan, Shuzhe Wu, Xilin Chen</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>リアルタイムで顔の回転に頑健な顔検出を行うProgressive Calibration Network(PCN)を提案。PCNは3つのステージで構成されており、それぞれのステージでは検出された領域を0° or 180°回転させる、
0° or 90° or -90°回転させる、頭が上にくるように顔を回転させる、という処理をそれぞれ行う。
また各ステージ共通で検出された領域が顔であるか顔でないかという識別を行う。第1,2ステージで粗く回転を行うことで第3ステージにおける回転量と、
各ステージにおける顔識別の学習が容易になったことで、高精度かつリアルタイムに顔検出を行うことが可能となった。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Real-Time_Rotation-Invariant_Face_Detection_with_Progressive_Calibration_Networks.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来手法であるデータオーギュメンテーション、角度の値域を分割してそれぞれの検出器を学習させる方法、角度の回転角を推定する流手法では、どれもネットワークが大きくなりすぎるためにリアルタイムでの実行が難しかった。</li><li>解像度が40x40以上の顔を検出。</li><li>state-of-the-artの手法と比べて同等の精度を達成し、かつGPUを使用した際の実行スピードは4.2倍となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li> <a href="https://github.com/Jack-CV/PCN">GitHub with Demos</a></li><li><a href="https://arxiv.org/abs/1804.06039">論文</a></li></ul></div></div><div class="slide_index">[#407]</div></div></section><section id="ID_Partially_Shared_Multi-Task_Convolutional_Neural_Network_with_Local_Constraint_for_Face_Attribute_Learning"><div class="paper-abstract"><div class="title">Partially Shared Multi-Task Convolutional Neural Network with Local Constraint for Face Attribute Learning</div><div class="info"><div class="authors">Jiajiong Cao, Yingming Li, Zhongfei Zhang</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔のアトリビュート推定に有効なネットワークであるPS-MCNN/-LCを提案。従来手法のMCNNでは、類似度の高いアトリビュートの識別率を高めるために、
類似度の高いアトリビュートのごとにグループを形成し、MCNNの高い層では各グループごとにCNNを形成して学習を行なっていた。
そのため低い層で得られていた特徴量が消失するという問題が起きていた。
これを解決するために、MCNNに対して各レベルで得られた特徴量を教諭するShared Netを導入したPS-MCNNを提案。
また同一人物において推定されたアトリビュート同士のロスをとるPS-MCNN-LCも提案した。
ネットワークの構築に関する議論も行なっている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Partially_Shared_Multi-Task_Convolutional_Neural_Network_with_Local_Constraint_for_Face_Attribute_Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>同一人物において推定されたアトリビュート同士のロスをとることで、アトリビュートの空間を限定することが可能となるという考えのもとPS-MCNN-LCを提案している。</li><li>state-of-the-artに比べて、CelebAデータセットではPS-MCNN-LCが40種全てのアトリビュートにおいて最も高い精度を達成、LFWAデータセットではPS-MCNN/-LCを合わせて37種において最も高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>精度が上がったことはもちろんだが、既存研究であるMCNNのリミテーションを正確に見抜いてネット枠を改善している点が採択につながったと考えられる。</li><li><a href="http://person.zju.edu.cn/attachments/2018-04/01-1524825782-717898.pdf">論文</a></li></ul></div></div><div class="slide_index">[#408]</div></div></section><section id="ID_Deep_Semantic_Face_Deblurring"><div class="paper-abstract"><div class="title">Deep Semantic Face Deblurring</div><div class="info"><div class="authors">Ziyi Shen, Wei-Sheng Lai, Tingfa Xu, Jan Kautz, Ming-Hsuan Yang</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔に対してセマンティックセグメンテーション(face sparsing)を利用することで、モーションブラーが加えられた正面顔画像に対するCNNベースのデブラーリング手法を提案。
face sparsingによって顔のパーツの位置関係や形といった情報を利用することができると主張。
また学習の際には様々なカーネルサイズによるブラー画像を同時に与えるのではなく、
小さなカーネルサイズのブラー画像から順々に学習させるincremental trainingことでデブラーリング精度を向上させた。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Semantic_Face_Deblurring.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ブラー画像はランダムな3D cameraの軌道によって与えられ、カーネルサイズは13x13~27x27までを学習させた。</li><li>ロスとしてデブラーリング画像のL1 loss, face parsing画像のL1 loss, adversarial loss, CNNの特徴量マップのL2 ロスを使用。</li><li>tate-of-the-artに比べてデブラーリング画像とソース画像のPSNR、SSIM、顔の検出率、個人認証の精度においてもっとも良い精度を達成し、それぞれ約5%, 5%, 28%, 4%向上した。</li><li>state-of-the-artに比べて実行スピードが約44%向上した。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>学習データを少しずつ変化させて、順々に最適化を行うincremental trainingは、学習データをパラメトリックに変化可能な他の問題に対しても有用なトレーニング方法だと思われる。</li><li><a href="https://arxiv.org/abs/1803.03345">論文</a></li></ul></div></div><div class="slide_index">[#409]</div></div></section><section id="ID_Learning_to_Adapt_Structured_Output_Space_for_Semantic_Segmentation"><div class="paper-abstract"><div class="title">Learning to Adapt Structured Output Space for Semantic Segmentation</div><div class="info"><div class="authors">Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, Manmohan Chandraker</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Hiroaki Aizawa</div><div class="item1"><div class="text"><h1>概要</h1><p>Semantic Segmentationに関するDomain Adaptationの研究。Semantic Segmentationをsource domainとtarget domain間の空間的な類似性を持つ構造的な出力として考え、出力空間(prediction map)でのDomain Adaptationを行う敵対的学習手法を提案。低次特徴は利用せず、高次特徴のみを複数のDiscriminatorにより異なる空間解像度ごとに適応させる(Multi-level Adversarial Learning)。実験ではsynthetic-to-realとcross-cityでの比較を行っている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Adapt_Structured_Output_Space_for_Semantic_Segmentation.png" alt="Learning_to_Adapt_Structured_Output_Space_for_Semantic_Segmentation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>画像分類タスクを中心に発展していたDomain Adaptationを画素単位の構造予測が必要なSemantic Segmentationに適用した点。</p><p>Semantic Segmentationに限らず構造予測をするタスクへも容易に拡張ができる。</p><p>画像分類と比較して、アノテーションの労力がかかるため実用性・将来性がある。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.10349">arxiv</a></li><li><a href="https://github.com/wasidennis/AdaptSegNet">github</a></li></ul></div></div><div class="slide_index">[#410]</div><div class="timestamp">2018.5.17 20:28:45</div></div></section><section id="ID_Multi_Task_Learning_Using_Uncertainty_to_Weigh_Losses_for_Scene_Geometry_and_Semantics"><div class="paper-abstract"><div class="title">Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</div><div class="info"><div class="authors">Alex Kendall et al., </div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>学習時のタスクごとの重みによって精度がかなり変化する。そこでNNのマルチタスクモデルにおいて各出力を分布表現にし、その同時確率を最尤推定するように学習することで結果的にタスクごとの不確実性を考慮した重み付けを損失関数に課す。実験ではSemantic Segmentation, Instance Segmentation, Depth estimationのマルチタスク学習を行い、等しい重みや手動での重み設計時よりも良い結果となった。</p></div></div><div class="item2"><img src="slides/figs/Multi_Task_Learning_Using_Uncertainty_to_Weigh_Losses_for_Scene_Geometry_and_Semantics.png" alt="Multi_Task_Learning_Using_Uncertainty_to_Weigh_Losses_for_Scene_Geometry_and_Semantics.png"></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>モデルから各タスクに対して不確実性を表す値を同時に出力させる。回帰タスクの場合はこれが分散を表し、最終的には回帰出力値を平均とするガウス分布として表現する。識別タスクについては不確実性が分布の温度パラメータとして扱われる。これらの同時確率を最尤推定すると、通常の損失に対してタスクごとに適応的に重み付けされた損失を最適化していることになる。理論的にも妥当であり、精度向上は大きくチューニングの手間が省けるという点でかなり便利である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>簡単な実装でハイパーパラメータが減るという点でかなり有用に感じた。様々なマルチタスクで行った訳ではないのでこの手法の汎用性がきになる。結局、識別の場合は通常でも不確実性は考慮しているので、本質的に新しいのは回帰の場合である。</p><ul><li><a href="https://arxiv.org/abs/1705.07115">論文</a></li></ul></div></div><div class="slide_index">[#411]</div></div></section><section id="ID_Compare_and_Contrast_Learning_Prominent_Visual_Differences"><div class="paper-abstract"><div class="title">Compare and Contrast: Learning Prominent Visual Differences</div><div class="info"><div class="authors">S.Chen and K.Grauman</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1804.00112</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>2つの画像間で最も顕著な違いは表せられるがその他の細かい違いは示されないことが多い．それに対して，より多くの違いによって画像を比較できるようなモデルの構築をした．また，そのモデルを使って，UT-Zap50K shoesとthe LFW10のデータセットを用いて評価したところSoTAであった．構築したモデルを画像記述と画像検索に導入し，拡張を図った．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Compare_and_Contrast_Learning_Prominent_Visual_Differences.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>画像中から目立つ部分をアノーテーションで収集し，ランク付けすることでモデルの構築．</li><li>UT-Zap50K shoes（靴）とthe LFW10（顔）のデータセットを用いて評価．</li><li>画像記述と画像検索のタスクに応用し，拡張を図る</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>画像説明文に応用できればキャプショニングの幅を広げられそう．</p><ul><li><a href="https://arxiv.org/pdf/1804.00112.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#412]</div><div class="timestamp">2018.5.17 16:18:51</div></div></section><section id="ID_Learning_Rich_Features_for_Image_Manipulation_Detection"><div class="paper-abstract"><div class="title">Learning Rich Features for Image Manipulation Detection</div><div class="info"><div class="authors">P. Zhou, X. Han, V.I. Morariu and L.S. Davis</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>画像修正検出．修正箇所をちゃんと注目すべきで，リッチな特徴の学習が必要．修正後画像から修正領域を検出するtwo-stream Faster R-CNNを提案．
RGB stream：コントラスト差，不自然境界とかを捉える．Noise stream：ノイズの非一貫性を捉える．<a href="https://ieeexplore.ieee.org/document/6197267/">Steganalysis Rich Model</a>でとれたノイズ特徴に基づく．
そして，両者のバイリニアプーリングで共起性を捉える．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_Rich_Features_for_Image_Manipulation_Detection.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>修正箇所のノイズ感の差を見るアイデアは昔にあったが，それを導入したという温故知新．</li><li>実験によりリサイズや圧縮に対するロバスト性におけるSOTAを確認．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1805.04953">arXiv</a></li><li><a href="http://www.cosy.sbg.ac.at/~uhl/mmsec/LukFriSPIE06_v9.pdf">過去のノイズ感の差を使った画像加工領域検出の例</a></li></ul></div></div><div class="slide_index">[#413]</div><div class="timestamp">2018.5.17 15:16:28</div></div></section><section id="ID_Real-Time_Seamless_Single_Shot_6D_Object_Pose_Prediction"><div class="paper-abstract"><div class="title">Real-Time Seamless Single Shot 6D Object Pose Prediction</div><div class="info"><div class="authors">Bugra Tekin et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>1枚のRGB画像から物体の6次元姿勢を推定する研究. CNN を用いた単一のネットワーク (YOLO v2 ベース) で RGB 画像から物体の 3D bounding box を直接推定する. post-process 無しで高精度な姿勢推定が可能なため, 実時間（従来手法の約５倍速）で従来手法と同程度の推定精度を達成した. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Real-Time-Seamless-Single-Shot-6D-Object-Pose-Prediction.png" alt="fukuhara-Real-Time-Seamless-Single-Shot-6D-Object-Pose-Prediction.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ネットワークはRGB画像1枚の入力に対して, 各物体の制御点（3D bounding box 8点 と centroid 1点）の位置, カテゴリー, 推定の確信度を出力する.</li><li>推定された物体の9つの制御点の位置に対して PnP 問題を解くことで6次元姿勢を推定する.</li><li>物体の bounding box の情報から学習を行うので物体の詳細な3次元モデルが必要無い. また, テクスチャーが殆ど無い物体に対しても適用が可能.</li><li>物体が複数あった場合でも PnP 以外の部分の計算量は増えないので, 物体数に関わらず計算時間はほぼ一定.(従来手法の SSD-6D は線型に増加.)</li><li>LINEMOD や OCCLUSION データセットを用いた評価実験では従来手法 (BB8 や SSD-6D) と同等かそれ以上の精度を 50fps (SSD-6Dの約５倍) で達成.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.08848" target="blank">[論文] Real-Time Seamless Single Shot 6D Object Pose Prediction</a></li><li><a href="https://btekin.github.io/" target="blank">[著者HP] Bugra Tekin</a></li></ul></div></div><div class="slide_index">[#414]</div><div class="timestamp">2018.5.17 12:19:55</div></div></section><section id="ID_Video_Captioning_via_Hierarchical_Reinforcement_Learning"><div class="paper-abstract"><div class="title">Video Captioning via Hierarchical Reinforcement Learning</div><div class="info"><div class="authors">Xin Wang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Video captioning のための階層型強化学習フレームワークを提案. Caption を複数のセグメントに分割し, High-level の Manager Module が各セグメントのコンテキストをデザインし, Low-level の Worker Modeule が単語を生成することで順次セグメントを作成する. 提案手法は MSR-VTT データセット を用いた評価実験で既存手法よりも複数の評価尺度で良い結果となった. また, video captioning のための新しい大規模データセットを公開. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Video-Captioning-via-Hierarchical Reinforcement-Learning.png" alt="fukuhara-Video-Captioning-via-Hierarchical Reinforcement-Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Video captioning の問題を強化学習の問題として定式化し直し, 効率的に学習をすることができる階層型強化学習手法を提案した.</li><li>High-level の Manager Module が目標を達成するために必要なゴールを設定し, Low-level の Worker Modeule がゴールを達成するための基本行動を行う. また, Internal Critic がゴールが達成されたかどうかの評価を行う.</li><li>Action recognition や segmentation で主に用いられている Charades データセットをもとにvideo captioning のための新しい大規模データセットを作成. 既存の MSR-VTT データセットよりも詳細で長い caption が与えられている.</li><li>MSR-VTT データセットを用いた評価実験では, 既存手法（Mean-Pooling, Soft-Attention, S2VT等）と比較して複数の評価尺度で最も良い結果を得た.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.11135" target="blank">[論文] Video Captioning via Hierarchical Reinforcement Learning</a></li><li><a href="http://www.cs.ucsb.edu/~xwang/#" target="blank">[著者HP] Xin Wang</a></li></ul></div></div><div class="slide_index">[#415]</div><div class="timestamp">2018.5.17 12:11:55</div></div></section><section id="ID_Multi-view_Consistency_as_Supervisory_Signal_for_Learning_Shape_and_Pose_Prediction"><div class="paper-abstract"><div class="title">Multi-view Consistency as Supervisory Signal for Learning Shape and Pose Prediction</div><div class="info"><div class="authors">Shubham Tulsiani et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>１枚のRGB画像から物体の形状とカメラ姿勢の両方を推定する研究. 異なる視点から見たときの一貫性(具体的には物体の輪郭または深度情報の一貫性)を教師情報として用いるため, 従来手法と異なり学習時に物体の３次元形状と姿勢のいずれについても直接の教師データも必要としない.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Multi-view-Consistency-as-Supervisory-Signal-for-Learning-Shape-and-Pose-Prediction.png" alt="fukuhara-Multi-view-Consistency-as-Supervisory-Signal-for-Learning-Shape-and-Pose-Prediction.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>物体の形状とカメラ姿勢の両方を推定するタスクに置いて, 直接の教師データを用いずに学習する方法を提案した.</li><li>学習時の入力は同一の物体を異なる位置から撮影したRGB画像２枚と２枚目の画像の物体の Mask または Depth 画像.</li><li>１枚目の画像から３次元形状, ２枚目の画像からカメラ姿勢をそれぞれ推定し, 推定された形状を推定された姿勢から見た時に, 与えられたマスク画像と同じ結果が得られるように学習を行う.</li><li>ShapeNet データセットを用いた評価実験では, 直接の教師あり学習を行った手法とほぼ同等の結果であった.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1801.03910" target="blank">[論文] Multi-view Consistency as Supervisory Signal for Learning Shape and Pose Prediction</a></li><li><a href="https://shubhtuls.github.io/mvcSnP/" target="blank">[Project Page]</a></li><li><a href="https://github.com/shubhtuls/mvcSnP" target="blank">[Code]</a></li></ul></div></div><div class="slide_index">[#416]</div><div class="timestamp">2018.5.12 13:05:55</div></div></section><section id="ID_PAD_Net_Multi-Tasks_Guided_Prediction_and_Distillation_Network_for_Simultaneous_Depth_Estimation_and_Scene_Parsing"><div class="paper-abstract"><div class="title">PAD-Net: Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing</div><div class="info"><div class="authors">Dan Xu et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"></div><h1>概要</h1><p>CNNに対して中間的に法線方向推定と輪郭推定も加えることで最終的にdepth推定とscene parsingの精度を向上させる。法線方向と輪郭についてはdepthとscene parsingのラベルから計算可能であるので追加にアノテーションする必要はない。
NYUD-v2とCityscapesにおいてSoTA。  </p></div><div class="item2"><img src="slides/figs/PAD_Net_Multi-Tasks_Guided_Prediction_and_Distillation_Network_for_Simultaneous_Depth_Estimation_and_Scene_Parsing.png" alt="PAD_Net_Multi-Tasks_Guided_Prediction_and_Distillation_Network_for_Simultaneous_Depth_Estimation_and_Scene_Parsing.png"></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>中間的に推定した結果を元に最終的な目的タスクを出力するが、その中間出力として3つのパターンを考えた(タスクをに分けずconcat, タスクごとにconcat, attention機構を取り入れたconcat)。 attention機構を取り入れたconcatが最も良い結果となった。シンプルな手法だが、実験結果が良いので評価されたと考えられる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>「distillation」という言葉を用いているが、生徒モデルと教師モデルがあるようなdistillation手法は使われておらず、単に複数の中間タスクからのMulti-modalな情報の統合に対してその言葉が使用されている。
単に通常のマルチタスク推定に中間タスクを導入したのみでかなりシンプルな印象。</p><ul><li><a href="https://scirate.com/arxiv/1805.04409">論文</a></li></ul></div></div><div class="slide_index">[#417]</div></div></section><section id="ID_Convolutional_Sequence_to_Sequence_Model_for_Human_Dynamics"><div class="paper-abstract"><div class="title">Convolutional Sequence to Sequence Model for Human Dynamics</div><div class="info"><div class="authors">Chen Li, Zhen Zhang, Wee Sun Lee, Gim Hee Lee</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>時空間的な特徴を捉えて、長期のモーション予測を行う研究である（ここではいかに最初の限られた情報量のみでシーケンスを推定できるかどうかについて検証を行なっている）。この課題に対し、Convolutional Long-term Encoderを用いてより長期的な隠れ変数をデコーダにより推定する。このエンコーダ-デコーダ構造にて短期〜より長期的な変数の予測を可能にする。本手法では主にRNNベースのSequence-to-SequenceなモデルにConvolutionalな要素を加えたことが技術的発展であると主張。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180517ConvSeq2Seq.png" alt="180517ConvSeq2Seq"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>より長期の（といっても数秒間のシーケンス？）人物モーション予測（ここでは人物姿勢位置を予測）を実現したことが課題設定として大きい。手法としてはConvolutional Long-term Encoderやその抽象化された特徴をデコーダにより長期隠れ変数を推定。Human3.6MやCMU Motion Capture datasetにて高い精度を実現した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Short-termからLong-term（Short-term: 〜3秒、Long-term: 5秒〜; 明確な定義はなされていないが。。）の行動/姿勢の予測はまだまだ未解決だし、何を予測するかに関しての定義づけ自体の整備も曖昧なままである。まだまだ参入の余地が残されているように見える。</p><ul><li><a href="https://arxiv.org/abs/1805.00655">論文</a></li><li><a href="https://github.com/chaneyddtt/Convolutional-Sequence-to-Sequence-Model-for-Human-Dynamics">GitHub</a></li></ul></div></div><div class="slide_index">[#418]</div><div class="timestamp">2018.5.17 01:05:51</div></div></section><section id="ID_LSTM_Pose_Machines"><div class="paper-abstract"><div class="title">LSTM Pose Machines</div><div class="info"><div class="authors">Yue Luo, Jimmy Ren, Zhouxia Wang, Wenxiu Sun, Jinshan Pan, Jianbo Liu, Jiahao Pang, Liang Lin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Convolutional Pose Machine (CPM)のCNN部分を再帰的ネットであるLSTM (Long-short term memory)により置き換えた人物姿勢推定手法。時系列的に連続するフレーム（e.g. t, t+1, t+2）の入力に対して処理を実行し姿勢を推定する。CPMとは基本となるアーキテクチャの考え方（multi-stage algorithm）は同様であるが、それぞれのステージ間でパラメータを共有している点で異なる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180516LSTMPoseMachine.png" alt="180516LSTMPoseMachine"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>CPMと同じmulti-stageの姿勢推定学習を、LSTMの構造にて実現したことが技術的なポイントである。さらに、CPMとは異なりステージ間でパラメータを共有することで精度向上が見られたと説明。Penn Action datasetやJHMDB datasetにて最高精度を叩き出した。JHMDBにて93.6@PCK(=0.2)、Penn Actionにて97.7@PCK(=0.2)を記録。さらに、各フレーム時のメモリチャンネルの挙動も可視化し、どのような際に成功するか/失敗するかを明らかにした。複雑姿勢（複雑背景？）の際にはエッジに着目していて、姿勢推定が成功する際にはピンポイントで関節位置を回帰する傾向にある。処理速度の面においても本論文の技術では25.6msで動作した（CPMは48.4ms）。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>アーキテクチャ自体上手くいったモデルを異なるアプローチ（この場合はConvolutionalなモデルをRecurrentに変更）で実行して、どういった改善ができるかを試しているところが面白い。LSTMの場合にはステージ間でパラメータを共有するところがポイントであった。その上で精度を向上している点が実行力に優れていると言える。</p><ul><li><a href="https://arxiv.org/abs/1712.06316">論文</a></li><li><a href="http://www.jimmyren.com/">著者</a></li><li><a href="https://github.com/lawy623/LSTM_Pose_Machines">GitHub</a></li><li><a href="https://www.youtube.com/watch?v=-sP3LWl6Ul0">YouTube</a></li></ul></div></div><div class="slide_index">[#419]</div><div class="timestamp">2018.5.16 17:25:49</div></div></section><section id="ID_DecideNet_Counting_Varying_Density_Crowds_Through_Attention_Guided_Detection_and_Density_Estimation"><div class="paper-abstract"><div class="title">DecideNet: Counting Varying Density Crowds Through Attention Guided Detection and Density Estimation</div><div class="info"><div class="authors">Jiang Liu, Chenqiang Gao, Deyu Meng, Alexander G. Hauptmann</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>混雑時の人数カウントにおける問題点を解決するため、End-to-Endで学習可能なDecideNet（DEteCtIon and Density Estimation Network）を提案する。混雑時の人数カウントでは、従来（１）人物検出では認識ミスによる過不足によりカウントを誤ってしまう、（２）回帰ベースの手法では人物が存在しない領域が蓄積されると実際のカウントよりも多く集計されてしまう、という問題が存在した。DecideNetでは検出ベース/回帰ベースを別々に行い、それらの結果を総合してカウントを行うという点で従来法を解決していると言える。実験では本論文で提案のDecideNetが混雑時の人数カウントにおいてもっとも優れた精度を達成したと主張。検出/回帰の手法としてはFaster R-CNN/RegNetを適用している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180516DecideNet.png" alt="180516DecideNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>3つのベンチマーク（Mall, ShanghaiTech PartB, WorldExpo10 dataset）においてState-of-the-artな精度を達成すると同時に、混雑時の人数カウントの問題と異なるアプローチを同時実行して相補的なアプローチDecideNetを提案したことが採択された大きな理由である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>異なる複数のアプローチを統合して最高精度を達成するためには、その分野における積み重ねと実装力が必要である。論文の書き方と合わせて鍛えていくことで毎回難関国際会議に突破できる力がつくと思われる。</p><ul><li><a href="https://arxiv.org/pdf/1712.06679.pdf">論文</a></li><li><a href="www">Project</a></li><li><a href="www">GitHub</a></li></ul></div></div><div class="slide_index">[#420]</div><div class="timestamp">2018.5.16 11:59:31</div></div></section><section id="ID_Cascaded_Pyramid_Network_for_Multi-Person_Pose_Estimation"><div class="paper-abstract"><div class="title">Cascaded Pyramid Network for Multi-Person Pose Estimation</div><div class="info"><div class="authors">Y. Chen, Z. Wang, Y. Peng, Z. Zhang, G. Yu and J. Sun</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>複数人ポーズ推定には，キーポイントの半／全遮蔽や，複雑な背景といった要素(hard keypoints)が問題になる．Cascaded Pyramid Networkを提案．
hard keypointに対応するためのもの．2つの構造からなる．</p><ul><li>GlobalNet<br>ピラミッド構造をしていて，遮蔽などの無いシンプルなキーポイントの検出として作用する．この時点ではhard性にはあまり対応していない．</li><li>RefineNet<br>hard keypointを考慮した層．
GlobalNetのピラミッドな特徴を拾って，ResNetのBottleneckにかける．
ここで，何もしないとシンプルキーポイントだけ見てしまうので，損失関数の計算時，online hard keypoints miningする．
テスト時のロスを参考にオンラインでhard keypointを選択，選んだキーポイントのものだけバックプロパゲーションにまわすという作業．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Cascaded_Pyramid_Network_for_Multi-Person_Pose_Estimation_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>新規ネットワーク構造の提案</li><li>MS COCO keypoint benchmarkにてSOTA</li><li>実験を結構頑張っている様子．online hard keypoint miningの有無に関する議論などある．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>online hard keypoint miningについて実装可能なレベルでは詳しく書いてなかった．コード読めということか．</p><ul><li><a href="https://arxiv.org/abs/1711.07319">arXiv</a></li><li><a href="https://github.com/chenyilun95/tf-cpn">GitHub</a></li></ul></div></div><div class="slide_index">[#421]</div><div class="timestamp">2018.5.16 18:24:47</div></div></section><section id="ID_One-shot_Action_Localization_by_Learning_Sequence_Matching_Network"><div class="paper-abstract"><div class="title">One-shot Action Localization by Learning Sequence Matching Network</div><div class="info"><div class="authors">H. Yang et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">ある長い動画中から指定した対象動画と同じActionを探してくるOne-shot Action Localizationの研究．
Matching Networkという手法がベースになっていて，それを動画のAction Localizationに応用．
基本的には動画をEncoding (Video Encoder) して，
類似度を計算 (Similarity Network) して，ラベリング (Labeling Network)．
長い方の動画はSliding Windowで分割 (Proposals) して，Proposalsと指定動画の間で類似度を計算．
Encoderは動画でよくやられるTwo-stream CNNとLSTMを利用．
学習はMeta Learningの形式で定式化され，End-to-Endで学習可能．
</div></div><div class="item2"><img src="slides/figs/One-shot_Action_Localization_by_Learning_Sequence_Matching_Network.png" alt="One-shot_Action_Localization_by_Learning_Sequence_Matching_Network.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>Deep時代になってからほとんどやられていなかったOne-shot Action Localization (Action search)</li><li>ProposalsのEncoding，類似度計算，ラベリングと3つすべてが微分可能でEnd-to-Endで学習可能</li><li>普通のTemporal Action LocalizationのSOTA手法よりもOne-shotの設定では高い性能を実現</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="http://www.porikli.com/mysite/pdfs/porikli%202018%20-%20One-shot%20action%20localization%20by%20learning%20sequence%20matching%20network.pdf">論文（著者ページ）</a></li><li>やっている事自体は至って普通のアプローチに感じる</li><li>End-to-End, Meta Learningと今風の形で実現できているのが評価されているのかな</li></ul></div></div><div class="slide_index">[#422]</div><div class="timestamp">2018.5.8 12:00:18</div></div></section><section id="ID_Exploit_the_Unknown_Gradually_One-Shot_Video-Based_Person_Re-Identification_by_Stepwise_Learning"><div class="paper-abstract"><div class="title">Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning</div><div class="info"><div class="authors">Yu Wu, Yutian Lin, Xuanyi Dong, Yan Yan, Wanli Ouyang, Yi Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ワンショット学習（One-shot Learning）により動画像における人物再同定（person re-identification）を実行する論文。ラベルなしのtracklets（人物から抽出した動線）が容易かつ事前に手に入ることから、このtrackletsを徐々に改善しつつ人物同定率を高めるようにCNNを学習していく手法を提案する。本論文での学習では、最初にひとつのラベルを用いて初期化したあと、（１）信頼度の高い少量のサンプル（簡単なサンプル）に対して擬似ラベルを付与、（２）擬似ラベルを含めたラベルを元にカテゴリを更新してより難しいサンプルも取り込む、を繰り返して学習を行う。実験的に擬似ラベルを選択する方法についても議論している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180516OneShotREID.png" alt="180516OneShotREID"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>正解ラベルが付与されたある画像一枚を準備するだけで擬似ラベルを推定して徐々に学習を進めていくワンショット学習を提案した。人物再同定の問題においては有効な解決策であることを示したことがCVPRに採択された基準である。ワンショット学習によりrank-1の精度が21.46@MARS dataset、16.53@DukeMTMC-VideoReID datasetであり、コードも公開されている。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>ワンショットのラベルと信頼できる擬似ラベルから徐々に概念を獲得するのはうまいやり方。あらゆる枠組みで用いることができそう。</p><ul><li><a href="https://yu-wu.net/pdf/CVPR2018_Exploit-Unknown-Gradually.pdf">論文</a></li><li><a href="http://xuanyidong.com/publication/cvpr-2018-eug/">Project</a></li><li><a href="https://github.com/Yu-Wu/Exploit-Unknown-Gradually">GitHub</a></li><li><a href="https://yu-wu.net/">著者</a></li></ul></div></div><div class="slide_index">[#423]</div><div class="timestamp">2018.5.16 11:18:38</div></div></section><section id="ID_PoseTrack_A_Benchmark_for_Human_Pose_Estimation_and_Tracking"><div class="paper-abstract"><div class="title">PoseTrack: A Benchmark for Human Pose Estimation and Tracking</div><div class="info"><div class="authors">Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton Milan, Juergen Gall, Bernt Schiele</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>動画シーケンスにおいて2D姿勢推定のベンチマークを提供する。本論文で提案するベンチマークでは特に、人物の重なりを含む混雑シーン、密なアノテーションを提供する。さらに右の画像で示すようにドメイン依存していない多様な（diverse）シーンを捉えつつ姿勢アノテーション数でも有数、1画像に対する複数人物/ビデオに対するラベルづけにも対応している。トータルでは23,000画像に対して153,615人の姿勢アノテーションを行なった。チャレンジとしては単一フレームに対する姿勢推定（single-frame pose estimation）、ビデオに対する姿勢推定（pose estimation in videos）、姿勢トラッキング（pose tracking）を提供し、評価用サーバも提供する。同DBに対するベンチマーキングではOpenPoseにも導入されているPAFを改良したML-LAB（引用52）がトップ（70.3@mAP）、Mask R-CNNをベースにしたProTracker（引用11）は64.1@mAPであった。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180515PoseTrackBenchmark.png" alt="180515PoseTrackBenchmark"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>大規模かつ静止画ではなく動画に対する人物姿勢データセットを構築し、さらには評価サーバを提供、さらに最先端手法に関するベンチマーキングを行なっていることが新規性およびCVPRに通った理由であると考える。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>データセットの比較図に多様なドメインから収集（diverse）と書かれているが、これらをすべて統合すると相当な量のデータになるのでは？（だれかやってそう）もしくはドメインを合わせれば学習の効果がありそう。</p><ul><li><a href="https://arxiv.org/abs/1710.10000">論文</a></li><li><a href="www.posetrack.net">Project</a></li><li><a href="https://scholar.google.com/citations?hl=ja&amp;user=6rl-XhwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate">著者</a></li></ul></div></div><div class="slide_index">[#424]</div><div class="timestamp">2018.5.15 12:16:20</div></div></section><section id="ID_Camera_Style_Adaptation_for_Person_Re-identification"><div class="paper-abstract"><div class="title">Camera Style Adaptation for Person Re-identification</div><div class="info"><div class="authors">Zhun Zhong, Liang Zheng, Zhedong Zheng, Shaozi Li, Yi Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Person Re-ID（人物再同定）は異なるカメラ間で同一人物を対応づける問題設定であり、画像の質や形式が異なるため非常に困難である。本論文ではカメラ間のスタイル変換を行うことでカメラに依存せず安定して認識できる特徴抽出（camera-invariant descriptor subspace）を行い、人物再同定の問題を高度に解決することを目的とする。この問題に対してCycleGANを適用することでカメラ間の特徴変換を捉えた上で、データ拡張を行う。存在するノイズへの対策として有効と思われる正則化:Label Smooth Regularization (LSR)を適用する。LSRを使用する場合では学習データに対するオーバーフィッティングが見られず、有効な手法であることが判明した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180515CamStyleTransferREID.png" alt="180515CamStyleTransferREID"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>CycleGANによるカメラ間のスタイル変換を実現してデータ拡張、LSRによりノイズへの対応を行いオーバーフィッティングを回避していることが新規性である。また、人物再同定においてその高い精度（Market-1501のrank-1にて89.49%、DukeMTMC-reIDのrank-1にて78.32%）を実現している。さらに、LSRを用いることでベースラインからの精度向上が見られる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>CycleGANが学習データを増やすという意味でも取り上げられている。例えば東大井上氏の論文もCycleGANを用いてデータ拡張を行い、スタイルの異なる画像に変換することでデータアノテーションの労力を削減している。</p><ul><li><a href="https://arxiv.org/abs/1711.10295">論文</a></li><li><a href="https://github.com/zhunzhong07/CamStyle">Project</a></li><li><a href="http://zhunzhong.site/">著者</a></li><li><a href="https://github.com/junyanz/CycleGAN">CycleGAN</a></li><li><a href="https://naoto0804.github.io/cross_domain_detection/">東大井上氏論文</a></li></ul></div></div><div class="slide_index">[#425]</div><div class="timestamp">2018.5.15 11:50:25</div></div></section><section id="ID_Dense_3D_Regression_for_Hand_Pose_Estimation"><div class="paper-abstract"><div class="title">Dense 3D Regression for Hand Pose Estimation</div><div class="info"><div class="authors">Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>単眼距離画像から簡易的かつ効果的に3次元手部姿勢推定を実施する技術について提案する。従来の3D手部姿勢回帰の手法と比較して、本論文ではピクセルごとの（pixel-wise）解析を可能とする。手法としては2D/3Dの関節点を返却するカスケード型の多タスクネットワーク（multi-task network cascades）を提案し、End-to-Endでの学習を行う。その後MeanShiftによりピクセルごとの姿勢位置を推定する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180515DenseHandReg.png" alt="180515DenseHandReg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来のほとんどの手法では関節レベルの手部姿勢推定であったのに対して、本論文で提供する技術はピクセルベースの3D手部姿勢推定であることが新規性である。ピクセルごとの回帰はノンパラメトリックな手法を構築した。MSRA/NYU hand datasetにてすべての従来手法よりも高い精度で手部姿勢推定を実行した。また、ICVL hand datasetでは（頭打ちになっていると思われる）論文5には及ばなかったが、接近した精度を叩き出すことに成功した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>HandTrackingも激戦であるが、本論文ではピクセルベースの回帰とState-of-the-art（最高精度）という強みを活かして論文を通している。</p><ul><li><a href="https://arxiv.org/abs/1711.08996">論文</a></li><li><a href="https://github.com/melonwan/denseReg">GitHub</a></li></ul></div></div><div class="slide_index">[#426]</div><div class="timestamp">2018.5.15 11:20:15</div></div></section><section id="ID_Disentangling_Features_in_3D_Face_Shapes_for_Joint_Face_Reconstruction_and_Recognition"><div class="paper-abstract"><div class="title">Disentangling Features in 3D Face Shapes for Joint Face Reconstruction and Recognition</div><div class="info"><div class="authors">Feng Liu, Ronghang Zhu, Dan Zeng, Qijun Zhao, Xiaoming Liu</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔画像からshapeの三次元復元を行う際に、画像から個人性(顔の形など)を反映した3Dモデルと、個人性以外(表情など)を反映した3Dモデルをencoderで別々に生成しdecoderで三次元復元を行う手法を提案。
生成された顔のshapeは三次元復元におけるstate-of-the-artよりも高い精度を達成し、
また生成されたshapeによる顔認証においても多くの既存手法より高い精度を達成した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Disentangling_Features_in_3D_Face_Shapes_for_Joint_Face_Reconstruction_and_Recognition.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来の三次元復元の手法では顔のディティールは再現するものの、アラインメントなどの個人性の再現が完全ではなかった。提案手法では個人性を反映したモデルとそうでないモデルを分離して学習させることで、この問題を解決した。</li><li>様々なデータセットにおいて、生成された顔の3D shapeはstate-of-the-artに比べて最も低いaccuracyを達成。</li><li>生成された3D shapeにおけるランドマークなどのaccuracyにおいてももっとも低い値を獲得。</li><li>生成された3D shapeによる個人認証においても、多くの既存手法よリも高い精度となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>disentangleのファクターとして個人性を選んだのはあくまで人間であって、今後の発展ではもっと優秀なファクターを深層学習が導き出してくれるかもしれない。</li><li><a href="https://arxiv.org/abs/1803.11366">論文</a></li></ul></div></div><div class="slide_index">[#427]</div></div></section><section id="ID_Seeing_Small_Faces_from_Robus_Anchors_Perspective"><div class="paper-abstract"><div class="title">Seeing Small Faces from Robust Anchor’s Perspective</div><div class="info"><div class="authors">Chenchen Zhu, Ran Tao, Khoa Luu, Marios Savvides</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>アンカーベースで画像中の小さな顔に対する検出精度を向上させる手法を提案。アンカーベースの手法では画像中に等間隔で並べられた点(アンカー)を中心とした矩形によって物体を検出する。
アンカーによる検出精度を評価する数値としてExpected Max Overlapping(EMO) scoreを提案し、
EMOを深層学習に学習させることで、小さな顔(16X16)に対する検出精度を向上した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Seeing_Small_Faces_from_Robus_Anchors_Perspective.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来のアンカーベースの手法ではIoUを学習させていたため、解像度が16x16などの小物体に対する学習が困難であったが、EOM scoreを学習させることで小物体の検出精度が大きく向上。</li><li>従来のアンカーベースの手法よりも検出精度が向上、特に小さな顔に対する検出精度が大きく向上したが、実行時におけるスピードは従来手法と同程度。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.09058">論文</a></li></ul></div></div><div class="slide_index">[#428]</div></div></section><section id="ID_Exploring_Disentangled_Feature_Representation_Beyond_Face_Identification"><div class="paper-abstract"><div class="title">Exploring Disentangled Feature Representation Beyond Face Identification</div><div class="info"><div class="authors">Yu Liu, Fangyin Wei, Jing Shao, Lu Sheng, Junjie Yan, Xiaogang Wang</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔に関するタスクに汎用的な特徴量を得ることができるDistilling and Dispelling Autoencoder(D2AE)を提案。Encoderによって顔から個人性を表現する特徴量(性別など)と個人性を排除した特徴量(表情など)を抽出する。
取得された特徴量により、個人識別、アトリビュートの識別、顔のアトリビュート編集、顔の生成を行うことができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Exploring_Disentangled_Feature_Representation_Beyond_Face_Identification.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Encoderによって顔から個人性を表現する特徴量と個人性を排除した特徴量を抽出することで、これらの特徴量により様々な顔のタスクを行うことが可能となった。</li><li>LFWデータセットにおける個人識別でaccuracyが約99.0%、TPRが約98.0%であり、既存手法と同等の精度を達成。</li><li>LFWA、CelebAデータセットにおける顔のアトリビュート認識は83.16%となり、アトリビュートを学習していないにも関わらず、アトリビュートを学習した既存手法と同等の精度を達成した。</li><li>顔のアトリビュートの編集、アトリビュートを保ったアイデンティティーの転写といった編集が可能。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>このネットワークを用いて他の物質の個人性を抽出して何が出てくるのか興味がある。例えば顔の代わりに魚を学習させて、鯛ごとの個人性、マグロごとの個人性を抜き出してみるなど。</li><li><a href="https://arxiv.org/abs/1804.03487">論文</a></li></ul></div></div><div class="slide_index">[#429]</div></div></section><section id="ID_Robust_Facial_Landmark_Detection_via_a_Fulaly-Convolutional_Local-Global_Context_Network"><div class="paper-abstract"><div class="title">Robust Facial Landmark Detection via a Fully-Convolutional Local-Global Context Network</div><div class="info"><div class="authors">D. Merget, M. Rock and R. Gerhard</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>FCNの中にKernel convolutionを暗黙的に入れ込み，大域的特徴情報を残すというアイデアを提案．Conv層で局所特徴を取り，KernelConvでそれをブラーにかけ，DilatedConv層で大局的特徴をリファインするという構造．</p><p>特に解像度に独立・きっちりROIがとれない・要複数検出対応・要遮蔽対応な顔ランドマーク検出タスクに有効．KernelConvによって勾配平滑化と過学習抑制が働き収束しやすくなる．
アウトライア弾きのために，事前処理ステップにおいて，ネットワーク出力をシンプルなPCAベース2D形状モデルにフィットしておく．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Robust_Facial_Landmark_Detection_via_a_Fulaly-Convolutional_Local-Global_Context_Network_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来は階層構造やプーリング，統計モデルへのフィッティングで対応していたところを，FCNに直に大域的特徴を入れ込むようにした．</li><li>構造単純化により，学習パラメータが少なくなる．</li><li>顔ランドマーク検出に適用してみて，いくつかのSOTAな手法より良い性能を出した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://www.mmk.ei.tum.de/fileadmin/w00bqn/www/Verschiedenes/cvpr2018.pdf">論文</a></li></ul></div></div><div class="slide_index">[#430]</div><div class="timestamp">2018.5.15 13:31:33</div></div></section><section id="ID_Direction-aware_Spatial_Context_Features_for_Shadow_Detection"><div class="paper-abstract"><div class="title">Direction-aware Spatial Context Features for Shadow Detection</div><div class="info"><div class="authors">X.Hu, L.Zhu, C.W.Fu, J.Qin, and P.A.Heng</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1712.04142</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>影の周りには様々な背景があり，セマンティクスを理解しなければならないため，影の検出は基本的のようで困難である．それに対して，方向認識の方法で画像のコンテキストを解析することで影検出手法を提案する．空間のRNN内のコンテキスト特徴が密集している箇所にアテンションを導入することで方向認識の手法を定式化する．97％の検出精度と38％のバランスエラー率の低減を実現．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Direction-aware_Spatial_Context_Features_for_Shadow_Detection.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>空間的なRNNに対してアテンション機構を設計しdirection-aware spatial context (DSC)モジュールを構築することで方向認識の方法で空間的なコンテキストを学習．</li><li>重み付き交差エントロピー損失が影と影でない領域における検出精度のバランスが取れるように設計．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>影の検出だけでなく，顕著性検出およびセマンティックセグメンテーションなどの他のアプリケーションで使用する事もできそう．</p><ul><li><a href="https://arxiv.org/pdf/1712.04142.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#431]</div><div class="timestamp">2018.5.15 02:31:23</div></div></section><section id="ID_Learning_to_Act_Properly_Predicting_and_Explaining_Affordances_from_Images"><div class="paper-abstract"><div class="title">Learning to Act Properly: Predicting and Explaining Affordances from Images</div><div class="info"><div class="authors">Ching-Yao Chuang, Jiaman Li, Antonio Torralba and Sanja Fidler</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Kazuho Kito</div><div class="item1"><div class="text"><h1>概要</h1><p>現実の多様な場面での環境の物体に対するアフォーダンスの推定する研究。ADE20kを基にしたADE-Affordanceというデータセットの提案。このデータセットはリビングなどの屋内から、道路や動物園などの屋外まで幅広いタイプの画像とそのannotationで構成。また、画像中の物体に対してアフォーダンスの推理を行うための，画像からcontextual informationを伝えるGraph Neural Networksの提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Act_Properly.PNG" alt="Learning_to_Act_Properly.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・ある場面の状況下での適切でない行動の理由について身体的や社会的な観点から説明・画像上のある物体に対してだけでなくその場面を全体としてとらえてアフォーダンスの推論を行っている．
・物体間の依存関係をモデル化することでアフォーダンスとその説明を生成</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.07576">論文</a></li><li><a href="http://www.cs.utoronto.ca/~cychuang/learning2act/">Project Page</a></li></ul></div></div><div class="slide_index">[#432]</div><div class="timestamp">2018.5.14 19:28:40</div></div></section><section id="ID_Discriminability_objective_for_training_descriptive_captions"><div class="paper-abstract"><div class="title">Discriminability objective for training descriptive captions</div><div class="info"><div class="authors">R.Luo, B.Price, Scott Cohen and G.Shakhnarovich</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1803.04376</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>現在のキャプショニング方法は，2つの異なる画像であるにも関わらず，同じキャプションを生成してしまうなどの弁別性にかけている．それに対して，学習の際に画像とキャプションの一致度を直接関連付けるLossを組み込むことによって他のキャプションよりも弁別性のあるキャプションを生成している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Discriminability_objective_for_training_descriptive_captions.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>機械翻訳の評価指標であるBLEU，METEOR，ROUGE，CIDErやSPICEにおいても既存のキャプショニング手法よりも高いスコアを示している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>これにより，同じような画像に対するバリエーションが増え，ユニークなイメージキャプショニングの幅が広がった!!</p><ul><li><a href="https://arxiv.org/pdf/1803.04376.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#433]</div><div class="timestamp">2018.5.14 19:39:23</div></div></section><section id="ID_A_Face-to-Face_Neural_Conversation_Model"><div class="paper-abstract"><div class="title">A Face-to-Face Neural Conversation Model</div><div class="info"><div class="authors">Hang Chu, Daiqing Li, Sanja Fidler</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>入力された会話文に対して、その返答と適切な顔のジェスチャーを生成する手法。映画データセットを元にトレーニングデータセットを構築。
RNNに対してディスクリミネータの出力を報酬とした強化学習を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/A_Face-to-Face_Neural_Conversation_Model.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>入力は会話文のみ、あるいは動画。動画が入力の場合には同じテキストでも発話者の表情によって出力される返答文が変化する。</li><li>出力が会話文だけの場合よりも、同時に顔のジェスチャを生成した方が生成された会話文がよりGTの会話文に近くなったことを主張。</li><li>データセットは250種類の映画データセットMovieQAにおいて単一人物が写っているシーンにおいて顔向、ジェスチャカテゴリ、タイムスタンプを取得することで構築した。</li><li>生成された返答文の妥当性を評価するためにamazon mechanical turkを実施。GANを導入したことで返答文の多様性、妥当性がstate-of-the-artの手法に勝った。</li><li>このモデルで学習したボットとリアルタイムで会話することも可能。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>デモを見るとまだ返答文自体には違和感があるが、顔のジェスチャがつくことで会話している気分になる。ボットのモデルが謎のおじさん。</li><li><a href="http://chuhang.github.io/files/publications/CVPR_18_2.pdf">論文</a></li><li><a href="http://www.cs.toronto.edu/face2face">Project page</a></li></ul></div></div><div class="slide_index">[#434]</div></div></section><section id="ID_CosFace_Large_Margin_Cosine_Loss_for_Deep_Face_Recogntion"><div class="paper-abstract"><div class="title">CosFace: Large Margin Cosine Loss for Deep Face Recognition</div><div class="info"><div class="authors">Syed Zulqarnain Gilani, Ajamal Mian</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔認識のための新たなロス関数としてソフトマックス関数をベースとしたLarge Margin Cosine Loss(LMCL)を提案した研究。LMCLはソフトマックス関数の指数部分を重みベクトルWと特徴量ベクトルxの内積においてWとxのノルムを1とし、定数mを引いた関数。
認識タスクでは異なるクラスタ間の距離を遠く、同じクラスタ間の距離を近くする、という基本的な考えがある。
LMCLはこの考えを元に上記のようにL2正則化を施すことで、Wとxのノルムに左右されることなくWとxの角度空間においてクラスタの分離を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/CosFace_Large_Margin_Cosine_Loss_for_Deep_Face_Recogntion.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ソフトマックス関数において重みベクトルの大きさ、入力特徴量のノルムを除外することで、cosの影響を最大限に大きくしWとxの角度空間におけるマージンの最大化を提案。</li><li>face identification(この人はAさんであるか？)、face verification(この人は女性であるか？)の多くのタスクにおいて,ソフトマックス関数由来のロス関数、state-of-the-artの手法よりも良い精度となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>汎用的な認識タスクに使用できそうだが、顔認識に限定したのはデータセットや既存研究との比較のため？</li><li><a href="https://arxiv.org/abs/1801.09414">論文</a></li></ul></div></div><div class="slide_index">[#435]</div></div></section><section id="ID_Sparse_Photometric_3D_Face_Reconstruction_Guided_by_Morphable_Models"><div class="paper-abstract"><div class="title">Sparse Photometric 3D Face Reconstruction Guided by Morphable Models</div><div class="info"><div class="authors">Xuan Cao, Zhang Chen, Anpei Chen, Xin Chen, Cen Wang, jingyi Yu</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>異なる位置の点光源１個によって照らされた５枚の正面顔画像から高品質な３次元顔形状を最適化によって復元する研究。被写体の正面に5つのLED点光源が配置されいている照明環境で撮影を行う。
入力画像に対して3D morphable modelを適用することで簡易的な3次元顔形状を生成し、法線マップ組み合わせることで点光源の位置をピクセル単位で推定する。
またセマンティックセグメンテーションを行うことで体毛が生えいてる領域とそうでない領域に分割し、体毛が生えている領域にはフィルタ処理を行うことでノイズを除去する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Sparse_Photometric_3D_Face_Reconstruction_Guided_by_Morphable_Models.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>顔画像からいきなり光源位置を推定するのではなく、一度morphalbe モデルに生成することで推定精度が大きく向上。</li><li>3Dスキャンなどの大掛かりな装置を必要としない。</li><li>顔の小じわ、毛穴、まつ毛なども再現するほど高品質な3次元顔形状を復元。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>推定された光源位置自体の精度結果を見てみたかった。</li><li>配置する点光源の位置については特に言及がなかったが、配置による影響の比較結果がみてみたかった。</li><li><a href="https://arxiv.org/abs/1711.10870">論文</a></li></ul></div></div><div class="slide_index">[#436]</div></div></section><section id="ID_FSRNet_End-to-End_Learning_Face_Super-Resolution_with_Facial_Priors"><div class="paper-abstract"><div class="title">FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors</div><div class="info"><div class="authors">Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, Jian Yang</div><div class="conference">CVPR 2018 Spotlight</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔の超解像度化を学習させる際にランドマーク、パーツの位置推定を同時に行うネットワーク(FSR Net)を提案した研究。同ネットワークをベースにFSR GANも提案。
また生成された高解像度画像に対する評価尺度として生成画像とGTにおけるランドマークのNRMSE、顔パーツに対するセマンティックセグメンテーション画像(parsing)に対するPSNR、SSIM、MSEを提案。
GANベースの手法では高精細な画像が生成されるがPSNR、SSIMが低くなり、MSEをロスとしたネットワークではPSNR、SSIMは高いがボケた画像になってしまう、というジレンマから上記の評価尺度を導入。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/FSRNet_End-to-End_Learning_Face_Super-Resolution_with_Facial_Priors.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>入力画像は16x16の様々な顔むきの画像、出力は128x128に超解像度化された画像。</li><li>state-of-the-artの手法よりもSSIM、PSNRが高く、また新たな評価尺度として提案したランドマーク、face parsingの位置推定も既存手法よりも高い精度となった。</li><li>新たに提案した評価指標自体の妥当性は、FSR GANとFSR Netを比べた際に、FSR Netの方がボケた画像を生成したにも関わらずSSIM、PSNRが高く、一方でFSR GANの方がランドマーク、face parsingの推定精度が高かったことを根拠に主張している。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>比較画像において既存手法の画像があまりにもボケているため、既存手法のコントリビューションを確かめるという意味でも調査が必要と感じた。</li><li><a href="https://arxiv.org/abs/1711.10703">論文</a></li><li><a href="https://github.com/tyshiwo/FSRNet">GitHub</a></li></ul></div></div><div class="slide_index">[#437]</div></div></section><section id="ID_2D3D_Pose_Estimation_and_Action_Recognition_using_Multitask_Deep_Learning"><div class="paper-abstract"><div class="title">2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning</div><div class="info"><div class="authors">Diogo C. Luvizon, David Picard, Hedi Tabia</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>相互に関連性がある2D/3D姿勢推定+人物行動認識を多タスク学習（Multi-task Learning）により最適化した論文である。それぞれで学習を行ったときよりも高い精度を実現することを明らかにし、複数のデータセットにてState-of-the-artな性能を叩き出した。2Dと3Dの姿勢推定、人物行動の特徴量が相補的に補完し合い特徴学習をより高度にしている？</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180514PoseActionMultiTask.png" alt="180514PoseActionMultiTask"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>姿勢推定（しかも3D姿勢推定も含めて）や人物行動認識を単一の枠組みで解決、さらには多タスク学習により別々に学習したときよりも高い精度でふたつの問題を解決した。さらに複数のベンチマーク（姿勢推定：Human3.6M, MPII/行動認識：PennAction, NTU）にて最高精度も叩き出したことが採択の理由である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>動画シーケンスから姿勢と行動を同時出力する、ありそうでなかった研究である。先にやったもの勝ちだが、高度な最適化を実施し特に最高精度を出すのは難しい。CVPRではState-of-the-artとなるかどうかがひとつの採点基準でもある（が、全てではない）ため、実装力をつけておくに越したことはない。</p><ul><li><a href="https://arxiv.org/abs/1802.09232">論文</a></li><li><a href="https://www.youtube.com/watch?v=MNEZACbFA4Y">Youtube</a></li></ul></div></div><div class="slide_index">[#438]</div><div class="timestamp">2018.5.14 13:04:47</div></div></section><section id="ID_Maximum_Classifier_Discrepancy_for_Unsupervised_Domain_Adaptation"><div class="paper-abstract"><div class="title">Maximum Classifier Discrepancy for Unsupervised Domain Adaptation</div><div class="info"><div class="authors">Kuniaki Saito et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>目的のタスクに特化した２つの分離境界を利用したドメイン適応手法。従来の埋め込み空間においてドメイン間の分布を単に近づける方法に対して、あるタスクと解くための分離境界を考慮して適応を行う。この枠組みでの適応はtargetでの損失の上界を下げる埋め込み空間への写像を求める作業と類似している。さまざまなドメイン適応のベンチマークにおいてSoTA。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Maximum_Classifier_Discrepancy_for_Unsupervised_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>Source(S)で学習を行った二つの識別境界を作成する。その識別器がTarget(T)で異なる判断を行ったサンプル(discrepancy)はSの分布とは乖離している領域であると考えられる。以下のような敵対的な適応を行う。(1) TにおけるDiscrepancyが増加するよう識別境界を学習。(2) Discrepancyが減少するように埋め込み空間を学習。(3)Sでの識別は常にうまくいくよう学習。
識別境界を考慮した適応という新規性、理論的な背景、論文の明快さ、精度としての結果が揃っている。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>アイデアの面白さと同時に論文が非常にわかりやすかった。識別境界はあくまで埋め込み関数を適化するために得たものなので、この枠組みで得られる最終的なもの以外(得られた埋め込み空間上で新たに学習したもの)でもうまくいくのではないかと感じた。</p><ul><li><a href="https://arxiv.org/abs/1712.02560">論文</a></li></ul></div></div><div class="slide_index">[#439]</div></div></section><section id="ID_Generative_Non-Rigid_Shape_Completion_with_Graph_Convolutional_Autoencoders"><div class="paper-abstract"><div class="title">Generative Non-Rigid Shape Completion with Graph Convolutional Autoencoders</div><div class="info"><div class="authors">Or Litany, Alex Bronstein, Michael Bronstein, Ameesh Makadia</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>非剛体的な変形を伴う３Dオブジェクトの形状補完．部分的な形状補完のための学習ベースの手法としてgraph-convolutionを含むVAEを提案した．推論時には，既知の部分的な入力データに合う形状を生成できる変数を潜在空間で探すように最適化する．結果として人体と顔の合成データ，リアルなスキャンデータに対する補完が可能であることを示した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Generative_Non-Rigid_Shape_Completion_with_Graph_Convolutional_Autoencoders_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>従来手法よりも優れている点</h1><ul><li>訓練中に部分的な形状を見る必要なしに，任意スタイルで一部として切り出されたデータを扱えること</li><li>人間以外にも，任意の種類の３Dデータに適用できる手法であること</li><li>形状補完はデータに適合する解が複数ある問題であり，複数のもっともらしい解を生成し，この問題に対応できること</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.00268">arXiv</a></li></ul></div></div><div class="slide_index">[#440]</div><div class="timestamp">2018.5.13 16:22:39</div></div></section><section id="ID_Eye_In-Painting_with_Exemplar_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">Eye In-Painting with Exemplar Generative Adversarial Networks</div><div class="info"><div class="authors">Brian Dolhansky, Cristian Canton Ferrer</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要，新規性</h1><p>eye-Inpaintingを行う手法．顔のようなそれぞれ固有の特徴を持つ画像においてのInpaintingで，従来のDNNによる手法は新しい顔を生成するなどidentityを保たなかった．exemplar informationを利用するconditional GAN（ExGANs）を提案．参照画像やperceptual codeというidentifying information（exemplar information）をGANの複数の箇所で利用することで，perceptualに優れ，identityを反映した結果を生成することができた．identifying informationをGANの複数の箇所で利用することが新しい．さらに，将来の比較のためにEye-Inpaintingのタスクの新しいベンチマークとデータセットを用意した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Eye_In-Painting_with_Exemplar_Generative_Adversarial_Networks_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>手法概要</h1><p>cGANの一種．参照画像のIdentityを符号化するネットワークと，Generator，Discriminatorから成る．identifying informationを生成に利用するだけでなく，DiscriminatorやPerceptual lossの算出にも利用している．参照画像をベースにした場合と符号をベースにした場合にアプローチを分けている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.03999">arXiv</a></li></ul></div></div><div class="slide_index">[#441]</div><div class="timestamp">2018.5.13 16:12:11</div></div></section><section id="ID_Logo_Synthesis_and_Manipulation_with_Clustered_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">Logo Synthesis and Manipulation with Clustered Generative Adversarial Networks</div><div class="info"><div class="authors">Alexander Sage, Eirikur Agustsson, Radu Timofte, Luc Van Gool</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>特徴ベクトルのクラスタリングでGANの入力ベクトルを作成する学習方法で，ロゴの生成と操作が可能とした．ロゴのデータは高マルチモーダルのデータであり，従来のSoTAではmode collapseを起こしてしまうが，提案する学習方法では多様なロゴを生成する．iWGANをCIFER-10で学習するとき，提案する学習方法によって，Inception scoreでSoTA達成．Contribution:</p><ul><li>600k以上のロゴを収集してデータセットを構築</li><li>マルチモーダルなロゴデータでのGANの学習方法</li><li>潜在空間の探索によって，インタラクティブなロゴ生成</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Logo_Synthesis_and_Manipulation_with_Clustered_Generative_Adversarial_Networks_fig.png" alt="Image"></p><p>上段はデータセットから．下段が生成結果．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>Clustered GAN Trainingと読んでいる．GANのネットワークは，DCGANとimproved Wasserstein GAN with gradi- ent penalty (iWGAN)を利用．オートエンコーダーの中間特徴ベクトルもしくは，Resnetの特徴ベクトルをクラスタリングして，Generatorの入力ベクトルとする．このクラスタリングでセマンティックに意味のあるクラスタを形成し，GANの学習を向上させることが可能．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://data.vision.ee.ethz.ch/sagea/lld/">データセット</a></li><li>ロゴ・ジェネレーター・インターフェースも用意されている．スライダーを動かして，生成結果を操作できる</li><li><a href="https://arxiv.org/abs/1712.04407">arXiv</a></li></ul></div></div><div class="slide_index">[#442]</div><div class="timestamp">2018.5.13 16:03:23</div></div></section><section id="ID_Multi-Agent_Diverse_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">Multi-Agent Diverse Generative Adversarial Networks</div><div class="info"><div class="authors">Arnab Ghosh, Viveka Kulharia, et al.</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>多様で意味のあるサンプルを生成可能な，複数のGeneratorと１つのDiscriminatorから成るGAN(MAD-GAN)を提案．一つのGeneratorが一つの構成要素を担当する混合モデルとしてはたらく．いくつかの従来のGAN手法と比較実験を行い，MAD-GANは多様なモードを獲得できることを確認．さらに，理論的な分析も行っている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Multi-Agent_Diverse_Generative_Adversarial_Networks_fig.png" alt="Image">それぞれの行が異なるGeneratorによって生成した結果．行はそのGeneratorにランダムなノイズzを入力して生成した結果．マルチビューなデータセットから異なるモードを異なるGeneratorが学習していることを確認できる．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>Multi-agent GAN．複数のGeneratorと１つのDiscriminatorで構成．</li><li>Generator同士は，最終層以外は重みを共有している．</li><li>複数のGeneratorの生成サンプルと真のサンプルをDに入力し，Discriminatorは，FakeとRealの判別だけではなくて，そのFakeの生成サンプルを与えるGeneratorがどれであるかも予測する．これによって，複数のモードがある時，個別のモードに対してそれぞれのGeneratorを振り分けるようにDiscriminatorが学習する．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>image-to-image変換,multi-view生成， face generationなど多数の実験を行っている．</li><li>展望は，MAD-GANでは複数のGeneratorを使うことになるが，いくつのGeneratorが必要なのかを推定できるようにすること．</li><li><a href="https://arxiv.org/abs/1704.02906">arXiv</a></li></ul></div></div><div class="slide_index">[#443]</div><div class="timestamp">2018.5.13 15:50:21</div></div></section><section id="ID_SketchyGAN_Towards_Diverse_and_Realistic_Sketch_to_Image_Synthesis"><div class="paper-abstract"><div class="title">SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis</div><div class="info"><div class="authors">Wengling Chen, James Hays</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>スケッチから写真を生成する手法の提案．50のカテゴリの写真を生成することができる．スケッチに対して，自動でデータ拡張をする方法を示し，その拡張方法がタスクに有効であることを示す．さらに追加の目的関数と新しいネットワーク構造も提案．マルチスケールの入力画像を入れることで情報の流れを向上させている．結果はまだphotorealisticとは言えないが，従来手法よりリアルでinception scoreの高い結果を得た．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SketchyGAN_Towards_Diverse_and_Realistic_Sketch_to_Image_Synthesis_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>データ拡張の方法として，エッジ検出などのいくつかの処理を組み合わせている．</li><li>ネットワーク構造はU-net構造だが，各ブロックで入力画像で条件付けを行うのが特徴．以前の層で抽出された特徴マップと比べ新しい特徴量を入力画像から選択的に抽出するための内部マスクを学習するため，Masked Residual Unitというブロックモジュールを導入した．（DCGAN, CRN, ResNetとの比較がある）</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>GeneratorにもDiscriminatorにも途中で画像やラベルの情報をinjectionする方法が増えている印象．</li><li>sketchから似ている写真を検索してくるという方法がこれまでよく研究されていた．今回は，スケッチから新しく写真を生成する（質はまだ低い）</li><li><a href="https://arxiv.org/abs/1801.02753">arXiv</a></li></ul></div></div><div class="slide_index">[#444]</div><div class="timestamp">2018.5.13 15:37:35</div></div></section><section id="ID_ScanComplete_Large-Scale_Scene_Completion_and_Semantic_Segmentation_for_3D_Scans"><div class="paper-abstract"><div class="title">ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans</div><div class="info"><div class="authors">Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed, Juergen Sturm, Matthias Nießner</div><div class="conference">CVPR 2018</div><div class="paper_id">584</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>部分的なシーンの3Dデータからシーンの幾何及びボクセルごとのセマンティック情報をコンプリートする手法ScanCompleteを提案した．</li><li>従来，シーンの3次元情報を完全に収集するのが非常に困難，シーンの３次元のデータの膨大さや形状情報のバリエーションの多さは従来のシーン補完に対して困難な問題設定である．そういったため，シーンのコンプリートでは出力の質が低いという問題点がある(contentsとして応用するレベルではない)．こういった困難を解決するため，提案手法は①trainとtestデータの入力解像度を異なる値に設定し， testの場合シーンのサイズの変化を対応できるようにする．②coarse-to-fineなfully convolution 3DCNNを用いて，グローバルなシーンの構造特徴および精密な局所的補間をできるようにする．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/scancomplete.png" alt="scancomplete"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>異なる入力シーンのサイズを自由に対応できる（最大70×60×3m くらいまでできる）</li><li>従来の手法：3D-EPN,SSCNetなどの従来手法と比べ，scene completion, semantic labeling両方精度がSOTA</li><li>出力結果が3D Contentsとして応用できるレベル</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.10215">論文</a></li></ul></div></div><div class="slide_index">[#445]</div><div class="timestamp">2018.5.14 14:43:15</div></div></section><section id="ID_Learning_from_Millions_of_3D_Scans_for_Large-scale_3D_Face_Recognition"><div class="paper-abstract"><div class="title">Learning from Millions of 3D Scans for Large-scale 3D Face Recognition</div><div class="info"><div class="authors">Syed Zulqarnain Gilani, Ajamal Mian</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>大規模3D顔データセットを構築し、そのデータによってトレーニングされたCNNが高い3D顔認識精度を持つことを示した論文。従来の3D顔データセットはデータ数が少なく、最も多いND-2006でも888アイデンティティー・13540種類のみであったが、本論文で構築されたトレーニング用データセットはおよそ10万アイデンティティー・310万種類。
このトレーニングデータを用いてCNNを学習させることで、認識精度は98.74%となりstate-of-the-artよりも優っていることを確認した。
また既存の3D顔データセットをマージすることで、1853アイデンティティー・31K種類のテスト用3D顔データセットを構築した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180513Learning_from_Millions_of_3D_Scans-scale_3D_Face_Recognition.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>トレーニング用の3D顔データは1000人の3Dスキャンデータに対して、変形に要するエネルギーがもっとまた商用ソフトを使用すること300種類の顔のうち顔の形状・表情が似ている顔を合成して生成。も高くなる顔のペアを合成して生成。また商用ソフトを使用すること300種類の顔のうち顔の形状・表情が似ている顔を合成して生成。
前者は別の顔を識別するため、後者は似た顔を識別する目的で用意されたデータである。
生成された顔に対して水平方向、垂直方向から15度ずつ撮影することで、計100,005アイデンティティー・3,169,275種類の3D顔データを生成。</li><li>既存の3D顔認識・2D顔認識手法に対してオープン・クローズドテスト両方における精度を比較したところ、提案モデルがもっとも良い精度となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.05942">論文</a></li></ul></div></div><div class="slide_index">[#446]</div></div></section><section id="ID_Learning_to_Generate_Time-Lapse_Videos_Using_Multi-Stage_Dynamic_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks</div><div class="info"><div class="authors">Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, Jiebo Luo</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>高解像(128x128)のリアルタイムなタイムラプス動画の生成をするGANを提案．最初のフレームを与えると，近未来のフレームを生成する．新規性としては，</p><ul><li>タイムラプスデータセットを作成</li><li>タイムラプス向きの近未来予測ネットワークを提案（Multi-stage Dynamic Generative Adversarial Network (MD-GAN) ）</li><li>モーションのモデリングにGram matrixを導入し，実世界ビデオのモーションを模倣するためのadversarial ranking lossを提案</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Generate_Time-Lapse_Videos_Using_Multi-Stage_Dynamic_Generative_Adversarial_Networks_fig.png" alt="fig"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>corse-to-fineの２ステージアプローチのGAN．ステージを分けた狙いとしては，１ステージ目でコンテンツの生成を行い，２ステージ目でモーションのモデリングを行うこと．１ステージ目のU-net風のネットワークでは3D convolutions と deconvolutions を含んでいる．</p><p>２ステージ目のDiscriminatorとして，モーションパターンをモデル化するためにGram matrix使って，adversarial ranking lossを算出する．1ステージの出力ビデオ，2ステージ目の出力ビデオ，真のビデオからランキングをとる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1709.07592">arXiv</a></li></ul><p>タイムラプス用のGANが初めて提案されたことが評価されたのかなという印象．定量的な評価はメインがPreference Opinion Scoreで, 他はMSE, PSNR and SSIM．</p></div></div><div class="slide_index">[#447]</div><div class="timestamp">2018.5.13 12:45:36</div></div></section><section id="ID_Hyperparameter_Optimization_for_Tracking_with_Continuous_Deep_Q-Learning"><div class="paper-abstract"><div class="title">Hyperparameter Optimization for Tracking with Continuous Deep Q-Learning</div><div class="info"><div class="authors">Xingping Dong et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Object Tracking 手法において用いられる複数の Hyperparameter を強化学習によって各シークエンス毎に最適化する手法を提案. Hyperparameter の選択を Action, Tracking の精度の良さを Reward として, Normalized Advantage Functions (NAF) を用いた強化学習を行なっている. また, Heuristic を導入することで, 学習の遅さの問題を緩和した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Hyperparameter-Optimization-for-Tracking-with-Continuous-Deep-Q-Learning.png" alt="fukuhara-Hyperparameter-Optimization-for-Tracking-with-Continuous-Deep-Q-Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Object Tracking における Hyperparameter の最適化問題を強化学習の問題として定式化した.</li><li>上記の問題を既存の強化学習手法である NAF　(連続な行動が取れるように拡張された Q 学習の手法) を用いて解いた.</li><li>強化学習を適用した際に, 状態空間の次元の多さなどに由来する学習速度の遅さを huristic を導入することで緩和した.</li><li>OTB-2013 や VOT-2015 などのデータセットを用いて既存研究(Siam-py等)と比較. 同程度の速度で, 正確性とロバスト性の両方に置いて既存手法を上回った. </li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://www.porikli.com/mysite/pdfs/porikli%202018%20-%20Hyperparameter%20optimization%20for%20tracking%20with%20continuous%20deep%20Q-learning.pdf" target="blank">[論文] Hyperparameter Optimization for Tracking with Continuous Deep Q-Learning</a></li><li><a href="https://arxiv.org/abs/1603.00748" target="blank">[関連論文: NAF] Continuous Deep Q-Learning with Model-based Acceleration</a></li></ul></div></div><div class="slide_index">[#448]</div><div class="timestamp">2018.5.12 13:05:55</div></div></section><section id="ID_Tangent_Convolutions_for_Dense_Prediction_in_3D"><div class="paper-abstract"><div class="title">Tangent Convolutions for Dense Prediction in 3D</div><div class="info"><div class="authors">Maxim Tatarchenko et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>３次元データを扱う新しい convolutional の方法 "Tangent Convolution" を提案. 全ての点の近傍点を仮想的な接平面上に射影し, 接平面上で畳み込みを行う. 接平面は法線ベクトルが計算できれば構成する事ができるため, 複数のデータ形式に対して同様に適用が可能. また, 事前計算を行う事によって大規模なデータベースに対しても効率的に計算を行う事が可能となった.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuahra-Tangent-Convolutions-for-Dense-Prediction-in-3D.png" alt="fukuahra-Tangent-Convolutions-for-Dense-Prediction-in-3D.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>入力データの形式は法線ベクトルを近似的に求められるもの (point clouds, meshes, dpolygon soup) であればなんでも良い.</li><li>事前計算を行う事によって大規模なデータ（数百万オーダーの点群）も効率的に扱う事ができる.</li><li>提案手法の有効性を示すために Tangent Convolution を用いたネットワークを Semantic 3D Scene Segmentation のタスクに置いて既存手法 (PointNet, ScanNet, OctNet) と比較し, 複数の評価尺度に置いて最も良い精度となった.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://vladlen.info/papers/tangent-convolutions.pdf">[論文]</a></li></ul></div></div><div class="slide_index">[#449]</div><div class="timestamp">2018.5.12 11:33:55</div></div></section><section id="ID_Im2Pano3D_Extrapolating_360_Structure_and_Semantics_Beyond_the_Field_of_View"><div class="paper-abstract"><div class="title">Im2Pano3D: Extrapolating 360 Structure and Semantics Beyond the Field of View</div><div class="info"><div class="authors">Shuran Song, Andy Zeng, Angel Chang, Manolis Savva, Silvio Savarese, Thomas Funkhouser</div><div class="conference">CVPR 2018</div><div class="paper_id">466</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・部分的に観測されたシーン(RGB-D)から，full sceneの構造及びセマンティックラベルを推定する新規な問題設定”semantic-structure view extrapolation”及びフレームワークを提案した．</p><p>・従来のview extrapolationは画像のboundryの色情報しか行わず，シーンのセマンティック構造に対してextrapolationを行う研究がない．そこで，この論文で，著者達がsemantic-structure view extrapolationを提案し，50%以下のシーンの観測データから構造及びセマンティックをextrapolation予測する．</p><p>・提案フレームワークは：①一枚のマルチチャンネルpanorama画像でシーンの情報(RGB，構造，セマンティック)を表示する；②3次元構造をデプスのような詳細な三次元情報を用いずに，3次元平面方程式で表示する．③マルチロス関数(ピクセルレベル，グローバルコンテキスト)を用いる．</p><p>・提案フレームワークの考え方は入力と出力を一枚のマルチチャンネルpanorama画像として表示し，encoder-decoderにより，欠損した入力からfullなpanorama画像を出力する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Im2Pano3D.png" alt="Im2Pano3D"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・CG データセットSUNCG及びリアルシーンデータセットMatterport3Dを用いて従来手法よりシーンの構造及びセマンティックの予測が優位．</p><p>・一枚のマルチチャンネルpanorama画像でシーンの情報を表示し，シーンの情報を固定なサイズにできるので，2次元畳み込みを用いられる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・マルチチャンネルpanorama画像でシーンの情報を保存するところが賢い</p><p>・提案フレームワークは構造的に理解しやすい，実装してみたい</p><ul><li><a href="https://arxiv.org/abs/1712.04569">論文</a></li><li><a href="http://im2pano3d.cs.princeton.edu/">ポロジェクト</a></li></ul></div></div><div class="slide_index">[#450]</div><div class="timestamp">2018.5.11 17:40:13</div></div></section><section id="ID_Deep_image_prior"><div class="paper-abstract"><div class="title">Deep Image Prior </div><div class="info"><div class="authors">Dmitry Ulyanov et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>「CNNは理論上任意の関数を近似できるが、その構造自体に汎化性能をあげるようなPriorが含まれている」という考えのもと、ランダム初期化されたCNNを用いて高いレベルの画像復元、ノイズ除去などを行った。また、CNNのPrior をさらに裏付けるものとして、自然画像を復元するより、ノイズ画像を復元する学習の方がiteration数がかかることも示された。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_image_prior.png"></p></div></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>ノイズ画像ｚをencoder-decoderモデルに入力して、生成された画像を欠損画像にMSEで近づけるように学習するだけである。注意点として、完全に学習仕切ってしまうと欠損画像と同じものが出るだけなので、学習をある程度のiterationで止めると、復元されたような画像が得られる。また、CNNのPrior をさらに裏付けるものとして、自然画像を復元するより、ノイズ画像を復元する学習の方がiteration数がかかることも示された。着眼点や面白い実験方法に加え結果も伴っている研究</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>畳み込み処理×SGDの異常なまでの汎化性能を実験的に裏付けていると思われ非常に面白い。逆にCNNのPriorの苦手なところとして、Adversarial exampleやGANのチェッカーボード現象も関係してそう。畳み込み処理の派生(Deformable convなど)でのpriorの検証も気になる。</p><ul><li><a href="https://arxiv.org/abs/1711.10925">論文</a></li></ul></div></div><div class="slide_index">[#451]</div></div></section><section id="ID_Edit_Probability_for_Scene_Text_Recognition"><div class="paper-abstract"><div class="title">Edit Probability for Scene Text Recognition</div><div class="info"><div class="authors">F. Bai, Z. Cheng, Y. Niu, S. Pu and S. Zhou</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>OCRのstate-of-the-artな手法として，encoder-decoderで文字カテゴリごとのAttentionを取ってからテキスト認識をするvisual attentionベーステキスト認識があるが，
ある文字がよく見えなかったり1文字でも複数ピークが出てしまったりする問題はある．
GTとの差を取るとして，エンコード後の文字列で比較する<a href="https://ja.wikipedia.org/wiki/%E3%83%AC%E3%83%BC%E3%83%99%E3%83%B3%E3%82%B7%E3%83%A5%E3%82%BF%E3%82%A4%E3%83%B3%E8%B7%9D%E9%9B%A2">編集距離</a>を取ることが考えらえるが，
本稿ではVAで出る尤度分布で比較する，編集確率（Edit Probablity）を提案する．
これにより，字抜けや余分な字を拾ってしまうような誤認識に強い文字認識を実現可能．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Edit_Probability_for_Scene_Text_Recognition_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Attentionベーステキスト認識においてstate-of-the-artな性能．</li><li>まさに正統進化といえる．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>正統進化を，他のラボが，1年未満に行ってしまっているあたり，CV分野の流れの早さがうかがえる．</p><ul><li><a href="https://arxiv.org/abs/1805.03384">arXiv</a></li><li><a href="https://arxiv.org/abs/1706.01487">Visual attention models for scene text recognition</a>（<strong>ICDAR2017</strong>）</li></ul></div></div><div class="slide_index">[#452]</div><div class="timestamp">2018.5.10 18:29:27</div></div></section><section id="ID_iVQA_Inverse_Visual_Question_Answering"><div class="paper-abstract"><div class="title">iVQA: Inverse Visual Question Answering</div><div class="info"><div class="authors">Feng Liu, Tao Xiang, Timothy Hospedales, Wankou Yang, Changyin Sun</div><div class="conference">CVPR 2018</div><div class="paper_id">1199</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・VQA問題の逆問題iVQA設定及びモデルを提案し (画像及び回答文から，質問文を生成する)，更に iVQAもVQAと同じく“視覚-言語”の理解のベンチマック問題設定になれると指摘した．</p><p>・iVQAタスクに用いられるmulti-modal dynamic inferenceなフレームワークを提案した．提案フレームワークは回答文を生成する段階で，“回答文”，“生成した部分的な質問文”によって導かれ動的に画像attentionを調整できる．</p><p>・更に，回答文の従来の自然言語的評価に， ランキングベースなiVQAタスクの回答文を評価できる指標を提案した．その指標により，などの面を評価できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/iVQA.png" alt="iVQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・近年，従来のVQAの成功がデータセットバイアス及び質問文からの情報理解，画像の内容に対する理解がまだVQAにおいて深く利用されていないことが指摘された．そこで，画像と回答文から質問文を予測する問題設定iVQAを提案した， iVQAタスクにおいてはVQAと比べ，①画像内容の理解の要求が高い，②また回答文が常に短いので，質問文と比べよりスパースな情報抽出しかできないため，回答文に頼りすぎることにならない．③モデルの推定及びreasoning能力が更に必要である．</p><p>・提案フレームワークの各パーツ(dynamic attention, multi-modal inferenceなど)の有効性に関してAblation　studyを詳しく行った. 説得力がある．</p><p>・Dynamic attention mapsの可視化分析により問題文を生成する段階で，動的に関連する画像領域にattentionすることを指摘した．</p><p>・実験を通して，iVQAをVQAとヒュージョンしたら， VQAの精度を挙げられることを証明した．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・VQAの問題点を深く理解した上での新規問題設定．</p><p>・Dynamic attention mapsの可視化分析により問題文を生成する段階で，動的に関連する画像領域にattentionすることを指摘した．</p><p>・新奇な考え方・詳しい分析実験・論文の理解しやすさなどが非常に良い</p><ul><li><a href="https://arxiv.org/abs/1710.03370">論文</a></li></ul></div></div><div class="slide_index">[#453]</div><div class="timestamp">2018.5.10 15:08:46</div></div></section><section id="ID_Sketch-a-Classifier_Sketch-based_Photo_Classifier_Generation"><div class="paper-abstract"><div class="title">Sketch-a-Classifier: Sketch-based Photo Classifier Generation</div><div class="info"><div class="authors">C. Hu, D. Li, Y. Song, T. and T.M. Hospedales</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>手書き画像から，書いたものの判別をする画像分類器を出力するメタ学習の提案．学習していない手書きカテゴリでも，そのカテゴリの画像分類器が出力される．3つの枠組みが作れる．
(1)スケッチ画像カテゴリ分類モデルを入力
(2)スケッチ画像を入力
(3)コースなリアル画像分類モデル＋スケッチ画像を入力</p><p>枠組みとしては，<a href="https://www.robots.ox.ac.uk/~vgg/rg/papers/eccv2016_learntolearn.pdf">Model Regression Network</a>による．論文では，SVMパラメータの学習を行っている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Sketch-a-Classifier_Sketch-based_Photo_Classifier_Generation_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>多様性がある．作ったモデルの性質がよく把握されている</li><li>知識転用の新しい形が見える</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.01837">arXiv</a></li><li><a href="https://chenhsuanlin.bitbucket.io/spatial-transformer-GAN/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#454]</div><div class="timestamp">2018.5.10 13:49:15</div></div></section><section id="ID_ST-GAN_Spatial_Transformer_Generative_Adversarial_Networks_for_Image_Compositing"><div class="paper-abstract"><div class="title">ST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing</div><div class="info"><div class="authors">C. Lin, E. Yumer, O. Wang, E. Shechtman and S. Lucey</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>画像合成の際に，背景に対して位置やサイズ感などが正しくなるように幾何的変換を求め，修正を加えてくれるGANを提案．たとえば，家具が適切な場所に置かれたり，メガネが適切に掛けられたりする．</p><p>構造的には複数のSpatial Transformer Networkをジェネレータとして組み込んでいることが特徴．複数のSTNにおける，反復<a href="https://en.wikipedia.org/wiki/Image_warping">画像ワーピング</a>（画像変形方法の一つ）と逐次学習を導入している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/ST-GAN_Spatial_Transformer_Generative_Adversarial_Networks_for_Image_Compositing_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>画像変換が得られるので，間接的に高解像度画像に適用可能</li><li>ナイーブな単ジェネレータよりも高性能．</li><li>大きな差には弱い．奇抜なデザインのものや，大きな移動</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.01837">arXiv</a></li><li><a href="https://chenhsuanlin.bitbucket.io/spatial-transformer-GAN/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#455]</div><div class="timestamp">2018.5.10 12:27:25</div></div></section><section id="ID_Two_can_play_this_Game_Visual_Dialog_with_Discriminative_Visual_Question_Generation_and_Visual_Question_Answering"><div class="paper-abstract"><div class="title">Two can play this Game: Visual Dialog with Discriminative Visual Question Generation and Visual Question Answering</div><div class="info"><div class="authors">Unnat Jain, Lana Lazebnik, Alex Schwing</div><div class="conference">CVPR 2018</div><div class="paper_id">705</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・Visual Dialogタスクに用いられる質問の回答文と質問文を両方予測できるネットワークを提案した．</p><p>・提案フレームワークは100個の回答文(質問文)から正解を予測する(discriminative).  提案フレームワークは質問文，画像，キャプション，QA履歴，選択などの情報をsimilarity+Fusionネットにより100次元のベクトルを生成し，正解ラベルとのcross-entropy誤差を求める．</p><p>・また，従来Visual Dialogの質問文を評価する指標がない，著者達が質問文を評価できる“VisDial-Q evaluation protocol”を提案した．提案protocolは質問文を100個に固定し，予測した質問文がどれくらい通常の人により提出される可能性が高いかにより評価を行っている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/VisualDialog_DVQG_DVQA.png" alt="VisualDialog_DVQG_DVQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・同じネットワークで質問文と回答文を両方予測できる．</p><p>・質問文を評価できる指標の提案．</p><p>・Discriminative VQAタスクにおいて， VisDial評価指標は従来手法(HRE, MN, HCIAE-D-NP-ATT)より良い性能を達成した．</p><p>・VQGタスクにおいて，提案した評価指標“VisDial-Q evaluation protocol”により55.17% recall@5 と 9.32 mean rankを達成した．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.11186">論文</a></li></ul></div></div><div class="slide_index">[#456]</div><div class="timestamp">2018.5.10 04:08:59</div></div></section><section id="ID_Social_GAN_Socially_Acceptable_Trajectories_with_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks</div><div class="info"><div class="authors">Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese and Alexandre Alahi</div><div class="conference">CVPR2018</div><div class="paper_id">234</div></div><div class="slide_editor">Ryosuke Araki</div><div class="item1"><div class="text"><h1>概要</h1><p>人や自律移動プラットフォームが，移動している人を避けるにはいくつかの経路が考えられる．本手法は，人間の経路予測にシーケンス予測とGANを組み合わせたツールを用いて，複数の経路予測を行う．Recurrent sequence-to-sequence modelは，複数の人の間で情報を集約するための新しいプーリング手法を用いて，観測者の行動を予測する．そして，GANを用いてもっともらしい行動をいくつか予測する．予測された経路はDiscriminatorへ入力され，Fake/Real判別をしGANを訓練していく．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180509_SocialGAN1.jpg" alt="20180509_SocialGAN1.jpg"><img src="slides/figs/20180509_SocialGAN2.jpg" alt="20180509_SocialGAN2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Generatorでは，複数の人が同時にどう動くか予測するために，Encoderの各LSTMの出力をまとめるプーリングモジュールを導入した．Discriminatorは，経路そのものがFake（人として社会的にあり得ない行動）またはReal（あり得る行動）を判断する．ETHやHOTELなどのデータセットを用いて評価実験を行った．12ステップ後のAverage Displacement Error（全ての時間での真値と予測値の誤差）は0.58（Social LSTM: 0.72），Final Displacement Error（最終目的とでの真値と予測値の誤差）1.18（Social LSTM: 1.54）となった．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>GANを使う手法は多く出てきているが，これは面白い応用方法だと思った．Discriminatorをどうやって学習していくかが肝になりそう．</p><ul><li><a href="https://arxiv.org/abs/1803.10892">arXiv</a></li></ul></div></div><div class="slide_index">[#457]</div><div class="timestamp">2018.5.9 01:45:09</div></div></section><section id="ID_Neural_Baby_Talk"><div class="paper-abstract"><div class="title">Neural Baby Talk</div><div class="info"><div class="authors">Jiasen Lu, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Takumu Ikeya</div><div class="item1"><div class="text"><h1>概要</h1><p>画像内で検出した物体から文章を生成するイメージキャプショニングタスクを行うための新たなフレームワークの構築を行った．単語が格納されるスロットを文章内に生成し，生成したスロットを満たすように検出した物体を当てはめていくことでキャプションを行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180508NBT.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>検出された物体の名称が入るスロットを最初に生成し，生成したスロットを満たしていくことでキャプションを行う手法が新しい．</p><p>イメージキャプショニングタスクにおいてFlickr30KとCOCOデータセットでSOTAを達成した.</p></div></div><div class="item4"><div class="text"><p></p><h1>コメント・リンク集<li><a href="https://arxiv.org/pdf/1803.09845.pdf">論文</a></li><li><a href="https://github.com/jiasenlu/NeuralBabyTalk">github</a></li></h1></div></div><div class="slide_index">[#458]</div></div></section><section id="ID_Attentive_Generative_Adversarial_Network_for_Raindrop_Removal_from_a_Single_Image"><div class="paper-abstract"><div class="title">Attentive Generative Adversarial Network for Raindrop Removal from a Single Image</div><div class="info"><div class="authors">Rui Qian, Robby T. Tan, Wenhan Yang, Jiajun Su, and Jiaying Liu</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>写真から雨粒を除去する手法の提案</li><li>このタスクが難しいのは，<ol><li>どの領域が，雨粒によって隠されているか不明なこと</li><li>雨粒に隠された背景側の情報がないこと</li></ol></li><li>GAN，LSTMを利用</li><li>Generatorは，Attentive-Reccurent networkとContextual Autoencoderから構成</li><li>はじめにAttentive-Reccurent networkでattention mapを生成　次にContextual Autoencoderで，mapと入力画像から雨粒除去後の画像を生成
　attention mapは，Discriminatorの中間出力とMSE lossを取る際にも利用</li><li>visual attentionという情報によって，<ol><li>Generatorでは雨粒の領域と，周辺の構造にアテンションをより向けることができる</li><li>Discriminatorは復元した領域をより局所的に評価を行える</li></ol></li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Attentive_Generative_Adversarial_Network_for_Raindrop_Removal_from_a_Single_Image_fig.png" alt="Item2Image"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><ul><li>GeneratorとDiscriminatorの両方でvisual attentionを利用するようにしたこと</li><li>自作の1119枚の雨粒ありと無しのペア画像を用意し学習に利用</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.10098">arxiv</a></li></ul></div></div><div class="slide_index">[#459]</div><div class="timestamp">2018.5.8 21:05:34</div></div></section><section id="ID_Deformable_GANs_for_Pose-based_Human_Image_Generation"><div class="paper-abstract"><div class="title">Deformable GANs for Pose-based Human Image Generation</div><div class="info"><div class="authors">Aliaksandr Siarohin, Enver Sangineto, Ste ́phane Lathuilie`re, and Nicu Sebe</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>与えられたポーズ情報を条件として人物画像を生成するタスクを扱う．任意ポーズへの変形タスクで発生する，（服などの）変換前のピクセルと変換後のピクセルの対応が不整列である問題に対応するために，deformable skip connectionを対案する．
従来手法と比べ，条件画像の服の色・テクスチャを保存して別ポーズの画像を生成できている．
人物画像の生成に限らず，キーポイントを与えることのできる不整列のオブジェクトであれば，この手法が適用できると著者らは考えている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deformable_GANs_for_Pose-based_Human_Image_Generation_fig2.png" alt="fig2"><img src="slides/figs/Deformable_GANs_for_Pose-based_Human_Image_Generation_fig3.png" alt="fig3"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>U-net likeのEncoder-Decoder, GANdeformable skip connectionについて．
変換前後の両方のポーズ情報が既知なので，キーポイント周辺のピクセルが変換前から変換後にどこへ移動するか知ることができる．したがって，キーポイントの座標からアフィン変換を求め，畳み込みから得た特徴マップをアフィン変換することで，服の色やテクスチャを変換前から変換後の画像に移して生成できる．
Encoderの特徴量をアフィン変換し，Decoderの特徴量にskipするのがdeformable skip connectionである．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1801.00055">arXiv</a></li><li><a href="https://github.com/AliaksandrSiarohin/pose-gan">プログラム</a></li></ul></div></div><div class="slide_index">[#460]</div><div class="timestamp">2018.5.8 15:39:41</div></div></section><section id="ID_VizWiz_Grand_Challenge_Answering_Visual_Questions_from_Blind_People"><div class="paper-abstract"><div class="title">VizWiz Grand Challenge: Answering Visual Questions from Blind People</div><div class="info"><div class="authors">Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo</div><div class="conference">CVPR 2018</div><div class="paper_id">491</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・盲人に集められたVQAタスクのデータセットVizWiz（画像と音声質問文）を提案した．VizWizが31,000枚の盲人が携帯により撮影し，画像ごとに画像を撮影した盲人が提出した音声質問文一つ付き．質問文ごとに，10個の回答文がアノテーションされている．</p><p>・従来のVQAデータセットほぼ人工設定により作成された方が多く，また現実環境の盲人ユーザを対象に“goal oriented”なVQAデータセット未だにない．そこで，盲人がカメラにより周囲環境を撮影し，環境を理解することを目的にして，盲人ユーザにより集められた画像及び質問文のデータセットを構築した．</p><p>・ 盲人ユーザにより撮影されたのでVizWizは画像の質が良くなく，又質問文が音声情報なので，はっきり発音が取れない場合などの問題点がある．提案データセットで現状のVQAモデルで検証した結果，性能が従来のデータセットで検証した性能より劣るので， VizWizが将来的の盲人のためのVQA応用に新たな挑戦を提出した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/VizWiz.png" alt="VizWiz"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・初めての盲人により撮影及び質問したVQAデータセット．</p><p>・従来のVQAデータセットと比べ，もっと画像の周りの環境に関する質問文が多い．</p><p>・従来のVQAデータセットとの質問文の詳細的な特徴比べも行っている．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・盲人のためのVQAシステム構築に有力なデータセット．</p><ul><li><a href="https://arxiv.org/pdf/1802.08218.pdf">論文</a></li></ul></div></div><div class="slide_index">[#461]</div><div class="timestamp">2018.5.8 14:33:52</div></div></section><section id="ID_Glimpse_Clouds_Human_Activity_Recognition_from_Unstructured_Feature_Points"><div class="paper-abstract"><div class="title">Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points</div><div class="info"><div class="authors">F. Baradel et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">RNNベースの行動認識を提案．
学習はRGB-Dを使うが，テスト時にはRGBのみを使うという設定．
テスト時にRGB-Dが使えてPose情報が使えればそれを使えばいいが，
それが使えないときもあるからそれに変わる手法を提案するという主張．
Poseでの間接位置に代わって，
Attentionベースでフレーム中から重要な局所要素 (Glimpse) を抽出＆トラッキング．
Glimpseの集合に基いて行動を認識するというフレームワーク．
Glimpseの抽出やトラッキングはそれぞれRNNベースで行う手法になっている．
</div></div><div class="item2"><img src="slides/figs/Glimpse_Clouds_Human_Activity_Recognition_from_Unstructured_Feature_Points.png" alt="Glimpse_Clouds_Human_Activity_Recognition_from_Unstructured_Feature_Points.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>姿勢の代わりに別の局所要素を使うフレームワークを提案</li><li>Attention, External Memoryといった流行り?の要素が詰め込んである</li><li>RGB-D行動認識データセットにおいてRGBのみの利用でSOTAを達成</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://perso.liris.cnrs.fr/christian.wolf/papers/cvpr2018.pdf">論文（著者版）</a></li><li><a href="https://arxiv.org/abs/1802.07898">論文 (Long-ver., arXiv)</a></li><li><a href="https://www.youtube.com/watch?v=7yPDYYhaYI4">動画 (YouTube)</a></li><li>姿勢ベースの行動認識を姿勢を使わずにやるような話に近い印象</li></ul></div></div><div class="slide_index">[#462]</div><div class="timestamp">2018.5.8 12:00:18</div></div></section><section id="ID_High-Resolution_Image_Synthesis_and_Semantic_Manipulation_with_Conditional_GANs"><div class="paper-abstract"><div class="title">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</div><div class="info"><div class="authors">Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro</div><div class="conference">CVPR 2018 (oral)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>GANの枠組みにてセマンティックラベルからの高精細画像（HD-Image）生成に関する研究。意味ラベルからリアルな画像を生成するのみならず、インタラクティブな操作で画像生成をコントロールすることも可能。Residual blocksにより構成されるエンコーダ/デコーダ構造を（入力をスケールが異なる画像として）入れ子構造にしデコーダ直前の中間層で統合して画像生成を実行する。さらに、ラベルのみならずインスタンスレベルの特徴量を用いることで写実性が向上したと主張（論文中図4では物体境界面あたりに出ているボケが綺麗になっている）。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180508HDImageGAN.png" alt="180508HDImageGAN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来法より、見た目の画像生成が明らかに良くなり、高画質の画像を対象にしても画像生成ができるようになった。従来手法（pix2pix（論文中文献21）, CRN（論文中文献5））さらに、インタラクティブな操作により生成画像を所望の結果に近づけることができる。動画像を見れば従来手法よりも鮮明になっていることは明らかであり、アーキテクチャや生成に関する知見も得ている。CVPRでoralになるための準備やプレゼンが論文中にも書かれていると感じた。やはりNVIDIAはずるいと言われるくらいの計算機環境が揃っているのではないか。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>これはもう、学習画像として使えるのでは？（すでにだれか使って精度検証しているのでは？）</p><ul><li><a href="https://arxiv.org/abs/1711.11585">論文</a></li><li><a href="https://tcwang0509.github.io/pix2pixHD/">Project</a></li><li><a href="https://github.com/NVIDIA/pix2pixHD">GitHub</a></li><li><a href="https://github.com/arXivTimes/arXivTimes/issues/544">arXivTimes</a></li><li><a href="https://www.youtube.com/watch?v=3AIpPlzM_qs">YouTube</a></li></ul></div></div><div class="slide_index">[#463]</div><div class="timestamp">2018.5.8 12:46:17</div></div></section><section id="ID_Five-point_Fundamental_Matrix_Estimation_for_Uncalibrated_Cameras"><div class="paper-abstract"><div class="title">Five-point Fundamental Matrix Estimation for Uncalibrated Cameras</div><div class="info"><div class="authors">D. Barath</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>2つの未キャリブレーションカメラにおいて，<strong>5点のみ</strong>で基礎行列を推定する手法を提案．</p><p>回転不変な特徴点（SIFT等）を使う．3点は平面にあれば，他2点はどこでも可能．グラフカットRANSACのようなロバスト対応点推定と組み合わせれば，state-of-the-artな性能が出る．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Five-point_Fundamental_Matrix_Estimation_for_Uncalibrated_Cameras_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>通常，7点や8点取るアルゴリズムが用いられるが，リーズナブルな制約で，少ない情報のみでキャリブレーションできるのはうれしい．例えば図のようにキャリブレーションボードを小さくできたりする．
大変有用な研究成果．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.00260">arXiv</a></li></ul></div></div><div class="slide_index">[#464]</div><div class="timestamp">2018.5.8 12:37:50</div></div></section><section id="ID_Defense_against_Adversarial_Attacks_Using_High-Level_Representation_Guided_Denoiser"><div class="paper-abstract"><div class="title">Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser</div><div class="info"><div class="authors">Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Jun Zhu and Xiaolin Hu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kodai Nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>画像分類におけるadrversarial attackの防御手法として, high-level representation guided denoiser (HGD) を提案.target model (メインの処理を担うネットワーク) への前処理段階で用いる.
HGDは, マルチスケールインフォメーションを得るためU-netの構造を使い,
トレーニングするための損失関数として, 元画像とノイズの乗った画像をそれぞれ入力したときの出力差を用いる.
右図に提案手法の詳細を示す.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/defence_against_adversarial_attacks_using_high-level_representation_guided_denoiser.png" alt="defence_against_adversarial_attacks_using_high-level_representation_guided_denoiser.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>pixel-levelの損失関数を課した従来のdenoiserと比べ, より良い結果が得られた.</p><p>state-of-the-artな防御手法であるensemble adversarial trainingと比べ, 3つのメリットがある.</p><ol><li>target modelがwhite-boxとblack-boxの両方に対してよりロバスト.</li><li>大規模データセットでの学習が簡単.</li><li>他のtarget modelへ使い回すことが可能.</li></ol></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.02976.pdf">論文URL</a></li></ul></div></div><div class="slide_index">[#465]</div><div class="timestamp">2018.5.8 12:24:05</div></div></section><section id="ID_Customized_Image_Narrative_Generation_via_Interactive_Visual_Question_Generation_and_Answering"><div class="paper-abstract"><div class="title">Customized Image Narrative Generation via Interactive Visual Question Generation and Answering</div><div class="info"><div class="authors">Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada</div><div class="conference">CVPR 2018</div><div class="paper_id">1224</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・新規の“Customized画像説明文生成”タスクを提案した．また，インタラクティブにユーザに自動的に画像に関する質問をし，回答文を収集できるような仕組みを提案した．・従来の画像説明文生成タスクにおいて，異なるユーザの性質や画像の注目領域などにより，多様な説明文を生成できることが検討されていない．このような性質に応じて，多様な質問文を生成できる仕組み及びユーザとインターアクションしユーザの個性的な回答文を収集しユーザの特徴を学習することにより，Customizedで画像説明文を生成できる仕組みを提案した．
・提案仕組みは具体的に：①画像から self Q&A modelにより，画像中のマルチリジョンを注目し(attention構造を利用した)質問文を生成し， VQAモデルにより回答する(マルチ回答がある質問文だけを保留)；②　①により生成できた質問文をユーザに提示し，回答させる；③画像リジョン・質問文・回答文の統合した画像説明文を生成する．
・画像リジョン・質問文・ユーザ特有な回答文からchoice vectorを抽出し，このベクトルを利用してほかの画像が入力された場合，ユーザの個性的な画像説明文を生成できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Customized_Image_Narrative_Generation.png" alt="Customized_Image_Narrative_Generation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・新規な問題設定“Customized画像説明文生成”・提案手法により，画像からより多様でユーザの個性を含んだ説明文を生成できる．
・ Automatic　Image　Narrative　Generationにおいて，従来のデータセットCOCO, SIND, DenseCapなどと比べ”diversity”,”interesting”,”naturalness”,”expressivity”などの指標に対しパフォーマンスが良い
・ Interactive　Image　Narrative　Generationにおいて，ヒューマンテストで良い評価を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・ユーザの個性を学習できる仕組みは応用場面が広そう</p><ul><li><a href="https://arxiv.org/pdf/1805.00460.pdf">論文</a></li></ul></div></div><div class="slide_index">[#466]</div><div class="timestamp">2018.5.8 12:19:18</div></div></section><section id="ID_First-Person_Hand_Action_Benchmark_with_RGB-D_Videos_and_3D_Hand_Pose_Annotations"><div class="paper-abstract"><div class="title">First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations</div><div class="info"><div class="authors">G Garcia-Hernando et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">一人称視点動画 (RGB-D) データセットの提供．
手（21点の3D間接位置）と物体（6D姿勢）の情報に加えて，45クラスの行動ラベルが付けられている．
データ数は1175シーケンス，10万フレーム．
手の3D姿勢と行動ラベルが付いている一人称視点動画データセットはこれまでになかった．
実験では従来手法やLSTMによるベースライン手法を合わせて18個を比較した結果が議論されており，
手の姿勢情報を使う手法が高い性能を示す傾向があることが確認されている．
</div></div><div class="item2"><img src="slides/figs/First-Person_Hand_Action_Benchmark_with_RGB-D_Videos_and_3D_Hand_Pose_Annotations.png" alt="First-Person_Hand_Action_Benchmark_with_RGB-D_Videos_and_3D_Hand_Pose_Annotations.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>手の3D姿勢を使った行動認識のためのデータセットを提供．</li><li>RGB, Depth, Poseといった様々な特徴を用いる各手法が詳細に議論されている．</li><li>一番良い手法で78%程度の認識率．</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://arxiv.org/abs/1704.02463">論文 (arXiv)</a></li><li><a href="https://www.youtube.com/watch?v=U5gleNWjz44">動画 (YouTube)</a></li></ul></div></div><div class="slide_index">[#467]</div><div class="timestamp">2018.5.8 12:00:18</div></div></section><section id="ID_PointFusion_Deep_Sensor_Fusion_for_3D_Bounding_Box_Estimation"><div class="paper-abstract"><div class="title">PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation</div><div class="info"><div class="authors">Danfei Xu, dragomir Anguelov, Ashesh Jain</div><div class="conference">CVPR 2018</div><div class="paper_id">50</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・画像と点群情報を利用した3D物体検出のフレームワークPointFusionを提案した．・従来のマルチセンサーの情報を利用した3D物体検出は前処理が必要、マルチセンサーを異なるパイプラインで処理し，他のセンサーのコンテキストをうまく利用できないなどの問題点がある．PointFusionは①異なるネットワーク構造を用いて画像(CNN)と点群情報(PointNet)を直接処理し，②デンスフュージョンネットワーク構造を提案し，画像と点群の抽出情報を統合しより精密な3D物体検出を行う．
・2種類のデンスフュージョンネットワークを提案した．①画像情報及びPointNetにより抽出したグローバル情報を統合し， 3Dボックスのコーナー位置を推定する．②画像情報及びPointNetにより抽出したグローバル情報、ポイントフィーチャーを統合し， 3Dボックスのオフセット及びconfidence scoresを予測する．最後の2つの結果を統合し，最終的な結果を予測する</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/PointFusion.png" alt="PointFusion"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・点群データの前処理が必要無し．・対応できるデータの形式が広い，室外環境と室内環境両方対応できる．
・多様な三次元センサーのデータを対応できる．(RGB-D, LiDar, Radar,…)
・KITTI, SUN-RGBDデータセットにおいてstate-of-the-artな結果</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・室内・外環境両方対応できるので、応用場面が広そう・将来的にend-to-endに実現できたら更に良い</p><ul><li><a href="https://arxiv.org/pdf/1711.10871.pdf">論文</a></li></ul></div></div><div class="slide_index">[#468]</div><div class="timestamp">2018.5.8 10:56:27</div></div></section><section id="ID_Path_Aggregation_Network_for_Instance_Segmentation"><div class="paper-abstract"><div class="title">Path Aggregation Network for Instance Segmentation</div><div class="info"><div class="authors">Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, Jiaya Jia</div><div class="conference">CVPR2018, arXive:1803.01534</div><div class="paper_id">912</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>Feature Pyramid Network(FPN)ベースのMask R-CNNに，下位層の特徴マップを上位層に伝播させるPath Aggregation Networkを提案．インスタンスセグメンテーションの傾向として，上位層では物体全体に強く反応するが，下位層では物体の局所的な領域に強く反応する．
そのため，Path Aggregation Networkでは，上位層と下位層の特徴マップを用いることで，インスタンスセグメンテーションの精度を向上させている．
Path Aggregation Networkは，COCOのベンチマークで2位の性能を達成しており，CityscapeとMVDでも高い性能を達成している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/912_overview.png" alt="912_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Path Aggregation Networkの構造は右図のようなシンプルな構造になっている．(a)の部分はFPNと同様の構造となっており，FPNの特徴マップから(b)で新しい特徴マップを作成する．
ここで，(a)と(b)では，緑線と赤線のように短距離と長距離のショートカットを導入する．
これにより，下位層の特徴を上位層に伝播することが可能である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.09020">論文リンク</a></li></ul></div></div><div class="slide_index">[#469]</div><div class="timestamp">2018.5.8 02:54:08</div></div></section><section id="ID_StarGAN_Unified_Generative_Adversarial_Networks_for_Multi_Domain_Image_to_Image_Translation"><div class="paper-abstract"><div class="title">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</div><div class="info"><div class="authors">Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo</div><div class="conference">CVPR2018, arXive:1711.09020</div><div class="paper_id">872</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>1つのネットワークでマルチドメイン対応の画像変換が可能なStarGANを提案．pix2pixやCycleGANの場合，左上図のように特定の1つのドメイン変換しかできないため，複数のドメイン変換をする時には各ドメインを変換するネットワークをそれぞれ構築しなければいけない．
StarGANでは，入力する条件とロス設計を適切に設計することで，シンプルなネットワークで多ドメインな画像変換を実現している．
実験では，顔属性のCelebAと表情のRaFD Datasetを使用し，2つのデータセットでGANを学習して下図のような多様な顔画像変換を可能にしている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/872_overview.png" alt="872_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>StarGANの構造は，右上図のようになっている．ここで，入力はそれぞれのドメインの画像がランダムに入力される．
まず，real imageとfake imageでDiscriminatorを学習する．
そして，次にGeneratorを学習する．
Generatorは，生成したい顔画像の条件とreal imageを入力して，画像変換する．
ここで，変換した画像はDiscriminatorに入力される．
変換した顔画像はCycleGANのようにreal imageを再変換する．
定義するロスは，一般的なAdversarial Loss，ドメインを認識するロス，real imageと再変換したimageのL1 Lossである．
また，複数のデータセットを学習するために，各データセットのラベルとデータセットの情報が格納されたMask vectorを導入している．
これにより，多ドメインかつ複数データセットに対応したGANを構築できている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>多ドメインかつ複数データセットに対応したGAN．変換するドメインの数に依存しないので，非常に用途が広がりそう．</p><ul><li><a href="https://arxiv.org/abs/1711.09020">論文リンク</a></li><li><a href="https://github.com/yunjey/StarGAN">コードリンク</a></li></ul></div></div><div class="slide_index">[#470]</div><div class="timestamp">2018.5.8 01:27:52</div></div></section><section id="ID_Semi-parametric_Image_Synthesis"><div class="paper-abstract"><div class="title">Semi-parametric Image Synthesis</div><div class="info"><div class="authors">Xiaojuan Qi, Qifeng Chen, Jiaya Jia, Vladlen Koltun</div><div class="conference">CVPR 2018 (oral)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>意味ラベル（Semantic Layout）から写真のようにリアルな画像をSemi-parametricな手法にて生成する。Semi-parametricはNon-parametricとParametricの強みを相補的に適用する手法である。セマンティックセグメンテーションのアノテーションとその対応する画像をペアとした外的なメモリにより対応関係を学習、Canvasとしてその順番や境界面を初期ステップとして出力する。次にCanvasと意味ラベルを入力としてConv-Deconv構造のネットワークにより写真のようにリアルな画像を出力とする。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180507SIMS.png" alt="180507SIMS"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Cityscapes, NYU, ADE20Kデータセットとセマンティックセグメンテーションに関するラベルが付与されていれば学習/テストが可能であり、同データセットにて従来法よりもさらにリアルな画像を生成するに至った。図には従来法（Chen and Koltun, ICCV 2017）との比較があり、従来法ではエッジ付近にボケが生じているが、提案法ではボケを相殺してさらに光の度合いまでもリアルに復元できている。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>意味ラベルから写真を復元することに成功した。今後、さらに生成するアピアランスや配置をコントロールする手法が登場すれば、学習データを無限に増やすことができたり、作りたい写真を再構成することが可能になる。</p><ul><li><a href="http://vladlen.info/papers/SIMS.pdf">論文</a></li><li><a href="http://vladlen.info/publications/sims/">Project</a></li><li><a href="https://github.com/xjqicuhk/SIMS">GitHub</a></li><li><a href="https://www.youtube.com/watch?v=U4Q98lenGLQ">Video</a></li></ul></div></div><div class="slide_index">[#471]</div><div class="timestamp">2018.5.7 13:32:33</div></div></section><section id="ID_Hierarchical_Novelty_Detection_for_Visual_Object_Recognition"><div class="paper-abstract"><div class="title">Hierarchical Novelty Detection for Visual Object Recognition</div><div class="info"><div class="authors">Kibok Lee, Kimin Lee, Kyle Min, Yuting Zhang, Jinwoo Shin, Honglak Lee</div><div class="conference">CVPR 2018</div><div class="paper_id">131</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・最も近いスーパークラスを予測することにより階層的新規(novelty)物体識別及び検出のフレームワークを提案した．・従来，新規なunseen物体識別は”known”と"unknown"に回帰する問題として対応されている．この論文で，物体のクラスを階層的に取り扱い，unseen物体の最も近いスーパークラスを求める．提案フレームワークによりgeneralized zero-shot learningタスクに用いられる階層的エンベディングを得られる．
・2種類の階層的な新規(novelty)物体検出構造を提案した．①top-down構造ではconfidence-calibrated classifierにより物体を分布の一致性が高いスーパークラスに分類する．②flatten構造では階層的分類構造の全体を用いずに error aggregationを避ける単一的なclassifierを用いる．また，①と②を組み合わせすることにより，階層的検出精度を向上できることを示した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/hierarchical_detection.png" alt="hierarchical_detection"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・従来のクローズデータセットを用いた物体検出と比べ，提案手法はオープンデータセットを対応できる．・generalized zero-shot learningタスクで提案フレームワークを用いられる
・ ImageNet, AwA2, CUBなどのデータセットで階層的新規(novelty)物体識別においてベースラインより高い精度を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.00722">論文</a></li></ul></div></div><div class="slide_index">[#472]</div><div class="timestamp">2018.5.7 13:08:17</div></div></section><section id="ID_Revisiting_Salient_Object_Detection_Simultaneous_Detection_Ranking_and_Subitizing_of_Multiple_Salient_Objects"><div class="paper-abstract"><div class="title">Revisiting Salient Object Detection: Simultaneous Detection, Ranking, and Subitizing of Multiple Salient Objects</div><div class="info"><div class="authors">Md Amirul Islam, Mahmoud Kalash, Neil D. B. Bruce</div><div class="conference">CVPR 2018</div><div class="paper_id">892</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・マルチsalientオブジェクトおよびそれぞれのsalientランキングを同時に検出するネットワークを提案した．・従来のsalientオブジェクトタスクに，salientランキングは観測者によって異なる結果が出る性質があるため，オブジェクトのsalientランキングについてまだ検討されていない．この文章でsalientランキングを有効的に得られるネットワークを提案した．またsalientランキング手法の評価方法も提案した．
・具体的なネットワーク構造はまずencoderネットワークにより粗末な相対salientスタックを生成し，そしてStacked Convolutional Module (SCM)により粗末なsaliency mapを生成する．またrank-awareでstage-wiseなネットワークによりsalientスタックをリファインする．ヒュージョンレイヤーにより各stageのsaliency mapを統合する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Revisiting-Salient-Object-Detection.png" alt="Revisiting-Salient-Object-Detection"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・saliency ランキングの提案・AUC, max　F-measure, median F-measure, average F-measure,MAE, and SORなどの
評価方法により，state-of-the-artなsalientオブジェクト検出性能を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.05082">論文</a></li></ul></div></div><div class="slide_index">[#473]</div><div class="timestamp">2018.5.7 12:45:59</div></div></section><section id="ID_Rethinking_the_Faster_R-CNN_Architecture_for_Temporal_Action_Localization"><div class="paper-abstract"><div class="title">Rethinking the Faster R-CNN Architecture for Temporal Action Localization</div><div class="info"><div class="authors">Y. Chao et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">動画中の行動のラベル，開始・終了時刻を推定するTemporal Action Localizationの研究．
Faster R-CNNによる物体検出をベースにLocalizationをする．
ここで，スケールのバリエーションが非常に大きい，前後の行動などのコンテキストが重要，
RGBとFlowをどう統合するか，といった3点の検討が重要としてこれらに取り組んでいる．
提案手法であるTAL-Netのポイントとしては，
アンカーごとに適切なスケールの受容野を持つ異なるCNNを組み合わせて利用している点．
各問題に対する設計がそれぞれ精度向上に寄与している点を実験から確認し，
THUMOS'14でのSOTAを達成．
</div></div><div class="item2"><img src="slides/figs/Rethinking_the_Faster_R-CNN_Architecture_for_Temporal_Action_Localization.png" alt="Rethinking_the_Faster_R-CNN_Architecture_for_Temporal_Action_Localization.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>行動の時間スケールについての検討をちゃんと行った点は新規性がある</li><li>提案手法の各要素についての実験がされていて，それぞれによる精度向上を確認できている</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://arxiv.org/abs/1804.07667">論文 (arXiv)</a></li><li>目新しいアイデアはないように思うが，問題点に対する解法を検討してかっちりと評価している</li><li>この辺りのスケールの話は大事そうなのにこれまで意外とちゃんとやられてきてなかったところ</li></ul></div></div><div class="slide_index">[#474]</div><div class="timestamp">2018.5.7 12:44:45</div></div></section><section id="ID_PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume"><div class="paper-abstract"><div class="title">PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume</div><div class="info"><div class="authors">Deqing Sun, Xiaodong Yang, Ming-Yu Liu, Jan Kautz</div><div class="conference">CVPR 2018 (oral)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>コンパクトかつ効果的なオプティカルフロー推定を実現するPWC-Netを提案する。ピラミッド構造かつ学習可能な階層的処理、射影（Warping）、コストボリュームにより設計され、軽量化しながら高精度なフロー推定を実現している。図は従来法（左図）と提案法（右図）のアーキテクチャの概略を示している。従来は画像のピラミッド構造により全てのサイズを階層的にオプティカルフローの射影や最適化を行い、最後に後処理をしていたが、提案法のPWCNetではあるひとつの階層内で後処理を行い、コンテキストを考慮したネットワーク（ContextNetwork; Dilated Convによる、各階層のオプティカルフローを入力するとそれらを総合的に解釈して最良のオプティカルフローを出力する）を通り抜けることで出力する。間には{Warping, Cont Volume, Optical flow}を行う層により構成される。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180507PWCNet.png" alt="180507PWCNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来法であるFlowNet2よりも17分の1の軽量化モデルでありながら、MPI Sintel final pass/KITTI 2015 BenchmarkにてState-of-the-art、Sintel 1024x436の解像度にて35fpsで動作する。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>オプティカルフロー/距離画像の推定など、RGBの入力から異なるチャンネルを出力する課題が登場して本論文のように精度向上やコンパクト化、処理速度向上が著しい。ただし、出力したオプティカルフローや距離画像の出力自体の正当性を保証するような評価方法が必要？特に、異なるドメイン（ドイツの道路データで学習して日本の道路データでテストするなど）での適応とその性能保証は欲しいところ。</li><li>（さすがNVIDIA！？）実験量がとても多く見える。Table1~7までびっしり実験結果が埋められている。</li><li><a href="http://xiaodongyang.org/publications/papers/pwc-cvpr18.pdf">論文</a></li><li><a href="http://research.nvidia.com/publication/2018-02_PWC-Net%3A-CNNs-for">Project</a></li><li><a href="https://github.com/deqings/PWC-Net">GitHub</a></li></ul></div></div><div class="slide_index">[#475]</div><div class="timestamp">2018.5.7 12:26:54</div></div></section><section id="ID_LEGO_Learning_Edge_with_Geometry_all_at_Once_by_Watching_Videos"><div class="paper-abstract"><div class="title">LEGO: Learning Edge with Geometry all at Once by Watching Videos</div><div class="info"><div class="authors">Z. Yang et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">ラベルなし動画からの3次元幾何 (Depth, Normal) の推定．
従来研究のものだと画素ごとの誤差で最適化していたのでボケた幾何構造推定になっていたのが問題と主張．
提案手法はエッジと3次元幾何を同時に推定して最適化することで，左図 (f) のような正確な幾何構造を推定可能にした．
ベースは従来手法同様で，カメラ姿勢を推定し，それに基づくWarping結果と元のフレームとの間の誤差をとって最適化．
これに，エッジ推定と3D-ASAP (as smooth as possible in 3D) Priorを導入したところがポイント．
3D-ASAPはある2点間の間にエッジがなければその2点は同一平面上にあるという仮定に基づく提案手法．</div></div><div class="item2"><img src="slides/figs/LEGO_Learning_Edge_with_Geometry_all_at_Once_by_Watching_Videos.png" alt="LEGO_Learning_Edge_with_Geometry_all_at_Once_by_Watching_Videos.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>3次元幾何とエッジ推定を同時にする手法の提案</li><li>3D-ASAP Priorの定式化とそれによる精度向上を実現</li><li>KITTIやCityScapesでのSOTAを達成</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://arxiv.org/abs/1803.05648">論文 (arXiv)</a></li><li><a href="https://www.youtube.com/watch?v=40-GAgdUwI0">結果動画 (YouTube)</a></li></ul></div></div><div class="slide_index">[#476]</div><div class="timestamp">2018.5.7 11:15:05</div></div></section><section id="ID_DA-GAN_Instance-level_Image_Translation_by_Deep_Attention_Generative_Adversarial_Network"><div class="paper-abstract"><div class="title">DA-GAN: Instance-level Image Translation by Deep Attention Generative Adversarial Network</div><div class="info"><div class="authors">Shuang Ma, Jianlong Fu, Chang Chen, Tao Mei</div><div class="conference">CVPR 2018</div><div class="paper_id">695</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・無監督インスタンスレベルのattentionを用いたImage Translationフレームワークを提案した．・従来の無監督Image Translationではセットレベルで実現され，物体パーツレベルの対応ができないため，従来手法より生成した物体画像が幾何や意味的な情報のリアル性が低い場合がある．それと比べ，提案フレームワークは①物体をはattentionを用いた高構造化latent空間に変換し，このlatent空間によりインスタンスレベルなImage Translationを可能にした．②さらに，source samplesとtranslated samplesをセマンティック的に対応させるconsistency lossを提案した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DA-GAN.png" alt="DA-GAN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・初めてattentionをGANに導入したと宣言・MNIST , CUB-200-2011, SVHN , FaceScrub and AnimePlanet 1などのデータセットを用いて実験を行い，ドメンadaption，テキスト-画像合成，ポーズモーフィング，顔‐アニメーション化などのタスクにおいて，state-of-the-artな精度を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・attentionをGANに導入し，さらに精密で構造化した画像生成ができるので，様々なアプリで応用できそう</p><ul><li><a href="https://arxiv.org/pdf/1802.06454.pdf">論文</a></li></ul></div></div><div class="slide_index">[#477]</div><div class="timestamp">2018.5.7 10:19:19</div></div></section><section id="ID_PhaseNet_for_Video_Frame_Interpolation"><div class="paper-abstract"><div class="title">PhaseNet for Video Frame Interpolation</div><div class="info"><div class="authors">Simone Meyer, et al.</div><div class="paper_id">1804.00884</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>様々なシーンに頑健かつ、大きな動きにも対処しながらビデオフレームの補間を行うPhaseNetの提案。中間のフレームにおける位相と階層構造を推定するnnのデコーダを搭載。これにより、既存の位相ベースの手法よりも広範囲に渡る動きに対応。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180507PhaseNet.jpg" alt="180507PhaseNet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>既存のビデオフレーム補間アプローチは、フレーム間において密な対応付けが必要であり、照明変化や被写体ブレに頑健でない。カーネルに依存した深層学習ベースの手法でもある程度緩和することはできるが不十分。ピクセル単位の位相ベースの手法ならば上手くいくことが実装されている。位相ベースでnnを用いた手法を提案。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>位相のlossとノルムを組み合わせることで、チャレンジングなシーンでも視覚的に綺麗な画像を生成できる。</p><ul><li><a href="https://arxiv.org/pdf/1804.00884.pdf">論文</a></li></ul></div></div><div class="slide_index">[#478]</div></div></section><section id="ID_Multi-scale_Location-aware_Kernel_Representation_for_Object_Detection"><div class="paper-abstract"><div class="title">Multi-scale Location-aware Kernel Representation for Object Detection</div><div class="info"><div class="authors">Hao Wang, Qilong Wang, Mingqi Gao, Peihua Li and Wangmeng Zuo</div><div class="conference">CVPR2018</div><div class="paper_id">153</div></div><div class="slide_editor">Ryosuke Araki</div><div class="item1"><div class="text"><h1>概要</h1><p>物体検出時に特徴量の高次の統計量（high-order statistics）を獲得するためのMulti-scale Location-aware Kernel Representation（MLKP)を提案する．MLKPはSSDで用いるような，複数解像度の特徴マップを結合したマルチスケール特徴マップを用いて効果的に計算できる．マルチスケール特徴マップをMLKPに入力すると，畳み込みと要素ごとの積算を行いr次の表現Z^rを得る．このとき，location-weight networkは各位置の寄与度を学習する．その後，各次の表現を重みつき結合し，RoI Poolingへ入力する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180507_mlkp1.jpg" alt="20180507_mlkp1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>最近の分類メソッドでよく用いられる高次統計量を物体検出器の高精度化に用いる手法である．Faster R-CNNにMLKPを統合することで，Faster R-CNNよりも精度が4.9%(mAP, VOC2007），4.7%（mAP, VOC2012），5.0%（MSCOCO）向上した．DSSDやR-FCNと比較しても同等もしくはそれ以上の性能である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>流行りのマルチスケール手法をR-CNNに昇華した感じ．R-CNNベースの手法もまだまだ煮詰める余地は十分ある．</p><ul><li><a href="https://arxiv.org/abs/1804.00428">arXiv</a></li><li><a href="https://github.com/Hwang64/MLKP">コード</a>py-faster-rcnnをベースにされている．マルチGPU版もあり</li></ul></div></div><div class="slide_index">[#479]</div><div class="timestamp">2018.5.7 01:24:41</div></div></section><section id="ID_Self_supervised_Learning_of_Geometrically_Stable_Features_Through_Probabilistic_Introspection"><div class="paper-abstract"><div class="title">Self-supervised Learning of Geometrically Stable Features Through Probabilistic Introspection</div><div class="info"><div class="authors">David Novotny et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>幾何学変換を利用したGeometrically Stable な特徴表現の獲得手法。オリジナル画像とそれに幾何学変換を施した画像を同じCNNに学習し、中間特徴マップ上で対応するpixelでの特徴量の類似度が高くなるように学習する。キーポイントマッチングなどの問題設定で教師あり学習以上の効果を発揮。Pixelによってはマッチングが困難ば場合も存在するため、不確実性を考慮した学習を提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Novotny.png" height="1000"></p></div></div><div class="item3"><div class="text"><h1>手法・新規性</h1><p>ペアとなる画像を同じNNに入力し、各pixel ペアの類似度と、不確実性を表す値を算出。不確実性を考慮した損失関数を定義することで、結果的にNNはマッチング可能かつ対応するpixelに関しては高い類似度と低い不確実性を、マッチングが困難なものに関しては高い不確実性を算出するように学習される。</p></div></div><div class="item4"><div class="text"><h1>メモ・リンク</h1><p>定義された距離尺度において対象に直接近づける枠組みが多い通常の類似度学習と異なり、連続値である類似度を確率変数とすることで、不確実性を考慮するのは興味深い。しかし、定式化としては論文内のものよりも、不確実性利用してモデルが類似度の分布を算出しているという定式化にした方がわかりやすいのではないかと思った。</p><ul><li><a href="https://arxiv.org/abs/1804.01552">論文</a></li></ul></div></div><div class="slide_index">[#480]</div></div></section><section id="ID_Squeeze-and-Excitation_Networks"><div class="paper-abstract"><div class="title">Squeeze-and-Excitation Networks</div><div class="info"><div class="authors">Jie Hu, Li Shen, Gang Sun</div><div class="conference">CVPR2018, arXive:1709.01507</div><div class="paper_id">891</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>Residualモジュール, Inceptionモジュールに対してAttention機構を導入したネットワーク．Squeeze-and-Excitation Networks(SENet)では，生成される特徴マップのチャンネルに対してAttentionを導入している．
SENetは，ImageNetでstate-of-the-artな性能を達成している．(現在1位)
また，Place Datasetでも高い性能を達成している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/891_overview.png" alt="891_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>SENetには，右図のように2つのモジュールが提案されている．SE Inception moduleは，VGGやAlexNet等の順伝播ネットワークで使われるSEモジュール．
SE Residual moduleは，ResNet系のネットワークに使われるSEモジュールである．
基本的には，Global Average Poolingを施した後に，全結合層を何層か通してチャンネル毎のAttentionを生成する．
この構造は，ResNet等の様々なネットワークモデルにも適応できる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Attention機構を導入した物体認識法．最近，物体認識にもAttentionが流行し始めているので，その先駆けな手法になりそう．
学習モデルもGitHub上で公開．</p><ul><li><a href="https://arxiv.org/abs/1709.01507">論文リンク</a></li><li><a href="https://github.com/hujie-frank/SENet">コードリンク</a></li></ul></div></div><div class="slide_index">[#481]</div><div class="timestamp">2018.5.6 23:46:46</div></div></section><section id="ID_ClusterNet_Detecting_Small_Objects_in_Large_Scenes_by_Exploiting_Spatio-Temporal_Information"><div class="paper-abstract"><div class="title">ClusterNet: Detecting Small Objects in Large Scenes by Exploiting Spatio-Temporal Information</div><div class="info"><div class="authors">Rodney LaLonde, Dong Zhang, Mubarak Shah</div><div class="paper_id">1704.02694</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>1平方キロメートル以上の広範囲の領域を撮影できるWide Area Motion Imagery(WAMI)の映像から、車などの小さい物体を検出する手法の提案。まず、ClusterNetでビデオフレームから、CNNを使って動きと外観情報を結合し、regions of objects of interest(ROOBI)を出力。次に、FoceaNetによって、ヒートマップ推定を介して、ROOBI内の物体の重心位置を推定する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180506ClusterNet.jpg" alt="180506ClusterNet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>WAMIを使った従来の物体検出は、アピアランスベースの分類器であまり精度が出ず、背景差分やフレーム間差分などの動き情報に依存しがち。Fast R-CNNなどにおけるこれらの問題を検証し、効率的かつ効果的な新たな2ステージCNNを提案。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>広範囲の情報から数百の物体を同時に検出する。他の手法では扱えない停止車両なども検出できる。</p><ul><li><a href="https://arxiv.org/pdf/1704.02694.pdf">論文</a></li><li><a href="https://www.tno.nl/en/focus-areas/defence-safety-security/roadmaps/information-sensor-systems/wide-area-motion-imagery-wami/">WAMI</a></li></ul></div></div><div class="slide_index">[#482]</div></div></section><section id="ID_An_Analysis_of_Scale_Invariance_in_Object_Detection-SNIP"><div class="paper-abstract"><div class="title">An Analysis of Scale Invariance in Object Detection – SNIP</div><div class="info"><div class="authors">Bharat Singh, Larry S. Davis</div><div class="paper_id">1711.08189</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>極端なスケール変化に頑健な物体検出手法であるSNIPの提案。物体検出において、大きな物体と小さな物体をそれぞれ検出することは困難。そこで、学習時に異なるサイズの物体における勾配を、選択して逆伝播する。物体の幅広いスペクトルに対処し、ドメインシフトを低減する。ピラミッド型のネットワークとなっており、end-to-end学習可能。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180506SNIP.jpg" alt="180506SNIP.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>まず、現代の物体検出手法の欠点として、スケール変化について解析している。小さい物体を検出するために“アップサンプリング画像が必要か”などを、ImageNetを使ってパフォーマンスを評価。これらの解析に基づいてSNIPを開発。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>COCO2017 challengeにおける、最優秀学生応募作品。</p><ul><li><a href="https://arxiv.org/pdf/1711.08189.pdf">論文</a></li><li><a href="http://bit.ly/2yXVg4c">コード</a></li></ul></div></div><div class="slide_index">[#483]</div></div></section><section id="ID_The_iNaturalist_Species_Classification_and_Detection_Dataset"><div class="paper-abstract"><div class="title">The iNaturalist Species Classification and Detection Dataset</div><div class="info"><div class="authors">Grant Van Horn, el al. </div><div class="paper_id">1707.06642</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>自然界にける、“写真に写り易さ”を考慮した画像分類・検出タスク用データセットの提案。5000種類以上の植物や動物からの85万9000の画像で構成。世界各地の多種多様な種やシチュエーションで撮影され、様々なカメラタイプで収集することで画質の変化し、クラスの均衡が大きい。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180506iNaturalist.jpg" alt="180506iNaturalist.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>従来の画像分類・検出用データセットでは、カテゴリごとに画像数が統一されている傾向にある。しかし，写真に収め易い種と、そうでない種があるため、自然界はとても不均衡。この差に着目し、現実世界の状況に近い状況で分類・検出に挑戦するデータセットを提案した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>やはり既存の手法では精度を出すのは難しそう。このデータセットで精度を出すチャレンジングな研究をするのはアリ。</p><ul><li><a href="https://arxiv.org/abs/1707.06642">論文</a></li></ul></div></div><div class="slide_index">[#484]</div></div></section><section id="ID_Between-class_Learning_for_Image_Classification"><div class="paper-abstract"><div class="title">Between-class Learning for Image Classification</div><div class="info"><div class="authors">Yuji Tokozume, Yoshitaka Ushiku and Tatsuya Harada</div><div class="paper_id">1711.10284</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>Between-Class learning(BC learn)という画像分類タスクにおける新学習方法の提案。まず、異なるクラスの2枚の画像をランダムな比率で混合したbetween-class imageを作成。そして、画像を波形として扱うためにミキシングを行う。混合画像をモデルに入力し、学習することで混合した比率を出力する。これにより、特徴分布の形状に制約をかけることができるため、汎化性能が向上する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180506Between-class.jpg" alt="180506Between-class.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>もともとは、混合できるデジタル音声のために開発された手法。CNNは“画像を波形として扱っている”という説から、本手法を提案。2つの画像を混合する意味に疑問はあるが、実際にパフォーマンスが向上している。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>混合とミキシングの提案手法によって分類精度が向上。画像の混合にどんな意味があるのかを解明してほしい。</p><ul><li><a href="https://arxiv.org/abs/1711.10284">論文</a></li></ul></div></div><div class="slide_index">[#485]</div></div></section><section id="ID_CleanNet_Transfer_Learning_for_Scalable_Image_Classifier_Training_with_Label_Noise"><div class="paper-abstract"><div class="title">CleanNet: Transfer Learning for Scalable Image Classifier Training with Label Noise</div><div class="info"><div class="authors">Kuang-Huei Lee, Xiaodong He, Lei Zhang and Linjun Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルノイズを使って、画像分類モデルを学習するCleanNetの提案。人間による“ラベルノイズの低減”という作業を低減する。事前知識として人の手で分類されたクラスの一部の情報だけを使い、ラベルノイズを他のクラスに移すことができる。また、CleanNetとCNNによるクラス分類ネットワークを1つのフレームワークとして統合。ラベルノイズ検出タスクと、統合した画像分類タスクの両方で、ノイジーなデータセットを使って精度検証。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180506CleanNet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>人間がラベルのアノテーションをすると時間がかかり、学習はスケーラブルじゃない。逆に人間に頼らない手法はスケーラブルだが、有効性が低い。少し人間に頼って、あとは自動的にノイズ除去をするというハイブリットな手法。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>弱教師付き学習と比較して、ノイズを41%低減。画像分類タスクにおいて、47%パフォーマンスが向上。</p><ul><li><a href="https://arxiv.org/abs/1711.07131">論文</a></li></ul></div></div><div class="slide_index">[#486]</div></div></section><section id="ID_Super-Resolving_Very_Low-Resolution_Face_Images_with_Supplementary_Attributes"><div class="paper-abstract"><div class="title">Super-Resolving Very Low-Resolution Face Images with Supplementary Attributes</div><div class="info"><div class="authors">Xin Yu, Basura Fernando, Richard Hartley, Faith Porikli</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔画像のアトリビュートを使用することでGTとなる高解像度画像(HR)を使用せずに低解像度画像(LR)を超解像度化する研究。LRとともに顔のアトリビュートも入力として使用することで超解像化における曖昧さを解決。
ネットワークの大枠はGANを採用。
ジェネレータにおいてLRをauto encoderに噛ませる際にエンコードされた特徴量にアトリビュートを付け足してでコードを行う。
ディスクリミネータはGTのHR画像なら1を、ジェネレータによる画像or画像にアトリビュートが含まれていないと判断した際には0を返す。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180506Super-Resolving_Very_Low-Resolution_Face_Images_with_Supplementary_Attributes.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>入力は16x16画像、出力は入力画像が128x128に超解像度化された画像。</li><li>PSNR、SSIMを評価指標として既存手法と比べたところもっとも良い精度を得た。</li><li>既存手法で入力されたLRに対して一意的なHRのみしか出力することができなかった。一方提案手法では入力するアトリビュートに伴って出力するHRの見た目を変更することが可能。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>トレーニングで使用したデータセットはCelebAであり、使用したアトリビュートはCelebAに付属する40種類のうちからgender, ageなど18種類。</li><li><a href="https://basurafernando.github.io/papers/XinYuCVPR18.pdf">論文</a></li></ul></div></div><div class="slide_index">[#487]</div></div></section><section id="ID_Single-Shot_Object_Detection_with_Enriched_Semantics"><div class="paper-abstract"><div class="title">Single-Shot Object Detection with Enriched Semantics</div><div class="info"><div class="authors">Z.Zhang, S.Qiao, C.Xie, W.Shen, B.Wang and A.L.Yuille</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1712.00433</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>Detection with Enriched Semantics (DES)というシングルショットオブジェクト検出器を提案．セマンティックセグメンテーションブランチとオブジェクト検出ブランチで構成.
セマンティックセグメンテーションブランチとグローバルアクティベーションモジュールによってオブジェクト検出の特徴であるセマンティクスを向上．
既存のSSDなどのシングルショット検出器よりも速度と精度が向上．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Single-Shot_Object_Detection_with_Enriched_Semantics.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>セマンティックセグメンテーションブランチに高レベルのオブジェクト特徴のためのオブジェクト検出特徴チャンネルとオブジェクトクラスとの意味的関係を学習するためのグローバルアクティベーションブロックを加える．</li><li>一般的なシングルショット検出器と比較して大幅に検出精度が向上，</li><li>Titan Xp GPU1台で、31.7 FPSを達成し、R-FCNやResNetベースのSSDよりも高速.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.00433.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#488]</div><div class="timestamp">2018.5.6 01:42:36</div></div></section><section id="ID_Revisiting_Deep_Intrinsic_Image_Decompositions"><div class="paper-abstract"><div class="title">Revisiting Deep Intrinsic Image Decompositions</div><div class="info"><div class="authors">Qingnan Fan, Jiaolong Yang, Gang Hua, Baoquan Chen, David Wipf</div><div class="conference">CVPR 2018 oral</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>光の反射やシェーディングなどを再計算することで自然画像の分解と再構成（Image Decomposition）を行う問題設定である。従来型の事前情報を陽に与えるフィルタリング手法とは異なり、深層学習による提案手法では（十分なラベル付きデータが存在すれば）画像の内的な情報を効果的に捉えて画像の再構成をより自然に行うことができると主張。この問題を解決するために、２種類のカテゴリに関する問い ー（１）詳細なラベル付きデータ（２）弱教師付き学習により比較的多様なラベル付きデータを学習ー を解決することができる。これにより学習データには詳細なラベル付けを行わず弱い事前知識（Loose Prior Knowledge）のみで大量のサンプルを準備することができる。手法面において、最初は荒く光の反射（Albedo）やシェーディングを推定し、次いでエッジやテクスチャ等を推定できるようにフィルタリングを学習する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180505DeepIntrinsic.png" alt="180505DeepIntrinsic"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>主要な画像再構成のベンチマークにおいて全てState-of-the-artの（最先端の）結果を達成した。さらに、従来まではデータセットに対してアドホックである（と思われる）が、本論文にて提供するデータや手法はよりオープンかつリアルな問題に対して汎用的に使用できる。弱い事前知識のみでリアルデータを学習できるようにしたことも新規性として挙げられる。CVPRの査読を突破できた理由として、State-of-the-artな精度を全てのデータにて達成したことや、その学習法/アーキテクチャの提案にあると考える。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>光の反射（Albedo）や陰影（shading）を同時に推定できる技術はよりリアルな画像の生成には重要技術なのでどんどん進んで欲しい。</p><ul><li><a href="https://arxiv.org/abs/1701.02965">論文</a></li><li><a href="http://www.cs.toronto.edu/~rgrosse/intrinsic/">MIT Intrinsic Images Dataset</a></li><li><a href="http://sintel.is.tue.mpg.de/">MPI Sintel Flow Dataset</a></li><li><a href="http://opensurfaces.cs.cornell.edu/publications/intrinsic/">Intrinsic Images in the Wild</a></li></ul></div></div><div class="slide_index">[#489]</div><div class="timestamp">2018.5.5 17:36:29</div></div></section><section id="ID_Self-supervised_Multi-level_Face_Model_Learning_for_Monocular_Reconstruction_at_over_250_Hz"><div class="paper-abstract"><div class="title">Self-supervised Multi-level Face Model Learning for Monocular Reconstruction at over 250 Hz</div><div class="info"><div class="authors">Ayush Tewari, Michael Zollhöfer, Pablo Garrido, Florian Bernard, Hyeongwoo Kim, Patrick Perez, Christian Theobalt</div><div class="conference">CVPR 2018 Oral</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>単眼顔画像からリフレクタンス、ジオメトリー、照明情報を推定する研究。トレーニングデータには上記の情報のアノテーションを必要とせず、3D Morpahlbe Modelを使用することで高品質な3Dパラメトリックモデルを生成。
3D Morpahlbe Modelを使用することで高品質な3Dパラメトリックモデルを生成。
テスト時には250Hz以上で実行することができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180505Self-supervised_Multi-level_Face_Model_Learning_for_Monocular_Reconstruction_at_over_250_Hz.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>大量のアノテーションが必要という既存手法の問題点を解決</li><li>様々な表情に対応することができ、口髭や化粧も再現することが可能。</li><li>既存のラーニングベースの手法と比較した結果、同等の実行時間でより精度の高いリコンストラクションが可能となった。最適化ベースの手法と比較すると10%ほど精度は落ちるものの、最適化ベースの手法では実行時間が120secかかるが提案手法では4msで実行可能。 </li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>目元やおでこの皺の再現には至っていない</li><li><a href="https://arxiv.org/abs/1712.02859">論文</a></li><li><a href="https://web.stanford.edu/~zollhoef/papers/CVPR18_FaceModel/page.html">Project page</a></li></ul></div></div><div class="slide_index">[#490]</div></div></section><section id="ID_TextureGAN_Controlling_Deep_Image_Synthesis_with_Texture_Patches"><div class="paper-abstract"><div class="title">TextureGAN: Controlling Deep Image Synthesis with Texture Patches</div><div class="info"><div class="authors">W.Xian,  P.Sangkloy, V. Agrawal, A.Raj, J.Lu, C.Fang, F.Yu and J.Hays</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1706.02823</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>ユーザが色，スケッチ，テクスチャから深層画像合成を行うTextureGANを提案．既存手法では，カラーやスケッチによる制御を行っているが今回の手法ではユーザがテクスチャパチをスケッチ上に配置することによってテクスチャによる制御を実現．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/TextureGAN_Controlling_Deep_Image_Synthesis_with_Texture_Patches.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>深層画像合成における細かいテクスチャ制御の妥当性を初めて実証</li><li>ユーザが特定のテクスチャをスケッチの境界に「ドラック＆ドロップ」するテクスチャインタフェースの提案.</li><li>生成ネットワークで既存のオブジェクトに見られないテキスチャであった場合でも扱うようにする局所テクスチャロスを定義．</li></ul></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><ul><li><p>TextureGANをローカルテクスチャで制約することにより，テクスチャとスケッチベースの画像合成の効果を実証．</p></li><li><p>別のテクスチャデータベースから抽出されたテクスチャから生成されたスケッチを用いて実験を行い、提案アルゴリズムがユーザコントロールに忠実な妥当な画像を生成されることを確認．</p></li><li><p><a href="https://arxiv.org/pdf/1706.02823.pdf">Paper</a></p></li></ul></div></div><div class="slide_index">[#491]</div><div class="timestamp">2018.5.5 01:54:38</div></div></section><section id="ID_Learning_Deep_Models_for_Face_Anti-Spoofing"><div class="paper-abstract"><div class="title">Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision</div><div class="info"><div class="authors">Yaojie Liu, Amin Jourabloo, Xiaoming Liu</div><div class="conference">CVPR2018 SPOTLIGHT</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>入力された動画が生身の人間によるものか、あるいはそれ以外のspoofing（撮影された動画や顔のプリントなど）を判定する研究。空間的な情報として顔のデプスマップ、時間的な情報としてrPPG（信号のパルス信号）。
CNN-RNNモデルを使用しCNNでデプスマップと顔の特徴量マップを、RNNは各時刻でCNNによって推定された顔の特徴量マップを入力としてrPPGを推定する。
既存研究では様々なパターンのspoofingがあるにも関わらずCNNによるバイナリの識別問題として捉えていたため、CNNの広すぎる空間を学習してしまい結果的に過学習をしてしまっていた。
提案手法では補助的な情報としてデプスマップ、rPPGを使用することで識別精度を向上した。
更に165の被写体に対して様々な照明環境、ポーズ、表情、顔むきごとの動画を収集し、anti-spoofingのためのSiWデータベースを構築した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180504Learning_Deep_Models_for_Face_Anti-Spoofing.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜCVPRに通ったか？</h1><ul><li>提案手法では既存研究のようにバイナリの識別問題とはとらえず、デプスマップとrPPGを使用することで学習したパターンのspoofingを確実に検出できることを目的とした。</li><li>既存研究とAPCER、BPCER、ACER、HTER値における比較を行なった結果、提案手法優位な結果となった。識別精度は約72%、state-of-the-artの研究では約34%。</li><li>165の被写体に対して様々な照明環境、ポーズ、表情、顔むきごとの動画を収集し、anti-spoofingのためのSiWデータベースを構築。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.11097">論文</a></li></ul></div></div><div class="slide_index">[#492]</div></div></section><section id="ID_Adversarially_Learned_One-Class_Classifier_for_Novelty_Detection"><div class="paper-abstract"><div class="title">Adversarially Learned One-Class Classifier for Novelty Detection</div><div class="info"><div class="authors">M.Sabokrou, M.Khalooei, M.Fathy and E.Adeli</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1802.09088</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>1クラス分類の際のノベリティ検出のために2段階のネットワークを構築．1つのネットワークはノベリティの検出をし，もう1つでは，inlierを強化しoutlierを歪ませる．
画像と動画で検証．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Adversarially_Learned_One-Class_Classifier_for_Novelty_Detection.png" alt="概要図"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>1クラス分類のためのend to endネットワークを導入したもの</li><li>GANを用いた手法では学習後に片方のモデルのみが使われるが，今回の手法ではテストの際に両方のモデルを掛け合わせることで効率化を図る</li></ul></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><ul><li>inlierとoutlierの分類は元のクラスのサンプルの決定よりも優れている．</li><li>ノベリティクラスのサンプルが無くても学習し，動画や画像の異常を検知でき，様々なアプリケーションで高いパフォーマンスを示す．</li><li><a href="https://arxiv.org/pdf/1802.09088.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#493]</div><div class="timestamp">2018.5.4 03:50:34</div></div></section><section id="ID_Feature_Space_Transfer_for_Data_Augmentation"><div class="paper-abstract"><div class="title">Feature Space Transfer for Data Augmentation</div><div class="info"><div class="authors">Bo Liu, Mandar Dixit, Roland Kwitt, Nuno Vasconcelos</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像空間上ではなく、特徴空間上でデータ拡張（Data Augmentation）を行う研究である。この課題に対して著者らは特徴空間上で物体姿勢/見え方のバリエーションを多様体として考慮するFeature Transfer Network (FATTEN)を提案。従来の特徴空間上でのデータ拡張とは異なり、提案法であるFATTENはEnd-to-Endでの学習が可能であり、より効果的にデータ拡張を実行可能である。同ネットワークは姿勢やカテゴリの多タスク学習により学習を行う。図は直感的な特徴空間上での挙動を示したもので、Pose/Appearanceにおける特徴空間の動線を把握した上でデータ拡張を行うことができる。One-/Few-shot学習でも効果を発揮し、特にOne-shotでは他を大きく離して優れていることを示した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180504FATTEN.png" alt="180504FATTEN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>新規性としては複数の属性（ここでは姿勢・アピアランス）を同時に考慮しながら特徴空間上でデータ拡張を行える点が新規性としてあげられ、さらに関連研究と異なるのはEnd-to-Endで学習できる点も優れている。直感的にはビューポイントの違いとそれに対応するアピアランスを拡張する形で特徴学習ができていると言える。FATTENを適用しModelNet/SUN-RGBDのデータセットにてデータ拡張を行った結果、はっきりとした精度向上を確認した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>RotationNetとの比較や統合（RotationNet+FATTEN）が気になる。もともとこの論文で扱っている問題に対して精度が高いRotationNetに本論文のデータ拡張手法を使用するとさらに大きく精度向上するのでは？</p><ul><li><a href="https://arxiv.org/abs/1801.04356">論文</a></li><li><a href="https://github.com/kanezaki/rotationnet">RotationNet</a></li></ul></div></div><div class="slide_index">[#494]</div><div class="timestamp">2018.5.4 00:21:12</div></div></section><section id="ID_Deep_Extreme_Cut_From_Extreme_Points_to_Object_Segmentation"><div class="paper-abstract"><div class="title">Deep Extreme Cut: From Extreme Points to Object Segmentation</div><div class="info"><div class="authors">Kevis-Kokitsi Maninis, Sergi Caelles, Jordi Pont-Tuset, Luc Van Gool</div><div class="conference">CVPR2018, arXiv:1711.09081</div><div class="paper_id">88</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>Extreme pointを用いた物体セマンティックセグメンテーション法．このExtreme pointは，セグメンテーションの上端，下端，右端，左端を使用している．
4つのExtreme pointは，物体の大まかな形状の情報を取り込みながらCNNを学習することができる．
Pascal VOC, COCO, DAVIS2016, DAVIS2017, Grabcutで評価し，どのベンチマークにおいても高い性能を示している．
また，セマンティックセグメンテーションのアノテーションツールとして応用できることも示している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/88_overview.png" alt="88_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>使用するネットワークは，ResNet101をBackboneにしたDeepLab-v2である．提案手法のDeep Extreme Cutでは，Extreme pointを有効的に学習するために，点にガウシガウシアンを施してヒートマップを作成し，そのヒートマップを入力画像のチャンネルに追加している．
この学習方法は，様々なタスクのセグメンテーションに有効であり，セマンティックセグメンテーション，動画のセグメンテーション，インスタンスセグメンテーション，インタラクションセグメンテーションに応用することができる．
また，セグメンテーションのアノテーションツールにも応用でき，従来のアノテーションコストを10分の1まで削減できていることを示している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.09081">論文リンク</a></li><li><a href="http://www.vision.ee.ethz.ch/~cvlsegmentation/dextr/">プロジェクト＆コードリンク</a></li></ul></div></div><div class="slide_index">[#495]</div><div class="timestamp">2018.5.3 23:45:06</div></div></section><section id="ID_Detail-Preserving_Pooling_in_Deep_Networks"><div class="paper-abstract"><div class="title">Detail-Preserving Pooling in Deep Networks</div><div class="info"><div class="authors">Faraz Saeedan, Nicolas Weber, Michael Goesele, Stefan Roth</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>徐々にダウンサイジングしながらも詳細な情報は保持するという問題設定を解決するDNN、特に微分可能なプーリング手法であるDetail-Preserving Pooling（DPP）を提案する。同ネットワークでは隠れ層にて徐々にダウンスケールを行う。図にはフローチャートが示されている。このように線形ダウンスケーリングを施した画像に対して、出力が情報量をできる限り失わないように学習できるプーリングを提案することで任意の畳み込みネットに対して性能向上を見込める手法とした。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180503DPP.png" alt="180503DPP"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>データセットにより最良なプーリングの手法が異なるという欠点を解決するべくDPPを提案した。また、グラフィクスの分野にて提案されているDPID（文献31）を参考にして微分可能（学習可能）なプーリング手法を提案した。このようにして作成されたプーリングはあらゆるネットワークに対し有効にフィットし、（max/average poolingなどより）精度向上を保証すると主張した。例として単純にResNet-101のアーキテクチャのプーリングを置き換えてもCIFAR10にてエラー率が下がっている。このように学習可能であり、汎用的に使用できて高精度が期待できるプーリング手法を提案したことが採択された理由であると考える。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>本手法が汎用的に使用できるのであれば、早い段階でDLフレームワーク（e.g. PyTorch, TensorFlow）などに実装されて使用されるかも？実装面の難しさがどの程度あるか次第か。</p><ul><li><a href="https://arxiv.org/abs/1804.04076">論文</a></li><li><a href="www">Project</a></li><li><a href="https://github.com/visinf/dpp">GitHub</a></li></ul></div></div><div class="slide_index">[#496]</div><div class="timestamp">2018.5.3 23:36:27</div></div></section><section id="ID_Learning_a_Single_Convolutional_Super-Resolution_Network_for_Multiple_Degradations"><div class="paper-abstract"><div class="title">Learning a Single Convolutional Super-Resolution Network for Multiple Degradations</div><div class="info"><div class="authors">Kai Zhang, Wangmeng Zuo and Lei Zhang</div><div class="conference">CVPR2018</div></div><div class="slide_editor">kodai nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>従来の単一画像の超解像手法では，低解像度の画像は，高解像度の画像からのバイキュービック的にダウンサンプリングされたものであるという仮定を置いている．そのため，この仮定に従わない場合，性能が低下する．さらに，複数の劣化に対処するスケーラビリティーも欠けている．本論文ではこれらの問題に対処するため，畳み込み超解像ネットーワークに低解像度画像とdegradation map（ブラーカーネルとノイズレベルから作成）を入力する方法を提案している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_a_Single_Convolutional_Super-Resolution_Network_for_Multiple_Degradations.PNG" alt="Learning_a_Single_Convolutional_Super-Resolution_Network_for_Multiple_Degradations.PNG"></p><p><img src="slides/figs/Learning_a_Single_Convolutional_Super-Resolution_Network_for_Multiple_Degradations_1.PNG" alt="Learning_a_Single_Convolutional_Super-Resolution_Network_for_Multiple_Degradations_1.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>畳み込み超解像ネットワークにブラーカーネルやノイズレベルも入力しようとすると，低解像度画像とのサイズの違いによりネットワークの設計が困難になる．本論文では，dimensionality stretcing strategyを導入することによりこの問題を解決した点が新しい．</p><p>劣化されたSet5などのデータセットに対して，従来法や提案手法を適用し，PSNRとSSIMにより評価した結果，提案手法が最も良い結果を示した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.06116.pdf">論文URL</a></li></ul></div></div><div class="slide_index">[#497]</div><div class="timestamp">2018.5.3 15:51:30</div></div></section><section id="ID_Super-FAN_Integrated_facial_landmark_localization_and_super-resolution_of_real-world_low_resolution_faces_in_arbitrary_poses_with_GANs"><div class="paper-abstract"><div class="title">Super-FAN: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs</div><div class="info"><div class="authors">Adrian Bulat, Georgios Tzimiropoulos</div><div class="conference">CVPR2018 SPOTLIGHT</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>任意の向きの低解像度顔画像に対して超解像度化する研究。生成された超解像度画像に対してランドマーク推定を同時に行うことで画像の精度が良くなることを主張。顔画像の高解像度化の際にランドマークを特定することは有用であることはすでに示されていたが、低解像度かつ任意の顔向きの際にはランドマークを使用して高解像度化することが難しかった。提案手法ではGANによって低解像度顔画像から超解像度化された顔画像を生成し、生成された顔画像に対してランドマークのヒートマップを推定を推定することでネットワークの学習を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180503Super-FAN.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜCVPRに通ったか？</h1><ul><li>解像度はそれぞれ入力画像が16x16、出力画像が64x64</li><li>生成された顔画像の評価指標としてPSNR、SSIMを、ランドマーク推定の評価指標としてAUCを使用し、 顔向きが30・60・90度の顔画像に対してどちらも既存研究より良い顔画像を生成することが可能となった。</li><li>トレーニングの際に複数のロス関数を提案しているが、各ロス関数ごとの結果に関しても議論を行っている。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.02765">論文</a></li></ul></div></div><div class="slide_index">[#498]</div></div></section><section id="ID_Image_Correction_via_Deep_Reciprocating_HDR_Transfromation"><div class="paper-abstract"><div class="title">Image Correction via Deep Reciprocating HDR Transfromation</div><div class="info"><div class="authors">Xin Yang, Ke Xu, Yibing Song, Qiang Zhang, Xiaopeng Wei, Rynson W.H.Lau</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>入力されたLDR画像に対する露光量の調節をend-to-endに行う研究。２つのU-Netを使用し、LDR画像からHDR画像の推定と、推定されたHDR画像からLDRドメインへの変換、という２つ学習によって実現する。LDR画像に内包されている問題として、露光量が少ない箇所ではピクセルが黒く塗りつぶされてしまい、実際のシーンにおける色の推定が難しいという問題がある。そこで、LDR画像から一度HDR画像を生成することで、塗りつぶされた領域を修復する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180503Image_Correction_via_Deep_Reciprocating_HDR_Transfromation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜCVPRに通ったか？</h1><ul><li>入力LDR画像の露光量が多い部分や少ない部分に対しても適切な画像修復が可能となった。</li><li>同様の問題を扱う最新手法と比較した結果、提案手法優位な結果となった。主な理由としてはHDR画像からLDR画像へ変換する際に画像の局所的な詳細情報を保てていることをあげている。</li><li>定量評価として画像の質を表す数値であるPSNR、SSIM、FSIM、Q-scoeによる評価を行った。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.04371">論文</a></li><li><a href="https://ybsong00.github.io/cvpr18_imgcorrect/index">Project page</a></li></ul></div></div><div class="slide_index">[#499]</div></div></section><section id="ID_Visual_Question_Answering_with_Memory_Augmented_Networks"><div class="paper-abstract"><div class="title">Visual Question Answering with Memory-Augmented Networks</div><div class="info"><div class="authors">Chao Ma, Chunhua Shen, Anthony Dick, Qi Wu, Peng Wang, Anton van den Hengel, Ian Reid</div><div class="conference">CVPR2018, arXive: 1707.04968</div><div class="paper_id">875</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>学習サンプルに少ないような質問に対しても回答ができるような手法を提案．ベースはMemory-Augmented Network (One-shot learningを導入したMemory Network)であり，記憶ブロックとAttentionの機能により，稀に発生する質問に対しても正確に回答をすることができる．
VQA benchmark datasetとCOCOのVQAタスクで評価し，高い性能を示している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/875_overview.png" alt="875_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>この手法の大まかな構造はMemory-Augmented Networkになっており，特徴抽出部分が質問文と画像特徴である．画像特徴はVGGやResNetの特徴マップを使用しており，質問文はLSTMの特徴ベクトルを使用している．
この2つの特徴ベクトルは結合され，質問と画像特徴の2つのAttentionがそれぞれ与えられてAugmented memoryに格納される．
そして，Augmented memoryを用いて最終的な回答が出力される．
提案手法では，右下図のように，稀に存在する困難な質問に対しても正確な回答を得ることができる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1707.04968">論文リンク</a></li></ul></div></div><div class="slide_index">[#500]</div><div class="timestamp">2018.5.2 14:29:29</div></div></section><section id="ID_Deep_Layer_Aggregation"><div class="paper-abstract"><div class="title">Deep Layer Aggregation</div><div class="info"><div class="authors">Fisher Yu, Dequan Wang, Evan Shelhamer, Trevor Darrell</div><div class="conference">CVPR2018, arXive: 1707.06484</div><div class="paper_id">272</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>Deep Neural Networkにおける，層間の結合に関して様々な検討を行った論文．従来のネットワーク(ResNet, DenseNet, FCN, U-Net等)のスキップ結合は，”浅い”結合しか適用されていなかった．
この論文では，より”深い” 結合をネットワークに取り入れ，少パラメータかつ高精度なネットワークモデルを構築している．
画像分類をはじめ，様々な認識タスクで実験を行い，高精度化を実現している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/272_overview.png" alt="272_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>この論文では，右図のような4つのモデルを検討している(c~f)．(c)のようにシンプルに特定の層を集約して連鎖的に入力していくモデルから，(d~f)のように様々な層を集約して連鎖的に集約していくモデルを検討しており，上位層と下位層の層を効率的に伝播することで，認識精度を向上させている．
また，(c)と(f)のモデルを組み合わせることで，より性能を向上させることも可能である．
画像分類，Fine-grained Recognition，物体検出，セマンティックセグメンテーションで実験を行っており，全ての認識タスクにおいて高い性能を示している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Deep CNNの次期モデルを検討しているような論文．結局，画像分類，検出，セグメンテーションではスキップ結合が重要であることを再確認できる．</p><ul><li><a href="https://arxiv.org/abs/1707.06484">論文リンク</a></li></ul></div></div><div class="slide_index">[#501]</div><div class="timestamp">2018.5.2 14:05:11</div></div></section><section id="ID_Data_Distillation_Towards_Omni_Supervised_Learning"><div class="paper-abstract"><div class="title">Data Distillation: Towards Omni-Supervised Learning</div><div class="info"><div class="authors">Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, Kaiming He</div><div class="conference">CVPR2018, arXive: 1712.04440</div><div class="paper_id">536</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>ラベル付きとラベルなしデータを用いることで画像認識の精度を向上させるData Distillationを提案．この手法では，self-trainingとHinton先生のKnowledge distributionをベースに提案されている．
この手法は，インターネット上のラベルなしデータを大量に学習できる．
この論文では，Mask R-CNNによる人のKeypoint検出と，FPNをbackboneにしたFaster R-CNNによる物体検出で高精度化を実現している．
(COCOをラベル付き，Sports-1M statistic framesとCOCO2017unlabel imagesをラベルなしデータとして使用．)</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/536_overview.png" alt="536_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>一般的なラベルなしデータを扱うModel Distillationとは異なり，Data Distillationは1つのteacher modelとstudent modelを用いる．構造としては，1つの画像を複数の単純な変形を加え，それぞれの認識結果を得る．
そして，それぞれの認識結果を統合し，統合した認識結果をラベルとしてstudent modelを学習する．
ここで，学習に使用するラベルは”soft”なラベルではなく，”hard”なラベル．COCOをベースに実験をしており，ラベルなしデータを併用することで人のKeypoint検出と物体検出で高精度化を実現している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>シンプルかつ少量データの学習にも応用できるできるので，今後これをベースにした手法が増えそう．</p><ul><li><a href="https://arxiv.org/abs/1712.04440">論文リンク</a></li></ul></div></div><div class="slide_index">[#502]</div><div class="timestamp">2018.5.2 14:10:01</div></div></section><section id="ID_Actor_and_Observer_Joint_Modeling_of_First_and_Third-Person_Videos"><div class="paper-abstract"><div class="title">Actor and Observer: Joint Modeling of First and Third-Person Videos</div><div class="info"><div class="authors">Gunnar A. Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, Karteek Alahari</div><div class="conference">CVPR 2018 (spotlight)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>一人称（First Person View; 頭部にカメラを装着して撮影）かつ三人称（Third Person View; 環境に設置したカメラから撮影）の視点から人物行動や操作している物体を撮影したデータセットCharades-Egoを提供する。一人称/三人称視点は互いに対応付けされており、実に157の行動カテゴリ、112人の実演、4,000の動画ペア、全8,000動画を保有するデータベースの構築に成功した。手法の側面ではTripletによる弱教師付き学習（Weakly-supervised Learning）により一人称/三人称から抽出した複数の特徴量を評価する枠組みActorObserverNetを提案する。さらには、三人称から一人称視点への知識転換（Transferring Knowledge）をZero-shot行動認識の枠組みで実行する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180503CharadesEgo.png" alt="180503CharadesEgo"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>一人称/三人称は従来独立に撮影されて、それぞれのデータベースを構築して来たが、ここでは同時解析することにより行動に関するより詳細な考察（e.g. 間接的に行動を観察した方が良い vs. 操作している物体で行動を認識する方が良い）を行えるようにした。また、弱教師付き学習により特徴学習できるActorObserverNetを提案した。CVPRに通った理由はなんといってもデータベース（とそのベンチマーキング）、弱教師付き学習によるものである。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Hollywood in HomesのようにAMT（クラウドソーシング）にてユーザがフリーで使用を許可した動画を収集するのはアリにしている。公開してフリーにしても良い人だけの動画を効率良く集める仕組みが今後流行ってくるか？（ただ日本だと難しいかも？）データベースに対するベンチマーキングは若干少ない印象を受けるが、データベースの意義自体が優れているため査読を突破したと思われる。</p><ul><li><a href="https://arxiv.org/abs/1804.09627">論文</a></li><li><a href="https://www.youtube.com/watch?v=JkBFE2pzJkw&amp;feature=youtu.be">YouTube</a></li><li><a href="http://www.cs.cmu.edu/~gsigurds/">著者</a></li><li><a href="http://allenai.org/plato/charades/">Project/Database</a></li><li><a href="https://github.com/gsig/charades-algorithms">GitHub</a></li></ul></div></div><div class="slide_index">[#503]</div><div class="timestamp">2018.5.3 02:45:18</div></div></section><section id="ID_The_Best_of_Both_Worlds_Combining_CNNs_and_Geometric_Constraints_for_Hierarchical_Motion_Segmentation"><div class="paper-abstract"><div class="title">The Best of Both Worlds: Combining CNNs and Geometric Constraints for Hierarchical Motion Segmentation</div><div class="info"><div class="authors">Pia Bideau et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>モーションセグメンテーションの問題を扱う。従来のモーションセグメンテーションは幾何的制約を設けることで効果的に動作をセグメントして来たが、高次なセグメントに失敗していた。一方でCNNについては従来方とは逆の特性があった。この両者の特性を活かして、両者にとって良いところどり（The Best of Both Worlds）することでモーションセグメンテーションの性能を向上させた。手法は図に示すようにオプティカルフローを用いた剛体の動き推定（Perspective Projection Constraints）、変形可能でより複雑な物体形状を推定できるようCNNによるセマンティックセグメンテーションを実行。物体のモーションモデルを形成するために、SharpMask（論文中文献35）による物体候補も導入し物体に関する知識を導入した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180503MotionSegmentation.png" alt="180503MotionSegmentation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>クラシカルなフローによる剛体モーション推定とCNNによる物体セグメンテーションを統合、両者の良い部分を引き出しているところが評価に値した。アブストラクト/図１が非常にわかりやすくこの２つで問題設定を把握できるところもグッド。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://vis-www.cs.umass.edu/motionSegmentation/website_CVPR18/cvpr18-bideau.pdf">論文</a></li><li><a href="http://vis-www.cs.umass.edu/">UMASS CV Lab.</a></li><li><a href="http://vis-www.cs.umass.edu/motionSegmentation/website_CVPR18/cvpr18-bideau-suppl.pdf">SupplementaryMaterial</a></li></ul></div></div><div class="slide_index">[#504]</div><div class="timestamp">2018.5.3 01:36:43</div></div></section><section id="ID_Regularizing_RNNs_for_Caption_Generation_by_Reconstructing_The_Past_with_The_Present"><div class="paper-abstract"><div class="title">Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present</div><div class="info"><div class="authors">Xi.Cheny, L.Mazx, W.Jiangzx, J.Yaoy and W.Liuz</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1803.11439</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>encorder/decorderモデルにhiden stateと過去のhiden stateを再構成することによって隣接するhiden stateの接続を強化するためのARNetを導入．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Regulari zing_RNNs_for_Caption_Generation_by_Reconstructing_The_Past_with_The_Present.png" alt="概要図"></p></div></div><div class="item3"><div class="text"><h1>従来手法問題点</h1><ul><li>従来のRNNのtrainとinferenceの間にはexposure biasと呼ばれる相違が存在する．</li><li>decorderはの入力に依存する演算子を用いて，キャプション生成する．</li></ul></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><ul><li>RNNにおけるtransition dynamicsの正則化を助け，シーケンス予測の不一致の緩和が見られた．</li><li>ソースコードキャプション，イメージキャプションの両方で精度の向上が見られた．</li><li><a href="https://arxiv.org/pdf/1803.11439.pdf">Paper</a></li><li><a href="https://github.com/chenxinpeng/ARNet">github</a></li></ul></div></div><div class="slide_index">[#505]</div><div class="timestamp">2018.5.2 23:09:07</div></div></section><section id="ID_Repulsion_Loss_Detecting_Pedestrian_in_a_Crowd"><div class="paper-abstract"><div class="title">Repulsion Loss : Detecting Pedestrian in a Crowd</div><div class="info"><div class="authors">Xinlong Wang, Tete Xiau, Yuning Jiang, Shuai Shao, Jian Sun and Chunhua Shen</div><div class="conference">CVPR2018, arXive:1711.07752</div><div class="paper_id">1005</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>群衆に頑健な歩行者検出法を提案．Faster R-CNNで群衆を検出したとき，歩行者同士の間にBounding Boxが出現しやすい．
これは，Bounding Box回帰の誤差を算出する時に誤差を最小にしようとして歩行者同士の間にBounding Boxが発生してしまう．
この現象を解決するために，新たにRepulsion Lossを導入し，群衆に対しても高精度な歩行者検出を実現している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1005_overview_repLoss.png" alt="1005_overview_repLoss.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Repulsion Lossの中身は， L1 smooth lossをベースにしたL_RepGTとL_RepBoxから構成されている．L_RepGTは，targetの歩行者付近から最も近いGTとの誤差を示しており，targetと最も近いGTにBounding Boxが検出されると誤差が大きくなるように誤差が設計されている．
L_RepBoxは，複数のBounding Boxが特定の箇所に集中するように誤差を設定している．
L_RepBoxの目的は，NMSの割合の影響を減らすためである．
歩行者検出のCaltech, CityPerson(Cityscape)でstate-of-the-artな性能を出しており，Pascal VOCにおいても有効であることを示している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>歩行者検出のベンチマークにおいて非常に高い性能を示しており，ResNetベースのFaster R-CNNに対してDilated Conv.を導入する等のちょっとしたテクニックも色々導入されている．</p><ul><li><a href="https://arxiv.org/abs/1711.07752">論文リンク</a></li></ul></div></div><div class="slide_index">[#506]</div><div class="timestamp">2018.5.2 12:15:00</div></div></section><section id="ID_PackNet_Adding_Multiple_Tasks_to_a_Single_Network_by_Iterative_Pruning"><div class="paper-abstract"><div class="title">PackNet : Adding Multiple Tasks to a Single Network by Iterative Pruning</div><div class="info"><div class="authors">Arun Mallya, Svetlana Lazebnik</div><div class="conference">CVPR2018, arXive:1711.05769</div><div class="paper_id">1004</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>複数のデータセットを1つのネットワークで学習する場合，通常は過去に学習したデータセットは段々と精度が低下していく．これは，全てのパラメータに対して更新するため，過去に学習したデータセットの特徴を抽出できなくなっていくのが原因である．
この論文で着目していることは，大規模なネットワークは特定のパラメータは学習をサボる傾向があるところであり，このサボっているパラメータを使って効率よく学習させて複数のデータセットを学習させている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1004_overview_packnet.png" alt="1004_overview_packnet.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>手法自体は非常にシンプルであり，特定のパラメータをプルーリング(右上図の白領域)して再学習する．そして，プルーリングしたパラメータのプルーリングを解放してパラメータをアップデートする．
特定のタスク(データセット)を学習した後は同じ要領でまたプルーリングと再学習を行う．
特定のパラメータを特定のタスクに割り当てるような学習をすることで，複数タスクに対応している．
結果としては，右図のようにタスクが追加されても性能がほとんど低下していない．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>単純な手法でありながら，非常に強力な手法．図2のインパクトがすごかった．様々な応用にも繋げれそう(Transfer Learning, Domain Adaptation等)</p><ul><li><a href="https://arxiv.org/abs/1711.05769">論文リンク</a></li><li><a href="https://github.com/arunmallya/packnet">コードリンク</a></li></ul></div></div><div class="slide_index">[#507]</div><div class="timestamp">2018.5.2 13:23:59</div></div></section><section id="ID_Tell_Me_Where_to_Look_Guided_Attention_Inference_Network"><div class="paper-abstract"><div class="title">Tell Me Where to Look : Guided Attention Inference Network</div><div class="info"><div class="authors">Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, Yun Fu</div><div class="conference">CVPR2018, arXive: 1802.10171</div><div class="paper_id">1247</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>弱教師あり学習で得られる物体のローカライゼーションを高精度にする研究．方法としては2つ提案しており，</p><ol><li>GAPのローカライゼーションを用いて物体の領域と背景の領域を明示的に学習させる方法と，</li><li>セマンティックセグメンテーションのラベルを用いて物体の詳細な領域を学習させる方法がある．セマンティックセグメンテーションと視覚的解釈に対する評価をしており，どちらのタスクも高い性能を示している．</li></ol></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1247_overview.png" alt="1247_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>1)の方法では，2streamなCNNをベースにしており，入力はそれぞれ通常の画像と，GAPのローカライゼーションから物体領域を排除した画像を入力する．この処理により，物体と背景を明示的に学習できる．
そして，セマンティックセグメンテーションでは，
1)のネットワークに加えて，セマンティックセグメンテーションのラベルと出力したAttention mapとの誤差を算出させることで，Attention mapを最適化させる．
Pascal VOCのweakly-supervisedによるセマンティックセグメンテーションのタスクで評価し，高い性能を示している．
また，発生するAttention mapの領域に対してオリジナルのデータセットを作成して評価している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.10171">論文リンク</a></li></ul></div></div><div class="slide_index">[#508]</div><div class="timestamp">2018.5.2 13:37:25</div></div></section><section id="ID_Beyond_Trade_off_Accelerate_FCN_based_Face_Detector_with_Higher_Accuracy"><div class="paper-abstract"><div class="title">Beyond Trade-off: Accelerate FCN-based Face Detector with Higher Accuracy</div><div class="info"><div class="authors">Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, Yun Fu</div><div class="conference">CVPR2018, arXive: 1802.10171</div><div class="paper_id">1003</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>一般的な顔(物体)検出法(Faster R-CNN, FPN, SSD, YOLO等)は，Backboneな部分がFCNベースで構築されているため，各ピクセルを密に畳み込んで検出結果を出力する．しかし，顔検出では背景領域を大量に含んでおり，検出に必要な領域はごく僅かである．
本論文では，顔検出を効率化するために，2つのAttentionを適応して高速化を試みており，左上図のように高い性能を維持しつつ，4倍以上の高速化を実現している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1003_overview.png" alt="1003_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>本手法で適応しているAttentionは，右上図のようなspatial attentionとscale attentionである．spatial attentionは2次元上における顔の位置を示しており，scale attentionは出力されたスケールピラミッドから最適な特徴マップをAttentionで表現している．
spatial attentionは2次元の位置のattentionから探索する領域を制限するために使用し，scale attentionは探索するスケールピラミッドを制限するために使用する．
ネットワークは下図のようになっており，2つのAttentionにより背景と判定された領域は，マスクされた状態で後段のMask FCNに入力される．
AFW, FDDB, MALFでstate-of-the-artな性能かつ，高速な検出が可能(最速で14.2ms)．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Attentionを計算コスト削減に適応した物体検出法．顔検出や車載系の物体検出等の背景領域を多く含む問題設定では非常に効果的に使えそうな手法．
(COCO, VOCではあまりコストに対しては言及していない)</p><ul><li><a href="https://arxiv.org/abs/1804.05197">論文リンク</a></li></ul></div></div><div class="slide_index">[#509]</div><div class="timestamp">2018.5.2 13:55:50</div></div></section><section id="ID_Deep_Marching_Cubes_Learning_Explicit_Surface_Representations"><div class="paper-abstract"><div class="title">Deep Marching Cubes: Learning Explicit Surface Representations</div><div class="info"><div class="authors">Y. Liao, S. Donné and A. Geiger</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>既存の学習ベースの3D面推定方法は，End-to-Endでの学習ができないが，本研究では，end-to-endでの学習を可能にした．3D面推定手法の一つのマーチングキューブは微分不可．そこで，代替の微分可能定式化を行い，これを3DNNの最終層として追加する．
また，疎な点群で学習が行えるようにロス関数群を提案．
サブボクセル精度での3D形状を推定可能であることを確認した．
本モデルは形状エンコーダ・推論と組み合わせられる柔軟さがある．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Marching_Cubes_Learning_Explicit_Surface_Representations_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>End-to-endで行われたものはない．適用範囲が広そう．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://www.cvlibs.net/publications/Liao2018CVPR.pdf">論文</a></li></ul></div></div><div class="slide_index">[#510]</div><div class="timestamp">2018.5.2 14:41:51</div></div></section><section id="ID_Convolutional_Image_Captioning"><div class="paper-abstract"><div class="title">Convolutional Image Captioning</div><div class="info"><div class="authors">J.Aneja, A.Deshpande and A.Schwing</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1711.09151v1</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>近年，条件付き画像生成や機械翻訳において畳み込みニューラルネットの功績は大きい，これを画像キャプションに応用してみた．ベースラインであるLSTMモデルと同等の精度を示し，パラメータ数ごとの学習時間の短縮をすることができた．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Convolutional_Image_Captioning.png" alt="概要図"></p></div></div><div class="item3"><div class="text"><h1>従来手法の問題提起</h1><p> <ul><li>RNNは学習プロセスが逐次的</li><li>LSTM，RNNは画像の分類精度が低い</li></ul></p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p><ul><li>RNNとCNNのアプローチを分析し，CNNを用いたアプローチは出力確率分布のエントロピーの増大，単語予測精度の向上，消失勾配の影響の低下を示すことができた．</li><li><a href="https://arxiv.org/pdf/1711.09151.pdf">論文</a></li><li><a href="https://github.com/aditya12agd5/convcap">github</a></li></ul></p></div></div><div class="slide_index">[#511]</div><div class="timestamp">2018.5.1 18:06:38</div></div></section><section id="ID_Are_You_Talking_to_Me_Reasoned_Visual_Dialog_Generation_through_Adversarial_Learning"><div class="paper-abstract"><div class="title">Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning</div><div class="info"><div class="authors">Qi Wu, Peng Wang, Chunhua Shen, Ian Reid, Anton Van den Hengel</div><div class="conference">CVPR 2018</div><div class="paper_id">741</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・強化学習とGANを用いたVisual Dialog回答文を自動生成する手法の提案．・従来のVisual Dialogシステムは画像とDialog履歴に基づきMLEにより回答文の予測を行う．こういった手法では回答文が短い，バリエーションが少ないなどの問題点がある．そこで， co-attentionを利用したジョイントで画像， Dialog履歴をreasonできる回答文生成器を提案した．提案モデルはsequential co-attention生成器と回答文が“human”からか“生成された”かを弁別できる弁別で構成される．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Generate_visual_dialog.png" alt="Generate_visual_dialog"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・GANを用いた提案手法はVisual Dialogタスク従来の学習データの不足，簡潔な回答しか生成できないなどの問題点を改善した．・attentionをGANと組み合わせ， 生成回答文のinterpretabilityを向上した
・ VisDial データセットにおいて,従来の手法より高い精度を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・interactive環境でVisual Dialog回答文の生成ができたら更に様々な場面で応用できる</p><ul><li><a href="https://arxiv.org/pdf/1711.07613.pdf">論文</a></li></ul></div></div><div class="slide_index">[#512]</div><div class="timestamp">2018.5.2 13:13:20</div></div></section><section id="ID_Density_Adaptive_Point_Set_Registration"><div class="paper-abstract"><div class="title">Density Adaptive Point Set Registration</div><div class="info"><div class="authors">Felix Järemo Lawin, Martin Danelljan, Fahad Khan, Per-Erik Forssen, Michael Felsberg</div><div class="conference">CVPR 2018</div><div class="paper_id">464</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・ 三次元センサーにより取得したPoint Set の密度の変動を対応できるPoint Set Registrationの手法を提案した．・従来の三次元センサー(例Lidar)により取得できるPoint Setの密度が均一ではない，一方，従来の確率的Point Set Registrationの手法は高密度の部分を対応させ，低密度の箇所の対応が重視されない問題点がある．提案手法はシーン構造の確率分布をモデリングすることにより，密度の変化にロバストに対応できる．
・提案手法は3次元シーンの構造及びフレーム間のカメラ移動量を同時にモデリングし， EMベースなフレームワークに基づきKL divergenceを最小化によりパラメータの最適化を行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Density-Adaptive-Point-Set-Registration.png" alt="Density-Adaptive-Point-Set-Registration"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・Lidarを用いたregistrationシステムのPoint Setの密度変化をロバストで対応できた．・ DAR-ideal、 VPS and TLS ETH datasetsなどのLidarデータセットで従来の確率的マルチビューRegistration手法より良い性能を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・deep learningを用いていない手法</p><ul><li><a href="https://arxiv.org/pdf/1804.01495.pdf">論文</a></li></ul></div></div><div class="slide_index">[#513]</div><div class="timestamp">2018.5.2 10:39:57</div></div></section><section id="ID_pOSE_Pseudo_Object_Space_Error_for_Initialization-Free_Bundle_Adjustment"><div class="paper-abstract"><div class="title">pOSE: Pseudo Object Space Error for Initialization-Free Bundle Adjustment</div><div class="info"><div class="authors">J. Hong and C. Zach</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>カメラ姿勢推定，3次元復元に使われるバンドル調整では，適した初期値を与える必要があるが，初期値を与える必要を無くす提案をする．</p><p>アフィンバンドル調整問題においては，任意の初期化から到達可能な使いやすいminimaがあることが知られているが，その主な要因は，収束のワイドな領域を持つことで知られているVariable Projection（VarPro）法の導入によるものである．本研究ではPseudo Object Space Error（pOSE）を提案する．これは，アフィンと射影のモデルのハイブリッドで表現される複数カメラにおける目的関数である．
この定式化で，VarPro法に適したバイリニア問題構造となり，真の射影復元と近い3D復元結果を得られる．
実験では，ランダムな初期化から高い成功率で正しい3D復元を得られることを確認した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/pOSE_Pseudo_Object_Space_Error_for_Initialization-Free_Bundle_Adjustment_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ランダム初期値でもメトリックの正しい3D復元が行える．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://github.com/jhh37/pose/blob/master/Documents/hong_and_zach_cvpr18.pdf">論文</a></li></ul></div></div><div class="slide_index">[#514]</div><div class="timestamp">2018.5.2 10:31:48</div></div></section><section id="ID_Finding_Tiny_Faces_in_the_Wild_with_Generative_Adversarial_Network"><div class="paper-abstract"><div class="title">Finding Tiny Faces in the Wild with Generative Adversarial Network</div><div class="info"><div class="authors">Yancheng Bai, Yongqiang Zhang, Mingli Ding, Bernard Ghanem</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>GANを用いて画像中の顔を検出する研究。検出が難しい顔として小さくかつボケている顔が挙げられるが、これらの顔をGANによって高解像度かつはっきりとした顔にすることで検出精度を向上させる手法を提案。
generatorは高解像度にするsuper resolution network(SRN)と顔の詳細な情報を復元するrefinment network(RN)を結合したネットワークである。
discriminatorはVGG19であり、ロスとしてデータセットの顔/generatorによる顔、顔/顔ではないモノを同時に行うロスを導入。
またよりはっきりとした顔を生成するために、generatorのロスとして物体識別のロスを導入。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Finding_Tiny_Faces_in_the_Wild_with_Generative_Adversarial_Network.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜCVPRに通ったか？</h1><ul><li>GANによって画像中の顔から高解像度かつはっきりとした顔を生成することで高精度な顔検出手法を提案。</li><li>GANの導入による精度の向上、導入したロスの有効性を確認している。</li><li>state-of-the-artと比較して、最も高い検出精度を達成</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>検出精度が非常に高く、データセットではアノテーションし忘れている顔すらも検出してしまい、これによって精度が悪いように見えてしまうと主張している。</li><li>テスト時も学習時と同様に画像全体ではなくROIを与えているため、実行時間はそれなりにかかりそう。</li><li><a href="https://ivul.kaust.edu.sa/Documents/Publications/2018/Finding%20Tiny%20Faces%20in%20the%20Wild%20with%20Generative%20Adversarial%20Network.pdf">論文</a></li><li><a href="https://ivul.kaust.edu.sa/Pages/pub-tiny-faces.aspx">Project page</a></li></ul></div></div><div class="slide_index">[#515]</div></div></section><section id="ID_Context_Encoding_for_Semantic_Segmentation"><div class="paper-abstract"><div class="title">Context Encoding for Semantic Segmentation</div><div class="info"><div class="authors">Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, Amit Agrawal</div><div class="conference">CVPR 2018</div><div class="paper_id">893</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・コンテキスト情報の抽出を利用したセマンティックセグメンテーションの効率を上げられるContext Encoding Moduleを提案した．・従来の階層式シーンの高レベルから低レベル特徴の抽出を行うネットワーク(eg. PSPNet)にはシーンのコンテキスト情報の抽出がexplicitではない問題点があり，従来のグローバル特徴抽出ネットワークの知識から，シーンのコンテキスト情報を抽出することにより，セマンティックセグメンテーションの効率を上げられるモジュールを提案した．
・具体的には：Encodingによりシーンのコンテキスト情報をキャプチャーし，クラス依存の特徴マップを選択的に強調表示できるContext Encoding Moduleを提案した； Semantic Encoding Loss (SE-loss)を提案した； Context Encoding Moduleを利用したセマンティックセグメンテーションネットワークEncNetを提案した</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Context-Encoding-for-Semantic-Segmentation.png" alt="Context-Encoding-for-Semantic-Segmentation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・ PASCAL VOC 2012において85.9% mIoUを達成した・提案ネットワークをCIFAR-10 datasetに応用し，14層だけのネットワークで100層超えのネットワークと同じレベルの精度を実現した</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・シンプルなネットワークでstate-of-the-artな精度を実現したので，将来的に広く用いられそう</p><ul><li><a href="https://arxiv.org/pdf/1803.08904.pdf">論文</a></li></ul></div></div><div class="slide_index">[#516]</div><div class="timestamp">2018.5.1 17:39:13</div></div></section><section id="ID_Video_Based_Reconstruction_of_3D_People_Models"><div class="paper-abstract"><div class="title">Video Based Reconstruction of 3D People Models</div><div class="info"><div class="authors">Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, Gerard Pons-Moll</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Shusuke Shigenaka</div><div class="item1"><div class="text"><h1>概要</h1><p>人間が動いている単眼のRGB映像から、正確な3次元物体モデルと任意の人物テクスチャを得る研究。仮想現実や拡張現実、監視やゲームなどの人間の追跡にはアニメーション可能な人間行動の3Dモデルが必要である。この研究では、動的な人間のシルエットに対応するシルエット形状を見つけ出し、テクスチャや骨格を推定して、アニメーション可能なデジタルダブルを作成することができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1803.04758_1.png" alt="1803.04758_1"><img src="slides/figs/1803.04758_2.png" alt="1803.04758_2"></p></div></div><div class="item3"><div class="text"><h1>手法・新規性・結果</h1><p>(a). SMPLモデルを用いてポーズを計算(b). シルエットの赤で描かれていないシルエットを取り除く
(c). 正規のTポーズで被写体の形状を最適化
(d). ティクスチャを計算しパーソナライズされた好みの形状を生成
・単眼のRGBビデオから髪や衣服を含む現実的なアバターを抽出
・被服を含む4.5mmの精度で人体形状を再構成</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p><a href="http://link.com/link1/">link</a></p></div></div><div class="slide_index">[#517]</div><div class="timestamp">2018.5.1 16:26:45</div></div></section><section id="ID_Relation_Networks_for_Object_Detection"><div class="paper-abstract"><div class="title">Relation Networks for Object Detection</div><div class="info"><div class="authors">Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei</div><div class="conference">CVPR 2018</div><div class="paper_id">439</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・マルチオブジェクトのアピアランス特徴及び幾何情報間の関係を取り扱える，様々なタスク（物体検出，VQAなど）に用いられるObject Relation Moduleを提案した．・最近attentionに関する研究が発展し，著者たちがattentionモジュールがelement間の依頼性を学習できる面から，物体検出に応用できるアテンションモジュールを提案した．
・提案モジュールを物体検出の2つの段階に応用できる：インスタンス認識段階で提案モジュールによりオブジェクト間の関係を習得でき，精度を上げられる；duplicate removal段階で提案モジュールにより有効的に物体領域を抽出できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Relation-Networks-for-Object-Detection.png" alt="Relation-Networks-for-Object-Detection"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・従来の物体検出手法は物体ごとに推定を行い，物体間の関係を利用しない．提案手法はObject Relation Moduleを提案し，物体間の関係を学習することで，物体検出の精度を更に向上した．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・提案モジュールが付加の監督信号不要，既存なネットワークに追加しやすい特徴があるため，様々なタスクでの応用が期待される</p><ul><li><a href="https://arxiv.org/abs/1711.11575">論文</a></li></ul></div></div><div class="slide_index">[#518]</div><div class="timestamp">2018.5.1 16:37:43</div></div></section><section id="ID_PPFNet_Global_Context_Aware_Local_Features_for_Robust_3D_Point_Matching"><div class="paper-abstract"><div class="title">PPFNet: Global Context Aware Local Features for Robust 3D Point Matching</div><div class="info"><div class="authors">Haowen Deng, Tolga Birdal, Slobodan Ilic</div><div class="conference">CVPR2018</div><div class="paper_id">45</div></div><div class="slide_editor">Shuichi Akizuki</div><div class="item1"><div class="text"><h1>概要</h1><p>点群データから直接3Dの局所特徴量を抽出するネットワークを提案．N-Tuple loss(Triplet lossの拡張)によって，
対応点間の特徴量が近く，それ以外の特徴量間の距離が遠くなるような変換を学習する．
PPFNetの入力は局所パッチ内の点の座標，法線，Point Pair Featureをまとめたデータ．
ネットワークの内部ではPointNetを利用する．
大域的な情報を得るために，各パッチから取得した局所特徴量を
Max poolingによって大域特徴量化し，局所特徴と結合する工夫も入れている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/45.png" alt="45"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>局所特徴量を生成するネットワークを構築した点，N-Tuple lossによる学習法を提案した点が新しい．
キーポイントマッチングのベンチマークでRecall rateが向上．
オーバーラップが少ないシーンでのレジストレーションも可能になっている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.02669">Paper</a></li></ul></div></div><div class="slide_index">[#519]</div><div class="timestamp">2018.5.1 15:53:31</div></div></section><section id="ID_Geometry-Aware_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">GAGAN: Geometry-Aware Generative Adversarial Networks</div><div class="info"><div class="authors">Jean Kossaifi, Linh Tran, Yannis Panagakis and Maja Pantic</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>既存のGANでは考慮されていなかった形状や位置といった幾何学的情報をGANの生成プロセスに組み込んだGeometry-Aware Generative Adversarial Networks (GAGAN) を提案．具体的にGAGANでは，ジェネレータで統計的情報な形状モデルの確率空間から潜在関数をサンプリングする．次にジェネレータの出力値を微分可能な幾何学変換を介して標準座標系にマッピングすることで，物体の形状や位置といった情報を強制し，生成を行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180501_gagan.png" alt="Item3Image"><img src="slides/figs/20180501_gagan2.png" alt="Item4Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>GAGANのような幾何学的情報を考慮した生成モデルはなく，GAGANが初</li><li>入力画像の属性の形状に合わせて，画像を生成することが可能</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>今後は，(i)より大きな画像の生成，(ii)アフィン変換によって起こりうる変形を緩和するより複雑な幾何学的変換の探索およびそれによるGAGANの拡張，(iii)顔のランドマーク検出のための従来CNNアーキテクチャの拡張に取り組む予定</p><ul><li><a href="https://arxiv.org/pdf/1712.00684.pdf">論文</a></li></ul></div></div><div class="slide_index">[#520]</div><div class="timestamp">2018.5.1 14:41:53</div></div></section><section id="ID_IQA_Visual_Question_Answering_in_Interactive_Environments"><div class="paper-abstract"><div class="title">IQA: Visual Question Answering in Interactive Environments</div><div class="info"><div class="authors">Daniel Gordon, Ali Farhadi, Aniruddha Kembhavi, Dieter Fox, Mohammad Rastegari, Joe Redmon</div><div class="conference">CVPR2018</div><div class="paper_id">533</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・新たな問題設定ー動的環境とインターアクトしながら視覚質問に答える(IQA)を提案した．・具体的には， IQAには4つの設定がある：環境でナビゲートする能力；環境中のオブジェクト，アクション及びアフォーダンスの理解；環境中のオブジェクトとインターアクトする能力；質問文に応じで環境での行動を計画する能力．
・提案の問題設定を解決するために，階層的マルチレベルで行動計画及びコントロールするネットワークHIMN及び空間的かつセマンティックなメモリを実現できる新たなrecurrent layer形式Egocentric Spatial GRUを提案した．
・更に，75000質問及びCGシーンを含んだデータセットIQUAD V1を提案した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Visual-Question-Answering-in-Interactive-Environments.png" alt="Visual-Question-Answering-in-Interactive-Environments"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・従来のVQAタスクをCGシーンでの自己ナビゲーションと組み合わせた新たな問題設定を提案した．・IQUAD V1で従来の手法よりstate-of-the-artな精度</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・従来のVQAタスクに更に環境での探索および環境中オブジェクトとのインターアクトを取り入れ，従来の問題設定より一層現実に近づいている．・質問文の自動生成にも応用できそう
・特に色々なタスクを取り扱えているので，技術の面では向上する空間がありそう</p><ul><li><a href="https://arxiv.org/pdf/1712.03316.pdf">論文</a></li></ul></div></div><div class="slide_index">[#521]</div><div class="timestamp">2018.5.1 15:29:03</div></div></section><section id="ID_On_the_Robustness_of_Semantic_Segmentation_Models_to_Adversarial_Attacks"><div class="paper-abstract"><div class="title">On the Robustness of Semantic Segmentation Models to Adversarial Attacks</div><div class="info"><div class="authors">Anurag Arnab, Ondrej Miksik and Philip H.S. Torr</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">kodai nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>adversarial attackに対するロバスト性の評価を, semantic segmentationにおいてstate-of-the-artな性能を持つネットワークを用いて実験した.Pascal VOCとCityscapesのデータセットに対して, FGSM, Interative FGSM, FGSM II, Interative FGSM IIで攻撃したときのIoU Ratioによりロバスト性を評価した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/On_the_Robustness_of_Semantic_Segmentation_Models_to_Adversarial_Attacks.PNG" alt="On_the_Robustness_of_Semantic_Segmentation_Models_to_Adversarial_Attacks.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>ResNetをバックボーンに持つネットワークがロバストであることがわかった. 中でもDeeplab v2が最もロバスト.</li><li>multi-scale processingやmean field CRFによりロバストになる.</li><li>画像分類の分野で一般的なロバスト性やモデルサイズについての知識がsemantic segmentationでも有用とは限らない.</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1711.09856.pdf">論文URL</a></li></ul></div></div><div class="slide_index">[#522]</div><div class="timestamp">2018.5.1 14:32:41</div></div></section><section id="ID_CodeSLAM_---_Learning_a_Compact_Optimisable_Representation_for_Dense_Visual_SLAM"><div class="paper-abstract"><div class="title">CodeSLAM --- Learning a Compact, Optimisable Representation for Dense Visual SLAM</div><div class="info"><div class="authors">Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, Andrew Davison</div><div class="conference">CVPR  2018</div><div class="paper_id">288</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・RGB画像の強度データと少数のパラメータを条件に，ほぼリアルタイムで行えるデンスなシーン幾何を推定手法を提案した．・提案手法UNet構造により強度画像の特徴抽出を行い，更に抽出特徴をauto-encoder構造を用いたデプス情報推定ネットワークに入力することで階層的にデプス情報推定を行う．また，カメラ移動中得られるマルチフレームに対し，フレームごとのデプス推定及びフレーム間のカメラモーションをジョイントで最適化を行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/codeSLAM.png" alt="codeSLAM"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・デンスなデプス情報推定を行うことでSLAMシステムの更なる精度向上できると宣言した．・初めてのほぼリアルタイムで行えるカメラモーションとシーンのデンス幾何をジョイントで推定する研究である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・著者たちは将来のワークとして，提案手法をリアルタイムでデンスなSLAMシステムの構築に拡張すると指摘し,将来的な研究を期待している．</p><ul><li><a href="https://arxiv.org/abs/1804.00874">論文</a></li></ul></div></div><div class="slide_index">[#523]</div><div class="timestamp">2018.5.1 14:08:56</div></div></section><section id="ID_Learning_by_asking_questions"><div class="paper-abstract"><div class="title">Learning by asking questions</div><div class="info"><div class="authors">Ishan Misra, Ross Girshick, Rob Fergus, Martial Hebert, Abhinav Gupta, Laurens van der Maaten</div><div class="conference">CVPR 2018</div><div class="paper_id">3</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・VQAタスクに用いられる新たなインターアクティブ学習フレームワークを提案した．・提案フレームワークは入力画像から，question proposal moduleにより問題集を生成し，画像との相関性を基準に問題集をフィルタリングし，残った問題をVQAにより解く．予測した答え，自己の知識及び過去の知識から質問を1つ選び，oracleにより答える．
・提案フレームワークにより，効率高い学習サンプルを得られる．また，従来のVQAネットワークで用いられるstate-of-the-artな問題集を生成できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/learning_by_asking_questions.png" alt="learning_by_asking_questions"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・従来のあらゆるフレームワークは学習データから学習を行う．この論文で，質問文の自動生成できる及び質問を選択する構造を導入し，自動的でインターアクティブで環境から情報を獲得することを可能にした．・実験を通し，提案手法により質問を選択する規制がsampleの効率を高められる．（従来と同じ精度の場合，学習データ量を40％減らせる）</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>real-worldバージョンのLBAシステムが実現されたら，機械で学習することは更に人の学習システムに近づく．</p><ul><li><a href="https://arxiv.org/abs/1712.01238">論文</a></li></ul></div></div><div class="slide_index">[#524]</div><div class="timestamp">2018.5.1 12:10:26</div></div></section><section id="ID_Learning_Spatial-Temporal_Regularized_Correlation_Filters_for_Visual_Tracking"><div class="paper-abstract"><div class="title">Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking</div><div class="info"><div class="authors">Feng Li, Cheng Tian, Wangmeng Zuo, Lei Zhang, Ming-Hsuan Yang</div><div class="conference">CVPR 2018</div><div class="paper_id">1353</div></div><div class="slide_editor"><a href="" target="blank">Takahiro Itazuri</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Spatially Regularized Discriminative Correlation Filters (SRDCF)に空間正則化を導入した一般物体追跡手法Spatial-Temporal Regularized Correlation Filters (STRCF)を提案. SRDCFは複数学習画像を利用するため, 計算量が大きくなってしまうことに着目し, 単一学習画像に対するSRDCFにonline Passive-Aggresive learningの考えに基づいて時間正則化を導入. STRCFはADMMで直接解くことができるため, DCFの高速性を保持したまま高い精度で追跡が可能となっている.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/STRCF.png" alt="STRCF"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>単一学習画像に対するSRDCFに時間正則化を導入することで, 複数学習画像に対するSRDCFを近似したSRTCFを定式化</li><li>online Passive-Aggresive learningを拡張することで, STRCFは大きな見た目の変化に対して頑健である</li><li>SRTCFはADMMを用いて, 3つの部分問題に帰着させ, Eckstein-Bertsekas条件を満たし, 大域的最適解への収束性を保証している</li><li>OTB-2015, Temple-Color, VOT-2016データセットにおいてSRDCFより精度も計算速度も向上させた</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.08679.pdf">論文</a></li><li><a href="https://github.com/lifeng9472/STRCF">コード</a></li></ul></div></div><div class="slide_index">[#525]</div></div></section><section id="ID_Learning_Spatial-Aware_Regressions_for_Visual_Tracking"><div class="paper-abstract"><div class="title">Learning Spatial-Aware Regressions for Visual Tracking</div><div class="info"><div class="authors">Chong Sun, Huchuan Lu, Ming-Hsuan Yang</div><div class="conference">CVPR 2018</div><div class="paper_id">1676</div></div><div class="slide_editor"><a href="" target="blank">Takahiro Itazuri</a></div><div class="item1"><div class="text"><h1>概要</h1><p>一般物体追跡手法の二大手法であるカーネルリッジ回帰（相関フィルタを含む）とCNNのハイブリッドな手法を提案した.カーネルリッジ回帰は全体的な情報に,CNNは局所的な情報に注目するように設計している.それぞれの導入がどの精度向上に結びついているかも検討している.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/LSART.png" alt="LSART"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>cross-patch similarityを用いたカーネルリッジ回帰モデルを提案し,それをニューラルネットに再定式化.</li><li>spatially reguralized kernelとdistance transform pool layerを用いて,出力の各チャンネルが特定の領域に反応するようなCNN提案.</li><li>提案したカーネルリッジ回帰とCNNを相補的に用いることで,OTB-2013,OTB-2015,VOT-2016データセットでstate-of-the-artな精度を達成.		</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="http://lanl.arxiv.org/pdf/1706.07457v1">論文1</a></li><li><a href="https://drive.google.com/file/d/1Lls2CK-yTkeOcOarNGapmcz4h2HQj6gf/edit">論文2</a></li></ul></div></div><div class="slide_index">[#526]</div></div></section><section id="ID_Improved_Fusion_of_Visual_and_Language_Representations_by_Dense_Symmetric_Co-Attention_for_Visual_Question_Answering"><div class="paper-abstract"><div class="title">Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering</div><div class="info"><div class="authors">Nguyen Duy Kien, Takayuki Okatani</div><div class="conference">CVPR 2018</div><div class="paper_id">739</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>VQAタスクに用いられるattentionメカニズム“Dense Co-attention Network”(DCN)を提案した．DCNはfully対称的で，階層的にスタックできるため，マルチステップで視覚及び言語特徴のインターアクションを可能にする．具体的には，まず言語から画像の注目マップ及び画像から言語の注目マップを生成し，そして連結によりマルチモデルの特徴を融合する（dense co-attention layer)．そして階層的にdense co-attention layerをスタックにより，さらにマルチモデル特徴を深く探る．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Co-attention_VQA.png" alt="Co-attention_VQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・従来のattention for VQAタスクより，有効的でデンスな視覚と言語モデルの特徴の融合メカニズムDCN（構造的にも簡潔で拡張しやすい）を提案し，将来の様々なVQAタスクに用いられる．・VQA, VQA2.0データセットで2017 VQA優勝したモデルより良い精度を達成した．
・定性的な実験により，提案モデルが有効的にattentionを抽出できることを証明した</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1804.00775.pdf">論文</a></li></ul></div></div><div class="slide_index">[#527]</div></div></section><section id="ID_Deep_Voting_A_Robust_and_Explainable_Deep_Network_for_Semantic_Part_Detection_under_Partial_Occlusion"><div class="paper-abstract"><div class="title">DeepVoting: A Robust and Explainable Deep Network for Semantic Part Detection under Partial Occlusion</div><div class="info"><div class="authors">Z. Zhang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><div class="text"><h1>概要</h1><p>画像中から物体のパーツ（車のタイヤなど）を検出するための新しい手法を提案．投票ベースの手法でオクルージョンへの頑健性を持つ．
Visual ConceptというMid-levelな特徴をベースにして，
個々のMid-level特徴から推定されるパーツの位置推定結果を積み重ねていくことでパーツを検出する．
Visual Conceptの検出とそれに基づく投票処理はConvolutionによって実装されており，
End-to-Endでの学習が可能になっているところがポイント．
Faster-RCNNといった物体検出アプローチよりもオクルージョンに頑健なことが実験的に確認できている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DeepVoting_A_Robust_and_Explainable_Deep_Network_for_Semantic_Part_Detection_under_Partial_Occlusion..png" alt="DeepVoting_A_Robust_and_Explainable_Deep_Network_for_Semantic_Part_Detection_under_Partial_Occlusion..png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>CNNベースのVotingによるオクルージョンに頑健なパーツ検出手法を提案</li><li>Visual Conceptの検出から投票までConvolutionで実装</li><li>人工的なオクルージョン環境下での有効性を確認</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク</h1><ul><li>投票処理までConvolutionで表現されているのが面白い</li><li><a href="https://arxiv.org/abs/1709.04577">論文</a></li><li><a href="http://lingxixie.com/PDFs/Zhang_CVPR18_DeepVoting_SuppMat.pdf">Supplementary Material</a></li></ul></div></div><div class="slide_index">[#528]</div><div class="timestamp">2018.4.23 06:07:59</div></div></section><section id="ID_Feature_Mapping_for_Learning_Fast_and_Accurate_3D_Pose_Inference_from_Synthetic_Images"><div class="paper-abstract"><div class="title">Feature Mapping for Learning Fast and Accurate 3D Pose Inference from Synthetic Images </div><div class="info"><div class="authors">Mahdi Rad   et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>合成データを利用した、6D pose estimationとdepth based 3D hand pose estimationの研究。</p>埋め込み空間内で、合成データから実データへのマッピング関数を学習する。その関数の学習のためには実データに対応する(grand truthが同じ)合成データが必要であるので、教師あり実データがある程度あることが前提としてある。</div></div><div class="item2"><div class="text"><p><img src="slides/figs/Mahdi.png" height="1000"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>残差構造を持つmapping netを対応するペアを用いて学習する。従来のドメイン適応手法と比較しても提案手法の精度が良く、適応の有無による性能の差も非常に大きい。</p></div></div><div class="item4"><div class="text"><h1>メモ・リンク</h1><p>手法としてはかなりstraight forwardな印象。実データの量を変化させた時の精度変化の結果はあったが、合成データの量を変化させた時の精度変化が気になる。</p><ul><li><a href="https://arxiv.org/pdf/1712.03904.pdf">論文</a></li></ul></div></div><div class="slide_index">[#529]</div></div></section><section id="ID_Embodied_Question_Answering"><div class="paper-abstract"><div class="title">Embodied Question Answering</div><div class="info"><div class="authors">Abhishek Das et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>3次元空間において、エージェントに質問の答え（例：車の色は？）を探させる研究。初期位置における視覚情報だけでは答えに行きつかないためにエージェントは移動しながら答えを探していく。
エージェントの移動には、どの方向（forward, rightなど)に進むかを決定するplannerとどこまで進むかを決定するcontrolerによって行う。
目的地(正解が分かる場所)にたどり着いた時点で、最後の5フレームを用いて172の選択肢から正解を出力する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Embodied_Question_Answering.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>LSTMを使った場合の方が目的地により近付けるという結果が得られた。強化学習なしのものは目的地により近づいている一方、ファインチューニング＋強化学習の方が正解率は高いという結果となった。
また、最短経路を与えてVQAによって答えさせる場合でも精度が悪く、答えを導くにあたってどの方向から目的地に近づくかも重要であるということが分かった。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://embodiedqa.org/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#530]</div><div class="timestamp">2018.4.23 12:59:26</div></div></section><section id="ID_Learning_from_Synthetic_Data_Addressing_Domain_Shift_for_Semantic_Segmentation"><div class="paper-abstract"><div class="title">Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation</div><div class="info"><div class="authors">Sankaranarayanan et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>GANによる画像生成の枠組みを中間的に取り入れることでSemantic segmentationにおけるドメイン適応を行う研究。</p>従来の特徴ベクトルに対する敵対的学習によって埋め込み空間におけるdomain gapを縮める手法に対して、この研究では特徴ベクトルから画像を復元し、その画像が識別器によってどのドメインからの復元か識別できないように埋め込み関数を学習させる。
合成データからのドメイン適応で最も良い精度を達成。</div></div><div class="item2"><div class="text"><p><img src="slides/figs/Swami.png" height="1000"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p> Source(S)は教師ありデータ、Target(T)は教師なしデータ。学習のフローは以下である: 
(1)識別器(D)は入力画像に対してpixel-wiseにsource real(SR), source fake(SF), target real(TR), target fake(TF)の4値分類を学習。(2)生成器(G)は入力特徴ベクトルからDによってSからの特徴はSRに、 Sからの特徴はTRに分類されるよう学習。
(+入力との担保を取るL2Loss)(3)埋め込み関数(F)はSからの入力はTRに、Tからの入力はSRに分類されるように学習。さらにSからのサンプルに対してはFからの特徴マップを入力としてsegmentation taskを解くCNNを学習。</p></div></div><div class="item4"><div class="text"><h1>メモ・リンク</h1><p>論文内にこの手法がうまくいく理由の裏付け的実験や考察が詳細にはなかったが、特徴量から画像再生成を行うことによる入力情報の保存とS/T間の敵対的学習による分布の混合が一つのフローで行えていることが効いているように思えた。実際特徴量に対するS/T間の敵対的学習のみの場合よりも大きく精度が向上している。</p><ul><li><a href="https://arxiv.org/abs/1711.06969">論文</a></li></ul></div></div><div class="slide_index">[#531]</div></div></section><section id="ID_Natural_and_Effective_Obfuscation_by_Head_Inpainting"><div class="paper-abstract"><div class="title">Natural and Effective Obfuscation by Head Inpainting</div><div class="info"><div class="authors">Qianru Sun, Liqian Ma, Seong Joon Oh, Luc Van Gool, Bernt Schiele, Mario Fritz</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>SNSなどで共有された画像には、プライバシー保護の問題が生じる。プライバシー保護のために顔領域にぼかしや黒塗りなどの処理がされることが多いが、画像としては不自然さが残ってしまう。
そこで、塗りつぶされた領域に顔を挿入することで自然な画像ではあるが別人のためプライバシーを保護できる画像を生成する。
提案手法は、特徴点検出（生成）と顔の挿入の2つのステップに分かれる。
特徴点検出（生成）では、オリジナルの顔画像が存在する場合は既存の特徴点検出によって特徴点を検出する。
対称の画像が既に黒塗りされているなどで特徴点検出ができない場合は、GANによって特徴点を生成する。
次のステップでは、黒塗りされている顔画像と特徴点を入力し、黒塗りされた領域に顔の挿入を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Natural_and_Effective_Obfuscation_by_Head_Inpainting.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>特徴点生成器は、GANによって生成することで正解値とのノルム最小化よりも高い精度で生成することを可能にした。画像に対する処理としてぼかしと黒塗りを比較したところ、ぼかしは顔の情報が一部残るため高い精度での生成が可能である一方、元の人物の情報は黒塗りよりも多く残ることが分かった。
また、顔の形状にも個人性が含まれるためオリジナル画像から検出した特徴点よりもGANによって生成した特徴点を使用した方が個人性は損なわれることが分かった。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.09001">論文</a></li></ul></div></div><div class="slide_index">[#532]</div><div class="timestamp">2018.4.18 15:45:44</div></div></section><section id="ID_Augmenting_Crowd-Sourced_3D_Reconstructions_using_Semantic_Detections"><div class="paper-abstract"><div class="title">Augmenting Crowd-Sourced 3D Reconstructions using Semantic Detections</div><div class="info"><div class="authors">T. Price, J. L. Schonberger, Z. Wei, M. Pollefeys and J.M. Frahm</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>SfMにおいて，一つの撮影にしか映らないような移動物体を考慮することで，そのシーンの絶対スケールが推定可能になるし，人混みだと見えにくい地平面の復元も成しうる．個々の撮影画像において検出された人を3次元空間に投影し，さらに物体の意味情報（本稿では背の高さの分布）から絶対スケールを推定する．
また，人検出結果を用いて地平面推定も行う．
ランダムなインターネット画像で手法をデモンストレーションし，量的評価を行う．</p><p>人検出はトルソモデルのフィッティングに基づく．画像における肩，腰の位置が推定でき，おおよその立ち位置も分かるということ．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Augmenting_Crowd-Sourced_3D_Reconstructions_using_Semantic_Detections_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>評価点</h1><p>若干SIGGRAPH的な気風のある，面白い視点を提供する論文．過去の知見に基づく高品質な人検出などを用いて成し得た，正統なアプリケーションに感じる．
動画のインパクトも大きいので，一度視聴を勧める．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://demuc.de/papers/price2018augmenting.pdf">論文</a></li><li><a href="https://www.youtube.com/watch?v=eQWXuPv5eC0">動画</a></li></ul></div></div><div class="slide_index">[#533]</div><div class="timestamp">2018.4.16 17:07:53</div></div></section><section id="ID_Single_View_Stereo_Matching"><div class="paper-abstract"><div class="title">Single View Stereo Matching</div><div class="info"><div class="authors">Yue Luo, Jimmy Ren, Mude Lin, Jiahao Pang, Wenxiu Sun, Hongsheng Li and Liang Lin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">kodai nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>従来の単眼奥行き推定法では, 推論の際に幾何的な制約を明示的に課していないことや多くのground truth labeled dataが必要といった問題があった.この研究では単眼奥行き推定問題をview synthesis問題とstereo matching問題に分けて考えることにより, 従来法の問題を解決する.
view synthesis問題では, 入力を左画像として捉え, view synthesis networkにより右画像を生成する. stereo matching問題では, 左画像を右画像を用いstereo matching networkにより奥行きを推定する.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/single_view_stereo_matching.PNG" alt="single_view_stereo_matching.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>単眼奥行き推定問題をview synthesis問題とstereo matching問題に分けて考えた.</li><li>従来法の問題を解決.</li><li>従来のどの方法よりも精度が高い.</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.02612.pdf">論文URL</a></li></ul></div></div><div class="slide_index">[#534]</div><div class="timestamp">2018.4.30 18:04:11</div></div></section><section id="ID_Learning_Face_Age_Progression_A_Pyramid_Architecture_of_GANs"><div class="paper-abstract"><div class="title">Learning Face Age Progression: A Pyramid Architecture of GANs</div><div class="info"><div class="authors">Hongyu Yang, Di Huang, Yunhong Wang and Anil K. Jain</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>入力画像中の人物の老化顔をGANによって生成する手法の提案。Discriminatorには生成した画像が合成画像であるか及び目標年代の特徴を保持しているかを判定させ、それに加え元の画像とのL2ノルム及び元の顔画像と同一人物であるかをロスに加えることで、同一人物性を保持している。
その際、Discriminatorの中間層の各出力を途中で取り出すことにより（ピラミッド型ネットワーク），様々な解像度からの年齢特徴の抽出を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_Face_Age_Progression_A_Pyramid_Architecture_of_GANs.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>年齢推定及び個人認証タスクによって有効性を確認した。従来手法では髪や額領域は変化できなかったが、提案手法によってこれらの要素を変化させることを可能とした。
Discriminatorをピラミッド型にすることにより、従来手法に比べてより詳細な老化特徴を取り出すことに成功。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.10352">論文</a></li></ul></div></div><div class="slide_index">[#535]</div><div class="timestamp">2018.4.16 16:14:24</div></div></section><section id="ID_Image_Generation_from_Scene_Graphs"><div class="paper-abstract"><div class="title">Image Generation from Scene Graphs</div><div class="info"><div class="authors">Justin Johnson et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>物体同士の関係を表すScene Graphsから画像を生成する手法の提案。従来のテキストから画像を生成する手法よりも物体の数が多く複雑なシーンの画像を生成することができる。
初めに、Scene Graphsを処理するネットワークによってScene Graphsを表現するベクトルを取得し、そこから画像のレイアウトを作成する。
次にレイアウトからCRN(参考文献)を用いて画像を作成する。
作成された画像は、画像全体のリアルさと各物体のリアルさを評価するDiscriminatorによってリアルな画像であるかを評価する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180411graph.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>ユーザースタディの結果、StackGANと比較して合成結果が良いと答えた人が68%、認識可能な物体を生成できてると答えた人が59%という結果が得られた。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.01622">論文</a></li><li><a href="https://arxiv.org/abs/1707.09405">CRN</a></li><li><a href="https://arxiv.org/abs/1612.03242">StackGAN</a></li></ul></div></div><div class="slide_index">[#536]</div><div class="timestamp">2018.4.11 15:58:22</div></div></section><section id="ID_Bottom-Up_and_Top-Down_Attention_for_Image_Captioning_and_Visual_Question_Answering"><div class="paper-abstract"><div class="title">Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</div><div class="info"><div class="authors">Peter Anderson, Xiaodong He, Chris Buehler,Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang</div><div class="conference">CVPR 2018</div><div class="paper_id">738</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>Image captioningとVQAタスクに用いられるBottom-upとtop-down attentionをコンバインするメカニズムを提案した．従来のオブジェクトレベルの領域の抽出のほか，salient 領域の抽出も行う．Faster R-CNNを利用したbottom-up的にsalient 領域を特徴ベクトルを抽出し， top-downにより特徴のウェットを決めることをベースに， Image captioningとVQAのアーキテクチャを提案し（右図），両方ともstate-of-artな性能を得られた．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Bottom_up_Top_down_VQA.png" alt="Bottom_up_Top_down_VQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・従来のVQAとImage captioningは主にタスクスペシフィックなtop-downタイプのattentionを用いる．この論文で，人の視覚attentionメカニズムから，タスクスペシフィックなtop-downタイプのattentionを及びsalient 領域に注目するBottom-upのattentionを用いることと主張した．・2017 VQA Challengeにおいて優勝した．VQA v2.0 test-standardにおいて70.3%の精度を達成した．また， Image captioning タスクに対しMSCOCO Karpathy testで従来の手法より良い性能を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1707.07998.pdf">論文</a></li></ul></div></div><div class="slide_index">[#537]</div><div class="timestamp">2018.4.27 10:27:30</div></div></section><section id="ID_Tips_and_Tricks_for_Visual_Question_Answering_Learning_from_the_2017_Challenge"><div class="paper-abstract"><div class="title">Tips and Tricks for Visual Question Answering: Learning from the 2017 Challenge</div><div class="info"><div class="authors">Damien Teney, Peter Anderson, Xiaodong He, Anton Van den Hengel</div><div class="conference">CVPR 2018</div><div class="paper_id">547</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>2017 VQA Challengeに優勝したモデルのモデル詳細を紹介し，さらにいかにVQAモデルの精度を上げられるかのコツとテクニックを紹介した．モデルのコアなところは視覚と質問文の意味特徴をジョイントでエンベディングし，さらにマルチ-ラベル予測を行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Tip_Tricks_VQA.png" alt="Tip_Tricks_VQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>論文により，VQAの性能上げるために，以下のテクニックがある：1.sigmoid outputsを用いて，マルチアンサーをできるようにする．2．Soft scoresを用いて，分類ではなく回帰を行う．3．Bottom-up attentionから注目領域の画像特徴を用いる．4．Gated tanhを活性化関数に用いる．5．Pre-trainedウェットで初期化する．6．ミニバッチサイズを大きく設定し，training-dataにシャッフリングを用いる</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1708.02711">論文</a></li></ul></div></div><div class="slide_index">[#538]</div><div class="timestamp">2018.4.26 16:58:02</div></div></section><section id="ID_What_Makes_a_Video_a_Video_Analyzing_Temporal_Information_in_Video_Understanding_Models_and_Datasets"><div class="paper-abstract"><div class="title">What Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and Datasets </div><div class="info"><div class="authors">Xiaolong Wang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>「3DCNNが実は動き特徴を捉えられていないのではないか」という考えのもと、3DCNNにおける動き特徴の影響の上界を実験的に求める。提案する工夫により、この影響のかなり低い上界を得ることができ、動き特徴を捉えているのではない(例えば実は複数フレーム入力から「重要なフレーム選択」を行っているなど)ことを示唆した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/What_Makes_a_Video_a_Video_Analyzing_Temporal_Information_in_Video_Understanding_Models_and_Datasets.png"></p></div></div><div class="item3"><div class="text"><h1>検証方法</h1><p> 通常の16frames入力で学習したC3Dにおいてtest時にsub-samplingした(動き情報を無くした)設定下でできるだけ精度を上げることで結果的に動き特徴の上界を得る。Naïveにsub-samplingを行うと入力のデータ分布の明らかな違いから動き以外の精度低下への影響をもたらすと考えられるため、 sub-samplingされたclipから元clipを生成するgeneratorを構築。学習はC3Dの中間層の値をMSEで近づける。
またsampling方法によっても精度は変わるという考えから、識別confidenceが最大となるframesをsamplingする。注意として、この際動きに関しては全く考慮せずにsamplingしてきている。</p></div></div><div class="item4"><div class="text"><h5>コメント・リンク</h5><p>結果として、かなりきつい上界を求められ、論文内では3DCNNが2Dよりも精度が良いのは動き特徴ではなく、複数フレーム入力の中で最も識別しやすいフレームを選択可能になるからではと述べられている。</p><p>フレーム選択をしているという仮説は面白いし、select frameによって精度が上昇したり、動きが大きい動画はフレーム単位での推定結果の分散が大きいなどから十分ありえそう。これが本当なら、optical flowを3dCNNに導入して大きく精度が向上することともつじつまが合いそう。</p><ul><li><a href="http://ai.stanford.edu/~dahuang/papers/cvpr18-fb.pdf">論文</a></li></ul></div></div><div class="slide_index">[#539]</div></div></section><section id="ID_Surface_Networks"><div class="paper-abstract"><div class="title">Surface Networks</div><div class="info"><div class="authors">Ilya Kostrikov, Joan Bruna, Daniele Panozzo, Denis Zorin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>3D triangleメッシュから有用的な三次元幾何情報を抽出するネットワークSurface Networkを提案した．従来のLaplace operatorがintrinsic三次元幾何情報しか抽出できない．しかし，様々な応用場面でextrinsic情報が必要となる．この文章で主要なcurvature方向を抽出できるDirac operator を提案し，従来のLaplace operatorより幅広い場面で応用できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SurfaceNetwork_result.png" alt="SurfaceNetwork_result"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・定性的および定性的な結果によりspatial-temporal predictionsタスクにおいて，従来手法より良い結果を得られている．・variationalエンコーダーを用いたメッシュ合成手法を提案し，有効的に3次元メッシュを生成できる．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1705.10819">論文</a></li></ul></div></div><div class="slide_index">[#540]</div><div class="timestamp">2018.4.13 11:16:55</div></div></section><section id="ID_SPLATNet_Sparse_Lattice_Networks_for_Point_Cloud_Processing"><div class="paper-abstract"><div class="title">SPLATNet: Sparse Lattice Networks for Point Cloud Processing</div><div class="info"><div class="authors">Hang Su, University of Massachusetts, Amherst; Varun Jampani, NVIDIA Research; Deqing Sun, NVIDIA; Evangelos Kalogerakis, UMass; Subhransu Maji, ; Ming-Hsuan Yang, UC Merced; Jan Kautz, NVIDIA</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>点群情報を直接処理できるSPLATNet（右図）を提案した．SPLATNetは直接点群から階層的な空間情報を抽出可能．また，2D情報と3D情報のマッピングも行えるので，点群とマルチ画像の両方をSPLATNetで処理可能．従来の直接点群情報を処理するネットワークはより局所的な空間情報を損失してしまう問題点がある．提案手法はこの問題を解決するために，BCLs層を用いた． BCLs層は点群をスパースなlatticeにマッピングし，さらにそのスパースなlatticeを畳み込みできる．それにより， unordered点群情報を処理できる上に点群のより局所的な情報も抽出可能にした．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SPLATNET.png" alt="SPLATNET"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>Façade segmentationタスクにおいて，点群とマルチ画像のラベリングに良い処理スピードと従来手法手法より優れた精度を得られた．ShapeNet part segmentationにおいて従来手法より優れた精度（クラスmIoU：83.7%）を得られた．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.08275">論文</a></li></ul></div></div><div class="slide_index">[#541]</div><div class="timestamp">2018.4.13 10:29:26</div></div></section><section id="ID_From_Lifestyle_Vlogs_to_Everyday_Interactions"><div class="paper-abstract"><div class="title">From Lifestyle Vlogs to Everyday Interactions</div><div class="info"><div class="authors">Fouhey et al.</div><div class="conference">CVPR 2018.</div><div class="paper_id">arXiv ID: 1712.02310</div></div><div class="item1"><h1>概要</h1><div class="text">従来のデータ取集手法（collection-by-acting）では難しいかった, バイアスの少ない, 多様で大規模な日常生活におけるインタラクションのデータベース Lifestyle VLOG dataset を公開した. </div></div><div class="item2"><img src="slides/figs/fukuhara-From-Lifestyle-Vlogs-to-Everyday-Interactions.png"></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>従来のデータセットが想定している陽的なデータ収集とは対照的に隠的なデータ収集方法を行うことで, バイアスを小さくすることに成功した.</li><li>ビデオに対してインタラクションのラベル, フレームに対してインタラクション時の手の状態のラベル付けられている.</li><li>従来のデータセットのBiasを分析するために, 従来のデータセットで訓練した手法が Lifestyle VLOG データセットに対しても上手く動作するか検証した.</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.02310.pdf" target="blank">[論文] From Lifestyle Vlogs to Everyday Interactions</a></li><li><a href="https://people.eecs.berkeley.edu/~dfouhey/2017/VLOG/index.html" target="blank">Project Page</a></li><li><a href="https://github.com/dfouhey/VLOGToolkit" target="blank">GitHub</a></li></ul></div></div><div class="slide_index">[#542]</div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="timestamp">2018.4.12.00:00:00</div></div></section><section id="ID_Seeing_Voices_and_Hearing_Faces_Cross-modal_biometric_matching"><div class="paper-abstract"><div class="title">Seeing Voices and Hearing Faces: Cross-modal biometric matching</div><div class="info"><div class="authors">A. Nagrani et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><div class="text"><h1>概要</h1><p>ある音声と2人分の顔画像から，どちらの人物の声かを推定する課題と，ある顔画像と2人分の音声から，どちらの音声がその人物の声かを推定する課題の2つを解くという問題設定の研究．
異なるモダリティ間でのマッチングという課題ということ．
ある入力に対応するのがどちらの人物かという2クラス識別の問題設定として定式化．
この問題を解くために，3入力を扱う3-streamのネットワーク構造を持つモデルを提案．
音声もスペクトログラムの形式で画像のように扱い，顔画像，音声ともにConvolutionしていくモデル．
実験では80%程度の識別率を達成し，人と同等の結果が出ている．
二人分の選択肢の性別，国籍，年齢などが同じという設定にすると，60%程度の正答率になるが，こちらでは人 (57%) を上回る結果となっている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Seeing_Voices_and_Hearing_Faces_Cross-modal_biometric_matching.png" alt="Seeing_Voices_and_Hearing_Faces_Cross-modal_biometric_matching.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>人物の顔画像と音声の対応付けという新しい問題設定</li><li>人間レベルの高い精度を実現</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.00326">論文 (arXiv)</a></li></ul></div></div><div class="slide_index">[#543]</div><div class="timestamp">2018.4.12 15:48:11</div></div></section><section id="ID_Actor_and_Action_Video_Segmentation_from_a_Sentence"><div class="paper-abstract"><div class="title">Actor and Action Video Segmentation from a Sentence</div><div class="info"><div class="authors">Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, Cees G.M. Snoek</div><div class="conference">CVPR 2018 (oral)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>センテンスの入力から、行動者と行動（Actor and Action）を同時に特定する研究である。複数の同様の物体から特定の人物など、詳細な分類が必要になる。ここではFully-Convolutional（構造の全てが畳み込みで構成される）モデルを適用してセグメンテーションベースで出力を行うモデルを提案。図は提案モデルを示す。I3Dにより動画像のエンコーディング、自然言語側はWord2Vecの特徴をさらにCNNによりエンコーディング。その後、動画像・言語特徴を統合してDeconvを繰り返しセグメントを獲得していく。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1803ActorAction.png" alt="1803ActorAction"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>文章（と動画像）の入力から行動者と行動の位置を特定すべくセグメンテーションを実行するという問題を提起した。また、二つの有名なデータセット（A2D/J-HMDB）を拡張して7,500を超える自然言語表現を含むデータとした。同問題に対してはSoTA。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>CVxNLPの問題はここにも進出して来た。画像キャプションに限らず、この手の統合は進められるはず。</p><ul><li><a href="https://arxiv.org/pdf/1803.07485.pdf">論文</a></li></ul></div></div><div class="slide_index">[#544]</div><div class="timestamp">2018.3.24 12:47:10</div></div></section><section id="ID_Alive_Caricature_from_2D_to_3D"><div class="paper-abstract"><div class="title">Alive Caricature from 2D to 3D</div><div class="info"><div class="authors">Qianyi Wu, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>2Dの似顔絵画像から3Dの似顔絵を作成するためのアルゴリズムの提案。似顔絵画像のテストデータとしてはカリカチュアを使用し、カリカチュア画像の3Dモデルとテクスチャ化された画像を生成する。データは、標準の3D顔の変形を座標系に配置(下図、 xは口の開き具合)し、金のオリジナルデータから線形結合によって白い顔を生成する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330AC2Dto3D_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・リンク集</h1><p>カリカチュアを集めたデータセットを作って学習するのではなく、標準の3D顔のデータセットから実装でき、アプリケーションの柔軟さを推している。</p><p>3DMMやFaceWareHouseなどの従来手法と比較して、形の歪みが少なく、従来のものよりも綺麗な3D顔の出力が可能。顔以外にも、概形の予測が可能なオブジェクトなら応用できる？</p><ul><li><a href="https://arxiv.org/pdf/1803.06802.pdf">論文</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180330AC2Dto3D_2.jpg"></p></div></div><div class="slide_index">[#545]</div></div></section><section id="ID_A_Minimalist_Approach_to_Type-Agnostic_Detection_of_Quadrics_in_Point_Clouds"><div class="paper-abstract"><div class="title">A Minimalist Approach to Type-Agnostic Detection of Quadrics in Point Clouds</div><div class="info"><div class="authors">Tolga Birdal, Benjamin Busam, Nassir Navab, Slobodan Ilic, Peter Sturm</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>オクルージョンが発生している場合/複雑な環境下でも簡単な形状がポイントクラウドから検出できる枠組みを提案する。手法は3D楕円形状のフィッティング、3次元空間操作、4点取得により構成。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180324Quadrics.png" alt="180324Quadrics"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>タイプに依存しない3次元の二次曲面（楕円球形状）検出を点群の入力から行う手法を考案した。さらに、4点探索問題を3点探索にしてRANSACベースの手法で解を求めた。モデルベースのアプローチよりはフィッティングの性能がよいが、キーポイントベースの手法よりは劣る。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>曖昧な教示のみで3次元形状探索問題が解決できるようになる？</p><ul><li><a href="https://arxiv.org/pdf/1803.07191.pdf">論文</a></li></ul></div></div><div class="slide_index">[#546]</div><div class="timestamp">2018.3.24 13:04:44</div></div></section><section id="ID_COCO-Stuff_Thing_and_Stuff_Classes_in_Context"><div class="paper-abstract"><div class="title">COCO-Stuff: Thing and Stuff Classes in Context</div><div class="info"><div class="authors">Holger Caesar, Jasper Uijlings, Vittorio Ferrari</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>MSCOCOデータセットに対してThing（もの）やStuff（材質）に関する追加アノテーションを行い、さらにコンテキスト情報も追加したCOCO-Stuffを提案した。このデータセットには主にシーンタイプ、そのものがどこに現れそうかという場所、物理的/材質的な属性などをアノテーションとして付与する。COCO2017をベースにして164Kに対して91カテゴリを付与し、スーパーピクセルを用いた効率的なアノテーションについてもトライした。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180329COCOStuff.png" alt="180329COCOStuff"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>材質的なアノテーションは画像キャプションに対して重要であることを確認、相対的な位置関係などデータセットのリッチなアノテーションが重要であること、セマンティックセグメンテーションベースの方法により今回のアノテーションを簡易的に行えたこと、などを示した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>さらにリッチなアノテーションは今後重要になる。この論文ではスーパーピクセルという弱い知識を用い、人間のアノテーションと組み合わせることでボトムアップ・トップダウンを効果的かつ効率的に組み合わせてアノテーションを行っている点が素晴らしい。ラストオーサのVittorio Ferrariは機械と人の協調によるアノテーションが得意（なので、既存データセットへのよりリッチなアノテーションを早いペースで提案できる）。</p><ul><li><a href="https://arxiv.org/pdf/1612.03716v4.pdf">論文</a></li><li><a href="https://github.com/nightrome/cocostuff10k">GitHub</a></li></ul></div></div><div class="slide_index">[#547]</div><div class="timestamp">2018.3.29 13:59:43</div></div></section><section id="ID_Context-aware_Synthesis_for_Video_Frame_Interpolation"><div class="paper-abstract"><div class="title">Context-aware Synthesis for Video Frame Interpolation</div><div class="info"><div class="authors">Simon Niklaus, Feng Liu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>入力フレームだけでなく、ピクセル単位の文脈情報を用いて、高品質の中間フレームを補間するためのコンテキスト認識手法の提案。まず、プレトレインモデルを使用して、入力フレームのピクセルごとのコンテキスト情報を抽出。オプティカルフローを使用して、双方向フローを推定し、入力フレームとそのコンテキストマップの両方をワープする。最後にコンテキストマップをsynthesis networkに入力し、補間フレームを生成。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401CaS.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>従来のビデオフレーム補間アルゴリズムは、オプティカルフローまたはその変動を推定し、それを用いて2つのフレーム間の中間フレームを生成する。本手法では、 2つの入力フレーム間の双方向フローを推定し、コンテキスト認識という方式をとることで精度向上を図る。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>高品質のビデオフレーム補間実験において、従来を上回る性能。</p><ul><li><a href="https://arxiv.org/pdf/1803.10967.pdf">論文</a></li></ul></div></div><div class="slide_index">[#548]</div></div></section><section id="ID_Deep_Depth_Completion_of_a_Single_RGB-D_Image"><div class="paper-abstract"><div class="title">Deep Depth Completion of a Single RGB-D Image</div><div class="info"><div class="authors">Yinda Zhang, Thomas Funkhouser</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>RGB画像から表面の法線とオクルージョン境界を予測し、 RGB-D画像と組み合わせて、欠けている奥行き情報を補完するDeep Depth Completionの提案。また、奥行き画像と対になったRGB-D画像のデータセットであるcompletion benchmark datasetを作成し、性能を評価。これは、低コストのRGB-Dカメラでキャプチャした画像と、高コストの深度センサで同時にキャプチャした画像で構成されている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401DDC.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>深度カメラは、光沢があり、明るく、透明で、遠い表面の深さを感知しないことが多い。 このような問題を解決するために、本手法ではRGB画像から得た情報と組み合わせて、 RGB-D画像の深度チャネルを完全なものにする。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>深さ修復および推定において従来よりも優れた性能。</p><ul><li><a href="https://arxiv.org/abs/1803.09326">論文</a></li><li><a href="http://deepcompletion.cs.princeton.edu/">Project webpage</a></li></ul></div></div><div class="slide_index">[#549]</div></div></section><section id="ID_Detecting_and_Recognizing_Human-Object_Interactions"><div class="paper-abstract"><div class="title">Detecting and Recognizing Human-Object Interactions</div><div class="info"><div class="authors">Georgia Gkioxari, Ross Girshick, Piotr Dollár, Kaiming He</div><div class="conference">CVPR 2018 (spotlight)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>人物検出と同時に人物行動やその物体とのインタラクションも含めて学習を行うモデルを提案する。本論文では物体候補の中でも特にインタラクションに関係ありそうな物体に特化して認識ができるようにする。さらに、検出された<human, verb, object>のペアを用いて学習する（図の場合には<human, cut, knnife>）。さらに、その他の行動（図の場合にはstand）を同時に推定することもできる。モデルはFaster R-CNNをベースとするが、物体検出（box, class）、行動推定（action, target）、インタラクション（action）を推定して誤差を計算する。さらに、推定した人物位置に対する対象物体の方向も確率的に計算することが可能。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180322HOI.png" alt="180322HOI"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>人間に特化した検出と行動推定の枠組みを提案した。V-COCO（Verbs in COCO）にて、相対的に26%精度が向上（31.8=>40.0）、HICO-DETデータセットにて27%相対的な精度向上が見られた。計算速度は135ms/imageであり、高速に計算が可能である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>単純な多タスク学習ではなく、人物に特化して対象物体の位置も確率的に推定しているところがGood。</p><ul><li><a href="https://arxiv.org/pdf/1704.07333.pdf">論文</a></li><li><a href="https://gkioxari.github.io/InteractNet/index.html">Project</a></li><li><a href="https://github.com/s-gupta/v-coco">Verbs in COCO DB</a></li></ul></div></div><div class="slide_index">[#550]</div><div class="timestamp">2018.3.22 19:55:34</div></div></section><section id="ID_Discriminative_Learning_of_Latent_Features_for_Zero-Shot_Recognition"><div class="paper-abstract"><div class="title">Discriminative Learning of Latent Features for Zero-Shot Recognition</div><div class="info"><div class="authors">Minghui Yan Li, et al</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>Zero-shot learning(ZSL)における、視覚的および意味的インスタンスを別々に表現し学習するLatent Discriminative Features Learning(LDF)の提案。 (1)ズームネットワークにより差別的な領域を自動的に発見することができるネットワークの提案。(2)ユーザによって定義された属性と潜在属性の両方について、拡張空間における弁別的意味表現の学習。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330LDF.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>ZSLは、画像表現と意味表現の間の空間を学習することによって、見えない画像カテゴリを認識する。 既存の手法では、視覚と意味空間を合わせたマッピングマトリックスを学習することが中心的課題。提案手法では、差別的に学習するとうアプローチで識別精度向上を図る。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>2つのコンポーネントによって、互いに支援しながら学習することで最先端の精度に。</p><ul><li><a href="https://arxiv.org/pdf/1803.06731.pdf">論文</a></li></ul></div></div><div class="slide_index">[#551]</div></div></section><section id="ID_Domain_Adaptive_Faster_R-CNN_for_Object_Detection_in_the_Wild"><div class="paper-abstract"><div class="title">Domain Adaptive Faster R-CNN for Object Detection in the Wild</div><div class="info"><div class="authors">Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, Luc Van Gool</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ドメイン変換について、ゲームなどのCG映像から実際の交通シーンに対応して物体検出を行うための学習方法を提案する。本論文では(i) 画像レベルのドメイン変換、(ii) インスタンス（ある物体）に対してのドメイン変換、の二種類の方法を提案し、整合性をとるように正規化する（図のConsistency Regularization; Global/Localな特徴変換を考慮）。ここで、物体検出はFaster R-CNNをベースとしてドメイン変換の手法も二種類（H-divergence、敵対的学習）用意する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180314DomainFRCNN.png" alt="180314DomainFRCNN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>CGで学習し実環境における自動運転などで使えるドメイン変換の手法を提案した。実験はCityscapes, KITTI, SIM10Kなどで行い、ロバストな物体検出を実行することができた。例えばCityscapesとKITTIの相互ドメイン変換でベースラインのFaster R-CNNが30.2 (K->C)、53.5 (C->K)のところ、Domain Adaptive Faster R-CNNでは38.5 (K->C)、64.1 (C->K)であった。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>データ収集は手動から自動の時代になって来た？データを手作業で集める時代からアルゴリズムを駆使して収集する時代へ移行。</p><ul><li><a href="https://arxiv.org/pdf/1803.03243.pdf">論文</a></li><li><a href="http://www.vision.ee.ethz.ch/~liwenw/">著者</a></li></ul></div></div><div class="slide_index">[#552]</div><div class="timestamp">2018.3.14 08:43:53</div></div></section><section id="ID_Efficient_Interactive_Annotation_of_Segmentation_Datasets_with_Polygon-RNN"><div class="paper-abstract"><div class="title">Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++</div><div class="info"><div class="authors">David Acuna, Huan Ling, Amlan Kar, Sanja Fidler</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>Polygon-RNNのアイデアを踏襲し、ヒューマン・イン・ザ・ループを使って対話的にオブジェクトのポリゴンアノテーションの生成。また、新しいCNNエンコーダアーキテクチャの設計、強化学習によるモデルの効果的な学習、 Graph Neural Networkを使用した出力解像度の向上を行う。これらのアーキテクチャをPolygon-RNN ++と呼ぶ。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180331PolygonRNN++_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・リンク集</h1><p>アノテーション作成時の負担を軽減。より正確にアノテーションを付加できるため、雑音の多いアノテーターに対しても頑健である。</p><p>高い汎化能力となり、既存のピクセルワイズメソッドよりも大幅に改善。ドメイン外のデータセットにも適応可能。</p><ul><li><a href="https://arxiv.org/pdf/1803.09693.pdf">論文</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180331PolygonRNN++_2.jpg"></p></div></div><div class="slide_index">[#553]</div></div></section><section id="ID_Egocentric_Basketball_Motion_Planning_from_a_Single_First-Person_Image"><div class="paper-abstract"><div class="title">Egocentric Basketball Motion Planning from a Single First-Person Image</div><div class="info"><div class="authors">Gedas Bertasius, Aaron Chan, Jianbo Shi</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>一人称視点の画像からゴールリングに到達するまでのバスケットボール選手の動線を生成する。本論文では3D位置や頭部方向も記録する。同タスクを実行するため、まずは画像空間から12Dのカメラ空間に投影を行うEgoCam CNNを学習。次に予測を行うCNN（Future CNN）を構築、さらに予測位置やゴールまでの位置が正確かどうかを検証するGoal Verifier CNNを用いることでより正確な推定を行うことができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180307EgoBasketball.png" alt="180307EgoBasketball"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>複数のネットワークの出力（ここではEgoCamCNNとFutureCNN）を検証するVerification Networkという考え方は面白い。他のネットワークの出力を、検証用のネットワークにより正すというのはあらゆる場面で用いることができる。RNN/LSTM/GANsなどよりも高度な推定ができることが判明した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>結果例は動画像を参照。未来予測・３次元投影などコンポーネントがDNNにより高度にできるようになってきたからできた研究。さらに検証用のネットワークを構築することで出力自体を操作している。</p><ul><li><a href="https://arxiv.org/pdf/1803.01413v1.pdf">論文</a></li><li><a href="https://www.youtube.com/watch?v=wRRRl4QsUQg">YouTube</a></li></ul></div></div><div class="slide_index">[#554]</div><div class="timestamp">2018.3.7 09:04:15</div></div></section><section id="ID_Fast_and_Accurate_Single_Image_Super-Resolution_via_Information_Distillation_Network"><div class="paper-abstract"><div class="title">Fast and Accurate Single Image Super-Resolution via Information Distillation Network</div><div class="info"><div class="authors">Zheng Hui, Xiumei Wang, Xinbo Gao</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>元の低解像度画像から高解像度画像を再構築するための、深くてコンパクトなCNNを提案。提案モデルは、特徴抽出ブロック、積み重ね情報蒸留ブロック、再構成ブロックの3部構成。これにより、情報量が豊富かつ効率的に特徴を徐々に抽出できる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180331FaASISR.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>CNNが超解像殿画像を扱うようになってきたが、ネットワークが増大するにつれて、計算上の複雑さとメモリ消費という問題が生じる。これらの問題を解決するためのコンパクトなCNN。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>PSNR、SSIM、IFCの4つのデータセットで検証し、精度向上を確認。デシジョンおよび圧縮アーチファクト低減などの他の画像修復問題にも応用可能？</p><ul><li><a href="https://arxiv.org/pdf/1803.08679.pdf">論文</a></li></ul></div></div><div class="slide_index">[#555]</div></div></section><section id="ID_Future_Frame_Prediction_for_Anomaly_Detection_--_A_New_Baseline"><div class="paper-abstract"><div class="title">Future Frame Prediction for Anomaly Detection -- A New Baseline</div><div class="info"><div class="authors">Wen Liu, Weixin Luo, Dongze Lian, Shenghua Gao</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>先の（未来の）フレーム予測と異常検知を同時に行う手法を提案する論文。予測したフレームと異常検知の正解値により誤差を計算して最適化を行う。図に本論文で提案するネットワークアーキテクチャの図を示す。U-Netにより画像予測やさらにオプティカルフロー推定を行い、RGB空間、オプティカルフロー空間にて誤差を計算しGANの枠組みでそれらがリアルかフェイクかを判定する。同フレームを用いて異常検知を実施する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180315PredictionAnomaly.png" alt="180315PredictionAnomaly"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>従来は現在フレームを入力として異常検知を行う手法は存在したが、未来フレームを予測して異常検知を行う枠組みは本論文による初めての試みである。異常値の正解値を与えることで画像予測にもフィードバックされるため、画像予測と異常検知の相互学習に良い影響を与える。オープンデータベースにてベンチマークした結果、何れもState-of-the-artな精度を達成。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>生成ベースで画像予測+X（Xは任意タスク）というものはSoTAが出せるくらいにはなってきた。</p><ul><li><a href="https://arxiv.org/pdf/1712.09867.pdf">論文</a></li><li><a href="https://github.com/StevenLiuWen/ano_pred_cvpr2018">Project</a></li></ul></div></div><div class="slide_index">[#556]</div><div class="timestamp">2018.3.15 09:04:03</div></div></section><section id="ID_Guided_Labeling_using_Convolutional_Neural_Networks"><div class="paper-abstract"><div class="title">Guided Labeling using Convolutional Neural Networks</div><div class="info"><div class="authors">Sebastian Stabinger, et al. </div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルの付いていないデータに対して、どの画像にラベルを付けてデータセットを構成すればよいかを判断するguided labelingの提案。ラベル付けを行う必要があるサンプルを見定めることで、データセットの量を大幅に減らすことができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180313GuidedLabeling.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>大規模データセットにおいて、手動でのラベル付けは大変。選別してラベル付けを行えば、作業を最小限に抑えられる。また、ある意味良いデータを選別できるため、場合によっては精度も向上。</p></div></div><div class="item4"><div class="text"><p>MNISTは、データセットのサイズを1/16に、CIFAR10は1/2に減らすことが可能に。また、MNISTの場合は、全部使った時よりも識別精度が向上した。普遍性を妨げる不必要なデータを取り除けたことが精度向上につながった？</p><ul><li><a href="https://arxiv.org/pdf/1712.02154.pdf">論文</a></li></ul></div></div><div class="slide_index">[#557]</div></div></section><section id="ID_HATS_Histograms_of_Averaged_Time_Surfaces_for_Robust_Event-based_Object_Classification"><div class="paper-abstract"><div class="title">HATS: Histograms of Averaged Time Surfaces for Robust Event-based Object Classification</div><div class="info"><div class="authors">Amos Sironi, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>イベントベースカメラにおける、識別アルゴリズムの提案。本研究では、(1)イベントベースのオブジェクト分類のための低レベル表現とアーキテクチャの欠如、(2)実世界における大きなイベントベースのデータセットの欠如、の2つの問題に取り組む。新しい機械学習アーキテクチャ、イベントベースの特徴表現(Histograms of Averaged Time Surfaces)、データセット(N-CARS)を提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330NCARS.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>イベントベースのカメラは、従来のフレームベースのカメラと比較して、高時間分解能、低消費電力、高ダイナミックレンジという点で優れており、様々なシーンで応用が利く。しかし、イベントベースのオブジェクト分類アルゴリズムの精度は未だ低い。特徴表現には過去時間の情報を使用。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>過去の情報を使うことで、既存のイベントベースカメラによる認識手法よりも優れた結果となった。</p><ul><li><a href="https://arxiv.org/pdf/1803.07913.pdf">論文</a></li><li><a href="http://www.prophesee.ai/dataset-n-cars/">データセット</a></li></ul></div></div><div class="slide_index">[#558]</div></div></section><section id="ID_Improving_Object_Localization_with_Fitness_NMS_and_Bounded_IoU_Loss"><div class="paper-abstract"><div class="title">Improving Object Localization with Fitness NMS and Bounded IoU Loss</div><div class="info"><div class="authors">Lachlan Tychsen-Smith, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>既存のNon-Max Supressionを改良したFitness NMSの提案。Soft NMSも同時に使用するとより効果的。</p><p>勾配降下法の収束特性(滑らかさ、堅牢性など)を維持しつつ、IoUを最大化するという目標により適した損失関数であるBounded IoU Loss の提案。これをRoIクラスタリングと組み合わせることで精度が向上する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180314FitnessNMS.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>バウンディングボックスのスコアを算出する関数を拡張する。具体的には、グランドトゥルースとのIoUと、クラスの期待値を追加する。これにより、IoUの重なり推定値と、クラス確率の両方が高いバウンディングボックスを優先して学習することができる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>MSCOCO、Titan X(Maxwell)使用時では、精度33.6％-79Hzまたは41.8％-5Hz。本論文ではDeNetでテストしたが、別の手法でも精度向上が望めるよう。</p><ul><li><a href="https://arxiv.org/pdf/1711.00164.pdf">論文</a></li><li><a href="https://github.com/lachlants/denet">ソースコード</a></li></ul></div></div><div class="slide_index">[#559]</div></div></section><section id="ID_Independently_Recurrent_Neural_Network_IndRNN_Building_A_Longer_and_Deeper_RNN"><div class="paper-abstract"><div class="title">Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN</div><div class="info"><div class="authors">Shuai Li, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>新しいRNN手法であるindependently recurrent neural network (IndRNN)の提案。一枚のレイヤ内のニューロンが独立しており、レイヤ間で接続されている。これにより、勾配消失問題や爆発問題を防ぎ、より長期的なデータを学習することができる。また、IndRNNは複数積み重ねることができるため、既存のRNNよりも深いネットワークを構築できる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180314IndRNN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>本手法によって下記の従来手法の問題を解決。</p><p>RNNは、勾配の消失や爆発の問題、長期パターンの学習が困難である。LSTMやGRUは、上記のRNNの問題を解決すべく開発されたが、層の勾配が減衰してしまう問題がある。また、RNNは全てのニューロンが接続されているため、挙動の解釈が困難。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>かなり長いシーケンス(5000回以上の時間ステップ)を処理でき、かなり深いネットワーク（実験では21レイヤー）を構築できる。</p><ul><li><a href="https://arxiv.org/pdf/1803.04831.pdf">論文</a></li></ul></div></div><div class="slide_index">[#560]</div></div></section><section id="ID_Iterative_Visual_Reasoning_Beyond_Convolutions"><div class="paper-abstract"><div class="title">Iterative Visual Reasoning Beyond Convolutions</div><div class="info"><div class="authors">Xinlei Chen, Li-Jia Li, Li Fei-Fei, Abhinav Gupta</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>CNNのような理由を突き止める能力がない認識システムを超えた、反復的なvisual reasoningのための新しいフレームワークの提案。畳み込みベースのローカルモジュールとグラフベースのグローバルモジュールの2コアで構成。2つのモジュールのを繰返し展開し、予測結果を相互にクロスフィードして絞り込む。最後に、両方のモジュールの最高値をアテンションベースのモジュールと組み合わせてプレディクト。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401IVRBC_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・リンク集</h1><p>ただ畳み込むだけでなく、Spatial(空間的)およびSemanticの空間を探索することができる。下図のように、「人」は「車」を運転するというSpatialとSemanticの双方を兼ね備えた認識を行うことで精度向上を図る。</p><p>通常のCNNと比較して、ADEで8.4％、COCOで3.7％の精度向上。</p><ul><li><a href="https://arxiv.org/pdf/1803.11189.pdf">論文</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180401IVRBC_2.jpg"></p></div></div><div class="slide_index">[#561]</div></div></section><section id="ID_LayoutNet_Reconstructing_the_3D_Room_Layout_from_a_Single_RGB_Image"><div class="paper-abstract"><div class="title">LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image</div><div class="info"><div class="authors">Chuhang Zou, Alex Colburn, Qi Shan, Derek Hoiem</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>単一のパースペクティブまたはパノラマ画像から屋内3Dルームレイアウトを推定するLayoutNetの提案。最初に、消失点を分析し、水平になるように画像を整列。これにより、壁と壁の境界が垂直になり、ノイズ低減。画像からコーナー(レイアウト接合点)と境界を、エンコーダ/デコーダ構造のCNNで出力。最後に、3D Layoutパラメータを、予測したコーナーと境界に適合するように最適化する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401LayoutNet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>アーキテクチャはRoomNetと似ているが、消失点に基づいて画像を整列させ、複数のレイアウト要素（コーナー、境界線、サイズ、平行移動）を予測し、 “L”形の部屋のような非直方体のマンハッタンレイアウトに対しても適応できる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>従来手法と比較して、処理速度と正確さにおいて性能の向上。</p><ul><li><a href="https://arxiv.org/pdf/1803.08999.pdf">論文</a></li><li><a href="https://github.com/zouchuhang/LayoutNet">ソースコード</a></li></ul></div></div><div class="slide_index">[#562]</div></div></section><section id="ID_Learning_to_Localize_Sound_Source_in_Visual_Scenes"><div class="paper-abstract"><div class="title">Learning to Localize Sound Source in Visual Scenes</div><div class="info"><div class="authors">Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, In So Kweon</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像と音声の入力から、音が画像のどこで鳴っているか（鳴りそうか？）を推定した研究。さらに、人の声なら人の領域、車の音なら車の領域にアテンションがあたるなど物体と音声の対応関係も学習することができる。学習には音源とその対応する物体の位置を対応づけたデータセット（144Kのペアが含まれるSound Source Localization Dataset）を準備した。さらに既存の物体認識と音声を対応づけて（？）Unsupervised/Semi-supervisedに学習することにも成功した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180322LocalizeSound.png" alt="180322LocalizeSound"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>教師あり、教師なし、半教師あり、いずれの枠組みでも音声ー物体の対応関係を学習することができるようにした。音源とそれに対応する物体領域の尤度がヒートマップにて高く表示されている。結果はビデオを参照されたい。教師なし学習はTriplet-lossにより構成され、ビデオと近い/遠い音声の誤差により計算。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>非常に面白い問題設定、プラス誤差関数を柔軟に抽出可能というところが上手。精読しても良いと感じた論文。CVにおいてビデオの音声は今まで使用しないことも多かった（もしくは精度向上のために活用していた）が、これからは使用方法を見直してもよいと感じた。</p><ul><li><a href="https://arxiv.org/pdf/1803.03849.pdf">論文</a></li><li><a href="https://www.youtube.com/watch?v=UyairkbzR_Y">YouTube</a></li></ul></div></div><div class="slide_index">[#563]</div><div class="timestamp">2018.3.22 19:18:32</div></div></section><section id="ID_Learning_to_Segment_Every_Thing"><div class="paper-abstract"><div class="title">Learning to Segment Every Thing</div><div class="info"><div class="authors">Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, Ross Girshick</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルが完全に手に入らない際にでも転移学習が可能なセグメンテーション手法（論文中ではPartially Supervised Training Paradigm, weight transfer functionを紹介）を提案する。条件として、bboxが手に入っている物体に対してセグメンテーション領域を学習可能。Mask R-CNNをベースとしているが、Weight Transfer Functionを追加、セグメントの重みを学習・推定して誤差計算と学習繰り返し。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180303SegmentEverything.png" alt="180303SegmentEverything"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>Visual Genome Datasetから3,000の視覚的概念を獲得、MSCOCOから80のマスクアノテーションを獲得した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>弱教師付き学習が現実的な精度で動作するようになってきた？アノテーションはお金や知識があっても非常に大変なタスクであり、いかに減らすかという方向に研究が進められている。（What's next?ー弱教師/教師なしの先とは？）</p><ul><li><a href="https://arxiv.org/pdf/1711.10370.pdf">論文</a></li><li><a href="http://ronghanghu.com/">著者</a></li><li><a href="http://kaiminghe.com/">Kaiming He</a></li></ul></div></div><div class="slide_index">[#564]</div><div class="timestamp">2018.3.3 10:46:40</div></div></section><section id="ID_MakeupGAN_Makeup_Transfer_via_Cycle-Consistent_Adversarial_Networks"><div class="paper-abstract"><div class="title">MakeupGAN: Makeup Transfer via Cycle-Consistent Adversarial Networks</div><div class="info"><div class="authors">Huiwen Chang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ソース画像のメイクをターゲット画像へ転写やメイクの除去をする研究。ターゲット画像とメイク済み画像の2枚を入力としメイクを転写するネットワークGとメイク済み画像らメイクを取り除くネットワークFを考え、2つのネットワークによって元の画像に戻るように学習していく。
その際、Fによってxに付与されたメイクがyのメイクと同じものであるかを評価するロスを加えることでメイクの特徴を捉える。
従来手法ではメイク転写・除去を独立した問題として考えていたが、この研究ではセットとして考えている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180408make.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>Youtubeのメイクチュートリアルの動画から、1148枚のメイクなし画像と1044枚のメイクあり画像を収集。ユーザースタディによって2つの既存手法と比較し、提案手法が一番いいと答えた人が65.7％（2番目と答えた人が31.4％）
従来手法では肌の色や表情の違いがあると上手くいかないのに対し、ソースとターゲット間でこれらが違ってもうまく転写できる。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://research.adobe.com/project/makeupgan-makeup-transfer-via-cycle-consistent-adversarial-networks/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#565]</div><div class="timestamp">2018.4.8 01:45:44</div></div></section><section id="ID_Motion-Appearance_Co-Memory_Networks_for_Video_Question_Answering"><div class="paper-abstract"><div class="title">Motion-Appearance Co-Memory Networks for Video Question Answering</div><div class="info"><div class="authors">Jiyang Gao, Runzhou Ge, Kan Chen, Ram Nevatia</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>ビデオQAのための、 Dynamic Memory Network(DMN) のコンセプトに基づいたmotion-appearance comemory networkの提案。本研究の特徴は次の3つである。(1)アテンションを生成するために動きと外観情報の両方を手がかりとして利用する共メモリアテンションメカニズム。(2) multi-level contextual factを生成するための時間的conv-deconv network。(3)異なる質問に対して動的な時間表現を構成するdynamic fact ensemble method。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401MACoMN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>本手法は、次のようなvideo QA特有の属性に基づいている。(1)豊富な情報を含む長い画像シーケンスを扱う。(2)動き情報と出現情報を相互に関連付け、アテンションキューを他の情報に応用できる。(3)答えを推論するために必要なフレーム数は質問によって異なる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>TGIF-QAの4つのタスクすべてにおいて、最先端技術よりも優れている。</p><ul><li><a href="https://arxiv.org/pdf/1803.10906.pdf">論文</a></li></ul></div></div><div class="slide_index">[#566]</div></div></section><section id="ID_Multi-Frame_Quality_Enhancement_for_Compressed_Video"><div class="paper-abstract"><div class="title">Multi-Frame Quality Enhancement for Compressed Video</div><div class="info"><div class="authors">Ren Yang, Mai Xu, Zulin Wang, Tianyi Li</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>圧縮した動画像に対して画質を向上させる研究。Peak Quality Frames (PQFs)を用いたSVMベースの手法やMulti-Frame CNN (MF-CNN)を提案。提案法により、圧縮動画における連続フレームからアーティファクトを補正するような働きが見られた。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180324PQF.png" alt="180324PQF"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>動画の画質改善手法においてState-of-the-art。動画に対する画質改善の結果は図を参照。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.04680">論文</a></li><li><a href="https://github.com/ryangBUAA/MFQE">GitHub</a></li></ul></div></div><div class="slide_index">[#567]</div><div class="timestamp">2018.3.24 15:14:35</div></div></section><section id="ID_Multi-Level_Factorisation_Net_for_Person_Re-Identification"><div class="paper-abstract"><div class="title">Multi-Level Factorisation Net for Person Re-Identification</div><div class="info"><div class="authors">Xiaobin Chang, Timothy M. Hospedales, Tao Xiang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>人間の視覚的外観を、人の手によるアノテーションなしかつ、複数のセマンティックレベルで識別因子に分解する Multi-Level Factorisation Net(MLFN)の提案。 MLFNは、複数のブロックで構成されており、各ブロックには、複数の因子モジュールと、各入力画像の内容を解釈するための因子選択モジュールが含まれている。 </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180331MLFN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>効果的なRe-IDを目指すには、高低のセマンティックレベルでの人の差別化かつ視界不変性をモデル化することである。 近年(2018)のdeep Re-IDモデルは、セマンティックレベルの特徴表現を学習するか、アノテーション付きデータが必要となる。MLFNではこれらを改善する。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>3つのRe-IDと、CIFAR-100の結果で最先端。</p><ul><li><a href="https://arxiv.org/pdf/1803.09132.pdf">論文</a></li></ul></div></div><div class="slide_index">[#568]</div></div></section><section id="ID_Non-local_Neural_Networks"><div class="paper-abstract"><div class="title">Non-local Neural Networks </div><div class="info"><div class="authors">Xiaolong Wang et al.</div><div class="conference">CVPR 2018</div></div><div class="item1"><h1>概要</h1><div class="text">NLPなどで効果を発揮しているself-attentionを多次元に一般化し、2D/3DCNNに導入することで新たな「non-local block」を形成し、画像や動画での実験を行った。
行動認識＠Kineticsでは非常に高い精度を達成。Instance segmentationやkey point detectionなどのタスクでも汎用的に効果を発揮。
</div></div><div class="item2"><img src="slides/figs/non_local.png"></div><div class="item3"><h1>手法</h1><div class="text">位置jと位置iに依存してアテンションを出力する関数f(.)とjのみに依存する関数g(.)の積を入力位置jに関して和をとることによって位置iの出力値を決定する。
位置情報の保存、可変入力サイズ、などの性質を持ち、全結合、畳み込みを特殊な形として含む。またf(.)の定義の仕方によってはself-attentionと一致する。
f(.)は様々な形が提案されているが、種類によらず効果を発揮している。実際に使用する場合は図のような残差構造を使用している。

</div></div><div class="item4"><div class="text"><h1>コメント・リンク</h1><p>効果のインパクトがすごい。学習曲線からもうまくいっていることが明らか。C2Dに対してspace-timeにnon-local blockを適用すると3Dconvよりも時系列方向への拡大として効果があったのが興味深い。
結局残差を用いたnon-local blockを使用していたので、単純にnon-local layerのみでの性能もきになる。
位置情報の保存は重要でも、局所性はあまり重要ではなかったのかと感じられる。</p><ul><li><a href="https://arxiv.org/abs/1711.07971">論文</a></li></ul></div></div><div class="slide_index">[#569]</div><div class="slide_editor">Tomoyuki Suzuki</div></div></section><section id="ID_Pose-Robust_Face_Recognition_via_Deep_Residual_Equivariant_Mapping"><div class="paper-abstract"><div class="title">Pose-Robust Face Recognition via Deep Residual Equivariant Mapping</div><div class="info"><div class="authors">Kaidi Cao, Yu Rong, Cheng Li, Xiaoou Tang, Chen Change Loy</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>横顔の認識精度を高めるためにDeep Residual EquivAriant Mapping (DREAM)の提案。正面と側面の顔間のマッピングを行うことで特徴空間を対応付ける。これにより、横顔を正面の姿勢に変換して認識を単純化。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180313DREAM_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・手法・リンク集</h1><p>正面と側面のトレーニング数の不均衡から、現代の顔認識モデルの多くは、正面と比べて横顔を処理するのが比較的貧弱。本手法は姿勢変動を伴う顔認識に限定されない顔認識が可能で、横顔のデータを増やさなくても精度向上。</p><p>上図より、DREAMをCNNに追加し、入力に残差を動的に追加。下図はマッピングによる姿勢変換の例。</p><ul><li><a href="https://arxiv.org/pdf/1803.00839.pdf">論文</a></li><li><a href="http://mmlab.ie.cuhk.edu.hk/projects/DREAM">ソースコード</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180313DREAM_2.jpg"></p></div></div><div class="slide_index">[#570]</div></div></section><section id="ID_Pyramid_Stereo_Matching_Network"><div class="paper-abstract"><div class="title">Pyramid Stereo Matching Network</div><div class="info"><div class="authors">Jia-Ren Chang, Yong-Sheng Chen</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>空間ピラミッドプーリングと3D CNNの2つのモジュールから構成された、ステレオ画像対からの奥行き推定を行うPyramid Stereo Matching Network(PSMNet)の提案。空間ピラミッドプーリングは、異なるスケールおよび位置でコンテキストを集約し、コストボリュームを形成する。 3D CNNは、複数のhourglass networksを重ねて、コストボリュームを規則化することを学習。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330PSMN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>現在(2018)ではステレオ画像からの奥行き推定を、CNNの教師あり学習で解決されてきている。 コンテキスト情報を利用することで精度向上を図る。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>最先端の手法よりも優れている結果。</p><ul><li><a href="https://arxiv.org/pdf/1803.08669.pdf">論文</a></li><li><a href="https: //github.com/JiaRenChang/PSMNet">ソースコード</a></li></ul></div></div><div class="slide_index">[#571]</div></div></section><section id="ID_Referring_Relationships"><div class="paper-abstract"><div class="title">Referring Relationships</div><div class="info"><div class="authors">Ranjay Krishna, Ines Chami, Michael Bernstein, Li Fei-Fei</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>referring relationshipsを利用して同カテゴリのエンティティ間の曖昧さを解消するタスクの提案。特徴抽出後、アテンションを生成。述語を使用することで、アテンションをシフトさせる。この述語シフトモジュールを介して、subjectとobjectの間でメッセージを反復的に渡すことで、2つのエンティティをローカライズ。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401ReferringRelationships_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>画像中のエンティティ間の関係にはそれぞれ意味があり、画像の理解に役立つ。例えば、図のサッカーの試合の画像では、複数の人写っているが、それぞれは異なる関係を持っている。一人はボールを蹴っており、もう一人はゴールを守っている。 <person-kicking-ball>に着目すると、述語の”kick”を理解することにより、画像内のどの人物が”ball”を蹴っているのかを正しく識別する。</p><ul><li><a href="https://arxiv.org/pdf/1803.10362.pdf">論文</a></li><li><a href="https://github.com/StanfordVL/ReferringRelationships">ソースコード</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180401ReferringRelationships_2.jpg"></p></div></div><div class="slide_index">[#572]</div></div></section><section id="ID_Rethinking_Feature_Distribution_for_Loss_Functions_in_Image_Classification"><div class="paper-abstract"><div class="title">Rethinking Feature Distribution for Loss Functions in Image Classification</div><div class="info"><div class="authors">Weitao Wan, Yuanyi Zhong, Tianpeng Li, Jiansheng Chen</div><div class="conference">CVPR 2018 (spotlight)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>本論文ではLarge-margin Gaussian Mixture (L-GM) Lossを提案して画像識別タスクに応用する。Softmax Lossとの違いは、学習セットにおけるディープ特徴の混合ガウス分布をフォローしつつ仮説を設定するところである。識別境界や尤度正則化においてL-GM Lossは非常に高いパフォーマンスを実現している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180314LGM.png" alt="180314LGM"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>L-GM Lossは画像識別においてSoftmax Lossよりも精度が高いことはもちろん、特徴分布を考慮するため例えばAdversarial Examples（摂動ノイズ）などにおいても対応できる。MNIST, CIFAR, ImageNet, LFWにおける識別や摂動ノイズを加えた実験においても良好な性能を確かめた。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Softmax Lossよりも有意に精度向上が見られている。導入が簡単なら取り入れて精度向上したい。</p><ul><li><a href="https://arxiv.org/pdf/1803.02988.pdf">論文</a></li></ul></div></div><div class="slide_index">[#573]</div><div class="timestamp">2018.3.14 11:04:45</div></div></section><section id="ID_Robust_Depth_Estimation_from_Auto_Bracketed_Images"><div class="paper-abstract"><div class="title">Robust Depth Estimation from Auto Bracketed Images</div><div class="info"><div class="authors">Sunghoon Im, Hae-Gon Jeon, In So Kweon</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>HDRの画像の明るさを補正するためのブラケット撮影からの距離画像やカメラ姿勢を同時推定する手法を提案する論文。ブラケット撮影とは通常の露出撮影以外に意図的に「少し明るめの写真」と「少し暗めの写真」を同時に撮影。距離画像推定は幾何変換をResidual-flow Networkに統合したモデルにより行う。ここでは学習ベースのMulti-view stereo手法（Deep Multi-View Stereo; DMVS）を幾何推定（Structure-from-Small-Motion; SfSM）と組み合わせる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180323BracketedImages.png" alt="180323BracketedImages"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>距離画像推定において、スマートフォンやDSLRカメラなど種々のデータセットにてSoTAな精度を達成。モバイル環境でも動作するような小さなネットワークと処理速度についても同時に実現した。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.07702">論文</a></li><li><a href="https://sunghoonim.github.io/">著者</a></li></ul></div></div><div class="slide_index">[#574]</div><div class="timestamp">2018.3.23 19:11:04</div></div></section><section id="ID_Rotation-Sensitive_Regression_for_Oriented_Scene_Text_Detection"><div class="paper-abstract"><div class="title">Rotation-Sensitive Regression for Oriented Scene Text Detection</div><div class="info"><div class="authors">Minghui Liao, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>自然画像から文字を検出する。単なる検出ではなく、文字の方向を考慮したバウンディングボックスによる検出手法であるRotation-sensitive Regression Detector (RRD)の提案。回帰ブランチによって、畳み込みフィルタを回転させて回転感知特徴を抽出。分類ブランチによって、回転感性特徴をプーリングすることによって回転不変特徴を抽出。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180329RRD.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>文字をテーマにした研究では(1)テキストの向きを無視した分類方法と，(2)向きを考慮したバウンディングボックスによる回帰がある。従来研究では、両方のタスクの共有の特徴を使用していたが、互換性がなかったためにパフォーマンスが低下(図b)。そこで、異なる2つのネットワークから抽出した、異なる特性の特徴を分類および回帰することを提案(図d,e)。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>ICDAR 2015、MSRA-TD500、RCTW-17およびCOCO-Textを含む3つのシーンテキストのデータセットで最先端のパフォーマンスを達成。向きがある一般物体検出にも応用可能？</p><ul><li><a href="https://arxiv.org/pdf/1803.05265.pdf">論文</a></li></ul></div></div><div class="slide_index">[#575]</div></div></section><section id="ID_SketchMate_Deep_Hashing_for_Million-Scale_Human_Sketch_Retrieval"><div class="paper-abstract"><div class="title">SketchMate: Deep Hashing for Million-Scale Human Sketch Retrieval</div><div class="info"><div class="authors">Peng Xu, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>スケッチ検索のためのディープハッシングフレームワークの提案。3.8mの大規模スケッチデータセットを構築。CNNでスケッチの特徴抽出。RNNでペンストロークの時間情報をモデル化。CNN-RNNでエンコードすることで、スケッチ性質に対応した新しいhashing lossを導入。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180408SkechMate.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・差分</h1><p>従来のスケッチ認識タスクに従う代わりに、より困難な問題のスケッチハッシュ検索を行う。ネットワークをスケッチ認識のために再利用することもでき、どちらも高パフォーマンス。大規模なデータセットを利用することで、従来の文献ではあまり研究されていなかった、スケッチのユニークな特性を見出す。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1804.01401.pdf">論文</a></li><li><a href="http://sketchx.eecs.qmul.ac.uk/downloads/">ソースコード/データセット</a></li></ul></div></div><div class="slide_index">[#576]</div></div></section><section id="ID_Style_Aggregated_Network_for_Facial_Landmark_Detection"><div class="paper-abstract"><div class="title">Style Aggregated Network for Facial Landmark Detection</div><div class="info"><div class="authors">Xuanyi Dong, Yan Yan, Wanli Ouyang, Yi Yang, University of Technology Sydney, The University of Sydney</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>顔のランドマーク検出。顔そのもののばらつきの他に、グレースケールやカラー画像、明暗などの画像スタイルが変わっても同様に検出できるStyle Aggregated Network(SAN)の提案。まず、(1)入力画像をさまざまなスタイルに変換し、スタイルを集約し、(2)顔のランドマーク予測する。(2)は、元画像とスタイルを集約した特徴の両方を入力し、融合してカスケード式のヒートマップ予測を生成する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330SAN_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>結果・リンク集</h1><p>Flickr8kとFlickr30kを使った実験において、最先端モデルと同等かそれ以上の結果。より正確で、より多様なキャプション生成。</p><ul><li><a href="https://arxiv.org/pdf/1803.04108.pdf">論文</a></li><li><a href="https://github.com/D-X-Y/SAN">ソースコード</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180330SAN_2.jpg"></p></div></div><div class="slide_index">[#577]</div></div></section><section id="ID_The_Unreasonable_Effectiveness_of_Deep_Features_as_a_Perceptual_Metric"><div class="paper-abstract"><div class="title">The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</div><div class="info"><div class="authors">Richard Zhang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>2枚の画像の類似度を表す指標は数多く提案されているが、その類似度は必ずしも人間の知覚と一致していない。近年はDNNにより高次の特徴を得ることが可能となっており、人間の知覚に近づいている。
そこで、既存の類似度の評価尺度とDNNベースの類似度判定を比較することでDNNベースの手法がより人間の知覚に近い類似度を表現できることを確認した。
具体的には、ある画像を異なる方法で加工したもの2つを用意し、どちらが元の画像に近いかを人間とコンピュータ両方に判定させることで検証を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180408perception.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>データセットとして、画像に様々な加工を施したデータを人間に類似度を評価してもらったものを作成。加工の例としては、ノイズの付与やオートエンコーダによる画像の復元などが挙げられる。
検証の結果、ＤＮＮベースの類似度の方が既存の尺度より人間の知覚に乗っ取ってることを示した。
また、DNNのネットワーク構造そのものは重要ではないことが分かった。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://richzhang.github.io/PerceptualSimilarity/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#578]</div><div class="timestamp">2018.4.8 01:36:55</div></div></section><section id="ID_TOM-Net_Learning_Transparent_Object_Matting_from_a_Single_Image"><div class="paper-abstract"><div class="title">TOM-Net: Learning Transparent Object Matting from a Single Image</div><div class="info"><div class="authors">Guanying Chen, Kai Han, Kwan-Yee K. Wong</div><div class="conference">CVPR 2018 (spotlight)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>透明物体の切り抜き（Transparent Object Matting; TOM）と反射特性を推定することが可能なネットワークTOM-Netを提案する。TOM-Netにより、物体の反射特性を保存しながら他の画像にレンダリングして、同画像のテクスチャを反映させることができる。同問題を反射フローの推定問題と捉えてDNNのモデルを構築することで解決した。荒い部分は多階層のEncoder-Decorderで推定し、詳細な部分はResidualNetで調整する。この問題を解決するために、データセットを構築した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180324TOMNet.png" alt="180324TOMNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>178Kの画像を含むデータセットを構築した。同DBには876サンプル、14の透明物体、60種の背景を含む。透明物体の推定と反射特性のレンダリングはGitHubページを参照。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.04636.pdf">論文</a></li><li><a href="http://gychen.org/TOM-Net/">Project</a></li><li><a href="https://github.com/guanyingc/TOM-Net_Rendering">GitHub</a></li></ul></div></div><div class="slide_index">[#579]</div><div class="timestamp">2018.3.24 18:05:46</div></div></section><section id="ID_Towards_Human-Machine_CooperationSelf-supervised_Sample_Mining_for_Object_Detection"><div class="paper-abstract"><div class="title">Towards Human-Machine Cooperation:Self-supervised Sample Mining for Object Detection</div><div class="info"><div class="authors">Keze Wang, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>物体検出の課題を考慮し、既存のActive Learning(AL)の欠点を改善することを目的とした、Self-Supervised Sample Mining(SSM)の提案。ラベルなし、もしくは一部ラベルのないデータを使って学習することができる。交差検証後のスコアによってサンプルを選別。低い場合にはユーザによってアノテーション、高い場合にはそのままラベルとして採用。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330SSM.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>既存のAL法では主に、単一の画像コンテクスト内でサンプル選択基準を定義し、大規模な物体検出において最適ではなく、頑強性および非実用的である。SSMによって、ユーザが必要な部分にだけ介入し、アノテーションの作業を軽減。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>アノテーションが少ないデータセットにおいても最先端の精度。</p><ul><li><a href="https://arxiv.org/pdf/1803.09867.pdf">論文</a></li></ul></div></div><div class="slide_index">[#580]</div></div></section><section id="ID_Towards_Open-Set_Identity_Preserving_Face_Synthesis"><div class="paper-abstract"><div class="title">Towards Open-Set Identity Preserving Face Synthesis</div><div class="info"><div class="authors">Jianmin Bao, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>顔画像からidentityとattributesを別々に再構成する、GANに基づいたOpen-Set Identity Generating Adversarial Networkの提案。 face synthesis networkは、ポーズや感情、照明、背景などをキャプチャする属性ベクトルを抽出することができる。図中の2つの入力画像AおよびBから抽出された識別を再結合することによって、A0およびB0を生成することができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401OSIPFS_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・リンク集</h1><p>顔の正面化、顔属性モーフィング、 face adversarial example detectionなど、より広範なアプリケーションに応用可能。</p><ul><li><a href="https://arxiv.org/pdf/1803.11182.pdf">論文</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180401OSIPFS_2.jpg"></p></div></div><div class="slide_index">[#581]</div></div></section><section id="ID_Towards_Universal_Representation_for_Unseen_Action_Recognition"><div class="paper-abstract"><div class="title">Towards Universal Representation for Unseen Action Recognition</div><div class="info"><div class="authors">Yi Zhu, Yang Long, Yu Guan, Shawn Newsam, Ling Shao</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>学習画像がなくても行動認識を実現する「Unseen Action Recognition (UAR)」についての研究。UARの問題をMIL（Multiple Instance Learning）の一般化（GMIL）として扱い、ActivityNetなど大規模動画データから分布推定して表現を獲得。図は提案手法であるCross-Domain UAR (CD-UAR)である。ビデオから抽出したDeep特徴はGMILによりカーネル化される。Word2Vecとの投稿によりURを獲得し、ドメイン変換により新しい概念を獲得する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180323UAR.png" alt="180323UAR"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>従来法では見た/見てないの対応関係をデータセット中に含ませていたが、本論文での提案はUniversal Representation（ユニバーサル表現）を獲得して同タスクを解決する。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.08460.pdf">論文</a></li></ul></div></div><div class="slide_index">[#582]</div><div class="timestamp">2018.3.23 19:40:06</div></div></section><section id="ID_Unsupervised_Cross-dataset_Person_Re-identification_by_Transfer_Learning_of_Spatial-Temporal_Patterns"><div class="paper-abstract"><div class="title">Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatial-Temporal Patterns</div><div class="info"><div class="authors">Jianming Lv, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>歩行者の時空間パターンを用いた、教師なし学習の人物再同定アルゴリズムであるTFusionを提案。既存の人物再同定アルゴリズムのほとんどは、小サイズのラベル付きデータセットを用いた教師付き学習手法である。そのため、大規模な実世界のカメラネットワークに適応することは困難である。また、そこで、ラベルなしデータセットも用いたクロスデータセット手法によって精度向上を図る。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330TFusion.jpg"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>まず、歩行者の空間的-時間的パターンを学習するために、ラベル付きデータセットを用いて学習した視覚的分類器を、ラベルなしデータセットに転送。次に、Bayesian fusion modelによって、学習された時空間パターンを視覚的特徴と組み合わせて、分類器を改善。最後に、ラベルのないデータを用いて分類器を段階的に最適化。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>人物再同定のための、教師なしクロスデータセット学習手法の中では最先端。</p><ul><li><a href="https://arxiv.org/pdf/1803.07293.pdf">論文</a></li></ul></div></div><div class="slide_index">[#583]</div></div></section><section id="ID_Unsupervised_Cross-dataset_Person_Re-identification_by_Transfer_Learning_of_Spatio-temporal_Patterns"><div class="paper-abstract"><div class="title">Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatio-temporal Patterns</div><div class="info"><div class="authors">Jianming, Lv and Weihang, Chen and Qing, Li and Can, Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルなし、ドメインが異なる環境に対して人物再同定を行う手法を提案する。モデルであるTFusionは４ステップにより構築（１）教師あり学習により識別器を構築（２）ターゲットであるラベルなしデータにより時空間特徴パターン（Spatio-temporal Pattern）を学習（３）統合モデルFを学習（４）ラベルなしのターゲットデータにて徐々に識別器を学習する（１〜４は図に示されている）。Bayesian Fusionを提案して、時空間特徴パターンと人物のアピアランス特徴を統合してドメイン変換を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180323CDReID.png" alt="180323CDReID"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>従来の人物再同定の設定では比較的小さいデータセットであり、完全に教師ありの環境を想定していたが、本論文ではラベルなし、ドメインが異なる環境に対して人物再同定を実行するため、非常に難しい問題となる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.07293.pdf">論文</a></li><li><a href="https://github.com/ahangchen/TFusion">GitHub</a></li></ul></div></div><div class="slide_index">[#584]</div><div class="timestamp">2018.3.23 20:37:22</div></div></section><section id="ID_Unsupervised_Textual_Grounding_Linking_Words_to_Image_Concepts"><div class="paper-abstract"><div class="title">Unsupervised Textual Grounding: Linking Words to Image Concepts</div><div class="info"><div class="authors">Raymond A. Yeh, Minh N. Do, Alexander G. Schwing</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>単語を検出された画像の概念に関連付けるための、仮説検定を用いた教師なしTextual grounding手法の提案。ネットワークにはVGG-16を採用し、画像内のオブジェクト/単語の空間情報やクラス情報、およびクラス外の新しい概念を学習できる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401UTG.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>Textual grounding、すなわち画像内のオブジェクトと単語をリンクさせる既存の技法は、教師付きのディープラーニングとして定式化されており、大規模なデータセットを用いてバウンディングボックスを推定する。しかし、データセットの構築には時間やコストがかかるので教師なしの手法を提案。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>ReferIt GameとFlickr30kを用いたベンチマークでそれぞれ7.98％と6.96％以上の精度。</p><ul><li><a href="https://arxiv.org/pdf/1803.11185.pdf">論文</a></li></ul></div></div><div class="slide_index">[#585]</div></div></section><section id="ID_Vision-and-Language_Navigation_Interpreting_visually-grounded_navigation_instructions_in_real_environments"><div class="paper-abstract"><div class="title">Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments</div><div class="info"><div class="authors">Peter Anderson, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>自然言語のナビゲーションを入力として、実空間の中をロボットが動き目的地に到達できるかどうかを競うベンチマーク（Visually-grounded natural language navigation in real buildings）を提案。データセットは3Dのシミュレータによりキャプチャされ、22Kのナビゲーション、文章の平均単語数は29で構成される。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180305R2RNavi.png" alt="180305R2RNavi"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>(1) Matterport3Dデータセットを強化学習を行えるように拡張。(2) 同タスクが行えるようなベンチマークであるRoom-to-Room (R2R)を提案して言語と視覚情報から実空間にてナビができるようにした。(3) seq-to-seqをベースとしたニューラルネットによりベンチマークを構築。VQAをベースにしていて、ナビゲーション（VQAでいう質問文）と移動アクション（VQAでいう回答）という組み合わせで同問題を解決する。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>自然言語の問題はキャプションや質問回答の枠を超えて実空間、さらにいうとロボットタスクに導入されつつある。この研究はビジョン側からのアプローチだが、ロボット側のアプローチが現在どこまでできているか気になる。すでに屋内環境をある程度自由に移動するロボットが実現しているとこの実現可能性が高くなる。SLAMとの組み合わせももう実行できるレベルにある？</p><ul><li><a href="https://arxiv.org/pdf/1711.07280.pdf">論文</a></li><li><a href="https://bringmeaspoon.org/">Project</a></li><li><a href="https://github.com/peteanderson80/Matterport3DSimulator">GitHub</a></li><li><a href="https://niessner.github.io/Matterport/">Matterport3D dataset</a></li></ul></div></div><div class="slide_index">[#586]</div><div class="timestamp">2018.3.5 19:53:46</div></div></section><section id="ID_Weakly-Supervised_Action_Segmentation_with_Iterative_Soft_Boundary_Assignment"><div class="paper-abstract"><div class="title">Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment</div><div class="info"><div class="authors">Li Ding, Chenliang Xu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>時系列の行動検出/セグメンテーション（Action Segmentation）に関する問題をWeakly-Supervised（WS学習）に解いた。ここではTemporal Convolutional Feature Pyramid Network (TCFPN)とIterative Soft Boundary Assignment (ISBA)を繰り返すことで行動に関する条件学習ができてくるという仕組み。TCFPNではフレームの行動を予測し、ISBAではそれを検証、それらを繰り返して行動間の境界線を定めながらWS学習の教師としていく。さらに、WS学習を促進するためにより弱い境界として行動間の繋がりを定義することでWS学習の精度を向上させる。学習はビデオ単位の誤差を最適化することで境界についても徐々に定まる（ここがWS学習の所以）ように学習する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180329ISBATCFN.png" alt="180329ISBATCFN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>Breakfast dataset, Hollywood extended datasetにて弱教師付き学習とテストを行いState-of-the-artな精度を達成した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>弱い教師データを大量に集めると、そろそろ（ある程度の）教師ありデータによる精度を超えそう？もっと汎用的に学習できる枠組みが必要か。</p><ul><li><a href="https://arxiv.org/pdf/1803.10699v1.pdf">論文</a></li></ul></div></div><div class="slide_index">[#587]</div><div class="timestamp">2018.3.29 14:27:12</div></div></section><section id="ID_Who_Let_The_Dogs_Out_Modeling_Dog_Behavior_From_Visual_Data"><div class="paper-abstract"><div class="title">Who Let The Dogs Out? Modeling Dog Behavior From Visual Data</div><div class="info"><div class="authors">Kiana Ehsani, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>犬視点の大規模ビデオデータセットを作成し、このデータを使用した、犬の行動や行動計画のモデル化。次の3つの問題に焦点を当てる。(1)犬の行動予測。(2)入力された画像対から犬のような行動計画を見出す。(3)例えば、歩行可能な表面推定などのタスクについて、学習された表現を利用。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401DogsOut.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>視覚情報からintelligent agent(知的エージェント)を直接的にモデリングするタスク。犬の視覚情報を使うことで、行動をモデル化する斬新な取り組み。得られたモデルをAIなどに応用する。特に、歩行可能な表面推定のタスクで良い結果となる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>様々なエージェントやシナリオで使用でき、ラベルがないにもかかわらず有用な情報を学習することが可能。今後は、モデルやデーセットの拡張に挑む。</p><ul><li><a href="https://arxiv.org/pdf/1803.10827.pdf">論文</a></li></ul></div></div><div class="slide_index">[#588]</div></div></section><section id="ID_Zero-shot_Recognition_via_Semantic_Embeddings_and_Knowledge_Graphs"><div class="paper-abstract"><div class="title">Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs</div><div class="info"><div class="authors">Xiaolong Wang, Yufei Ye, Abhinav Gupta, The Robotics Institute, Carnegie Mellon University</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>カテゴリの単語の埋め込みと他のカテゴリとの関係(視覚データが提供される)を使用するだけで、学習例がないカテゴリの分類器を学習するゼロショット認識モデルを提案。 knowledge graph (KG) を入力とし、Graph Convolutional Network(GCN)を基に、セマンティック埋め込みとカテゴリの関係の両方を使用して分類器を予測する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330KG.jpg"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>学習済のKGが与えられると、各ノードに対する意味的埋め込みとして入力を得る。一連のグラフ畳み込みの後、各カテゴリの視覚的分類器を予測する。トレーニング中に、カテゴリの視覚的分類器が与えられ、GCNパラメータを学習。テスト時に、これらのフィルタを使用して、見えないカテゴリの視覚的分類器を予測する。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>KGのノイズに対してロバストであり、最先端の精度。</p><ul><li><a href="https://arxiv.org/pdf/1803.08035.pdf">論文</a></li></ul></div></div><div class="slide_index">[#589]</div></div></section><section id="ID_Zoom_and_Learn_Generalizing_Deep_Stereo_Matching_to_Novel_Domains"><div class="paper-abstract"><div class="title">Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains</div><div class="info"><div class="authors">Jiahao Pang, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>学習済みデータと新しいドメイン(ground-truthなし)の両方を用いて、ディープステレオマッチングを行うZoom and Lean(ZOLE)の提案。これにより，他のドメインに一般化できるプレトレインモデルを作成することができる。一般化に際する不具合を抑制しながらアップサンプリングを行う、反復最適化問題を定式化する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330ZOLE.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>ground-truthデータが不足しているため、CNNを用いたステレオマッチングでは学習済みステレオモデルを新規ドメインに一般化することが困難とされていた。CNN学習時のイテレーションごとに最適化していくイメージ。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>スマートフォンで収集したデータを従来の手法に入力すると、物体のエッジがぼやけてしまうが、提案手法のZOLEではこれらを改善できる。</p><ul><li><a href="https://arxiv.org/pdf/1803.06641.pdf">論文</a></li></ul></div></div><div class="slide_index">[#590]</div></div></section></div></div><script src="lib/js/head.min.js"></script><script src="js/reveal.js"></script><script>Reveal.initialize({
  history: true,
  center: false,
  width: '100%',
  height: '100%',
  transition: 'none',
  dependencies: [
    { src: 'plugin/markdown/marked.js' },
    { src: 'plugin/markdown/markdown.js' },
    { src: 'plugin/notes/notes.js', async: true },
    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
  ]
});</script></body></html>